{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15efcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PubMedQA Evaluation (BASE vs CTX)\n",
    "# - BASE: question only\n",
    "# - CTX : question + its own context (open-book \"RAG\")\n",
    "# - Scoring: logprob over {yes,no,maybe} (NO format errors)\n",
    "# =========================\n",
    "\n",
    "import re, json, time, pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fefc97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda MODEL: ./gpt2\n"
     ]
    }
   ],
   "source": [
    "# ---- CONFIG ----\n",
    "\n",
    "TEST_PARQUET = \"./data/pubmedqa_hf/pqa_labeled_splits/test.parquet\"\n",
    "\n",
    "# 推荐先用 gpt2 跑通（显存小），llama2 可用 CPU 或量化\n",
    "GEN_MODEL_PATH = \"./gpt2\"    # or \"./llama2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# context 截断策略\n",
    "CTX_MAX_CHARS = 700\n",
    "CTX_N_SENTS = 3\n",
    "\n",
    "# 评测数量\n",
    "LIMIT = None  # None=全量（你test只有100），或 50/100\n",
    "\n",
    "SEED = 2025\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"DEVICE:\", DEVICE, \"MODEL:\", GEN_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38ba57da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_df) = 100\n",
      "columns: ['pubid', 'question', 'context', 'long_answer', 'final_decision']\n",
      "                                            question final_decision\n",
      "0  Malnutrition, a new inducer for arterial calci...            yes\n",
      "1  Should temperature be monitorized during kidne...             no\n",
      "2  Screening for gestational diabetes mellitus: a...            yes\n"
     ]
    }
   ],
   "source": [
    "# ---- Load data ----\n",
    "\n",
    "test_df = pd.read_parquet(TEST_PARQUET)\n",
    "print(\"len(test_df) =\", len(test_df))\n",
    "print(\"columns:\", list(test_df.columns))\n",
    "print(test_df[[\"question\",\"final_decision\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5fe382aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Text helpers ----\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return \" \".join(str(s).strip().split())\n",
    "\n",
    "_SENT_SPLIT = re.compile(r'(?<=[\\.\\?\\!])\\s+')\n",
    "\n",
    "def first_n_sents(text: str, n=3) -> str:\n",
    "    text = normalize_text(text)\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    sents = _SENT_SPLIT.split(text)\n",
    "    return \" \".join(sents[:n])\n",
    "\n",
    "def row_to_question(row: pd.Series) -> str:\n",
    "    return normalize_text(row[\"question\"])\n",
    "\n",
    "def row_to_context(row: pd.Series) -> str:\n",
    "    ctx = row[\"context\"]\n",
    "    # HF PubMedQA: context是dict {\"contexts\":[...]} 或者直接就是dict/str\n",
    "    if isinstance(ctx, dict):\n",
    "        contexts = ctx.get(\"contexts\", [])\n",
    "        if isinstance(contexts, list):\n",
    "            return \"\\n\".join([normalize_text(x) for x in contexts if x])\n",
    "        return normalize_text(str(ctx))\n",
    "    if isinstance(ctx, list):\n",
    "        return \"\\n\".join([normalize_text(x) for x in ctx if x])\n",
    "    return normalize_text(str(ctx))\n",
    "\n",
    "def row_to_gold(row: pd.Series) -> str:\n",
    "    g = normalize_text(row[\"final_decision\"]).lower()\n",
    "    assert g in [\"yes\",\"no\",\"maybe\"], f\"bad gold: {g}\"\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42c86938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded on: cuda\n"
     ]
    }
   ],
   "source": [
    "# ---- Safe load model (GPU fp16 if possible, else CPU) ----\n",
    "\n",
    "def load_gen_model_safe(model_path: str, prefer_cuda: bool = True):\n",
    "    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    if prefer_cuda and torch.cuda.is_available():\n",
    "        try:\n",
    "            mdl = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "            ).to(\"cuda\").eval()\n",
    "            return tok, mdl, \"cuda\"\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float32,\n",
    "        low_cpu_mem_usage=True,\n",
    "    ).to(\"cpu\").eval()\n",
    "    return tok, mdl, \"cpu\"\n",
    "\n",
    "gen_tok, gen_mdl, GEN_DEVICE = load_gen_model_safe(GEN_MODEL_PATH, prefer_cuda=(DEVICE==\"cuda\"))\n",
    "print(\"Loaded on:\", GEN_DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a99d8655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build_kb: 100%|██████████| 800/800 [00:00<00:00, 3582.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kb size: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Build KB docs from TRAIN ----\n",
    "# 每条 doc = 一个训练样本的 context（可以加 question 作为索引增强）\n",
    "\n",
    "kb_docs = []\n",
    "kb_texts = []\n",
    "for i in tqdm(range(len(train_df)), desc=\"build_kb\"):\n",
    "    r = train_df.iloc[i]\n",
    "    q = row_to_question(r)\n",
    "    ctx = row_to_context_str(r)\n",
    "    gold = row_to_gold_decision(r)\n",
    "    if not q or not ctx:\n",
    "        continue\n",
    "\n",
    "    # doc text 用于检索：question + context（更稳）\n",
    "    doc_text = f\"{q}\\n{ctx}\"\n",
    "    kb_docs.append({\"id\": int(i), \"q\": q, \"ctx\": ctx, \"gold\": gold, \"text\": doc_text})\n",
    "    kb_texts.append(\"passage: \" + doc_text)\n",
    "\n",
    "print(\"kb size:\", len(kb_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50d8b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Prompts ----\n",
    "# 这里不需要强行让它“只输出一个词”，因为我们不生成；我们算 logprob 三分类。\n",
    "\n",
    "def build_prompt_base(q: str) -> str:\n",
    "    return (\n",
    "        \"Answer the question with one of: yes, no, maybe.\\n\"\n",
    "        f\"Question: {q}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "def build_prompt_ctx(q: str, ctx: str) -> str:\n",
    "    return (\n",
    "        \"Use the given context to answer with one of: yes, no, maybe.\\n\"\n",
    "        f\"Context: {ctx}\\n\"\n",
    "        f\"Question: {q}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81d7e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Logprob scoring over choices (yes/no/maybe) ----\n",
    "# 关键：对每个候选词，计算 logP(candidate_tokens | prompt)\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_choice_logprob(prompt: str, choice: str) -> float:\n",
    "    # 让choice带前置空格更稳定（GPT2/BPE）\n",
    "    choice = choice if choice.startswith(\" \") else (\" \" + choice)\n",
    "\n",
    "    prompt_ids = gen_tok(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(GEN_DEVICE)\n",
    "    choice_ids = gen_tok(choice, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(GEN_DEVICE)\n",
    "\n",
    "    # 拼接后做 teacher forcing\n",
    "    input_ids = torch.cat([prompt_ids, choice_ids], dim=1)\n",
    "    out = gen_mdl(input_ids=input_ids)\n",
    "    logits = out.logits  # [1, T, V]\n",
    "\n",
    "    # choice部分每个token的logprob来自其前一位的logits\n",
    "    # 例如 choice_ids 的第0个token，其概率来自 logits 在 prompt末位那个位置\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # 取出对应 token 的 logprob 并求和\n",
    "    # positions: prompt_len-1 ... prompt_len+choice_len-2 对应预测 choice tokens\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "    choice_len = choice_ids.shape[1]\n",
    "\n",
    "    total = 0.0\n",
    "    for j in range(choice_len):\n",
    "        pos = prompt_len + j - 1\n",
    "        tok_id = int(choice_ids[0, j].item())\n",
    "        total += float(logp[0, pos, tok_id].item())\n",
    "    return total\n",
    "\n",
    "def predict_decision_logprob(prompt: str) -> Dict[str, float]:\n",
    "    scores = {\n",
    "        \"yes\": score_choice_logprob(prompt, \"yes\"),\n",
    "        \"no\": score_choice_logprob(prompt, \"no\"),\n",
    "        \"maybe\": score_choice_logprob(prompt, \"maybe\"),\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def argmax_key(d: Dict[str, float]) -> str:\n",
    "    return max(d, key=d.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38acd410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:base: 100%|██████████| 100/100 [00:02<00:00, 44.53it/s]\n",
      "eval:ctx: 100%|██████████| 100/100 [00:02<00:00, 43.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: {'acc': 0.6, 'support': 100, 'confusion': {'yes->yes': 59, 'no->yes': 29, 'yes->no': 3, 'maybe->yes': 8, 'no->no': 1}}\n",
      "CTX : {'acc': 0.61, 'support': 100, 'confusion': {'yes->yes': 57, 'no->yes': 26, 'yes->no': 5, 'maybe->yes': 7, 'no->no': 4, 'maybe->no': 1}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Eval ----\n",
    "\n",
    "def eval_pubmedqa(df: pd.DataFrame, mode: str, limit: Optional[int] = None):\n",
    "    rows = df.iloc[:limit] if limit else df\n",
    "    records = []\n",
    "\n",
    "    for i in tqdm(range(len(rows)), desc=f\"eval:{mode}\"):\n",
    "        r = rows.iloc[i]\n",
    "        q = row_to_question(r)\n",
    "        gold = row_to_gold(r)\n",
    "\n",
    "        if mode == \"base\":\n",
    "            prompt = build_prompt_base(q)\n",
    "            used_ctx = \"\"\n",
    "        elif mode == \"ctx\":\n",
    "            ctx_full = row_to_context(r)\n",
    "            ctx = first_n_sents(ctx_full, n=CTX_N_SENTS)[:CTX_MAX_CHARS]\n",
    "            used_ctx = ctx\n",
    "            prompt = build_prompt_ctx(q, ctx)\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'base' or 'ctx'\")\n",
    "\n",
    "        scores = predict_decision_logprob(prompt)\n",
    "        pred = argmax_key(scores)\n",
    "\n",
    "        records.append({\n",
    "            \"i\": int(i),\n",
    "            \"gold\": gold,\n",
    "            \"pred\": pred,\n",
    "            \"scores\": scores,\n",
    "            \"ctx_used\": used_ctx,\n",
    "            \"question\": q,\n",
    "        })\n",
    "    return records\n",
    "\n",
    "def decision_report(records: List[dict]) -> dict:\n",
    "    n = len(records)\n",
    "    correct = 0\n",
    "    conf = {}\n",
    "    for r in records:\n",
    "        g, p = r[\"gold\"], r[\"pred\"]\n",
    "        conf[f\"{g}->{p}\"] = conf.get(f\"{g}->{p}\", 0) + 1\n",
    "        if g == p:\n",
    "            correct += 1\n",
    "    return {\"acc\": correct / n if n else 0.0, \"support\": n, \"confusion\": conf}\n",
    "\n",
    "base_recs = eval_pubmedqa(test_df, \"base\", limit=LIMIT)\n",
    "ctx_recs  = eval_pubmedqa(test_df, \"ctx\",  limit=LIMIT)\n",
    "\n",
    "print(\"BASE:\", decision_report(base_recs))\n",
    "print(\"CTX :\", decision_report(ctx_recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcbcb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: eval_out/pubmedqa_base_logprob_20251222_150027.jsonl\n",
      "Saved: eval_out/pubmedqa_ctx_logprob_20251222_150027.jsonl\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# ---- Save ----\n",
    "\n",
    "out_dir = Path(\"./eval_out\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "base_path = out_dir / f\"pubmedqa_base_logprob_{ts}.jsonl\"\n",
    "ctx_path  = out_dir / f\"pubmedqa_ctx_logprob_{ts}.jsonl\"\n",
    "\n",
    "with base_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in base_recs:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with ctx_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in ctx_recs:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", base_path)\n",
    "print(\"Saved:\", ctx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fa6512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Generate ----\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(prompt: str, max_new_tokens=128) -> str:\n",
    "    t = gen_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    t = {k:v.to(DEVICE) for k,v in t.items()}\n",
    "    out = gen_mdl.generate(\n",
    "        **t,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        pad_token_id=gen_tok.eos_token_id,\n",
    "        eos_token_id=gen_tok.eos_token_id,\n",
    "    )\n",
    "    text = gen_tok.decode(out[0], skip_special_tokens=True)\n",
    "    # 截掉 prompt 前缀，保留新增部分（更干净）\n",
    "    if text.startswith(prompt):\n",
    "        text = text[len(prompt):]\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee218da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:base: 100%|██████████| 100/100 [00:02<00:00, 45.49it/s]\n",
      "eval:rag: 100%|██████████| 100/100 [00:02<00:00, 33.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Eval loop: base vs rag ----\n",
    "\n",
    "def eval_pubmedqa(df: pd.DataFrame, mode: str, limit: Optional[int] = None):\n",
    "    rows = df.iloc[:limit] if limit else df\n",
    "    records = []\n",
    "    for i in tqdm(range(len(rows)), desc=f\"eval:{mode}\"):\n",
    "        r = rows.iloc[i]\n",
    "        q = row_to_question(r)\n",
    "        gold_dec = row_to_gold_decision(r)\n",
    "        gold_text = row_to_gold_text(r)\n",
    "\n",
    "        if mode == \"base\":\n",
    "            prompt = build_prompt_base(q)\n",
    "            evid = \"\"\n",
    "        elif mode == \"rag\":\n",
    "            hits = retrieve(q, k=RAG_K)\n",
    "            evid = build_evidence_from_hits(hits, max_chars=CTX_MAX_CHARS)\n",
    "            prompt = build_prompt_rag(q, evid)\n",
    "        else:\n",
    "            raise ValueError(\"mode must be base or rag\")\n",
    "\n",
    "        pred_text = generate_text(prompt, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        pred_dec = parse_decision(pred_text)\n",
    "\n",
    "        records.append({\n",
    "            \"i\": int(i),\n",
    "            \"question\": q,\n",
    "            \"gold_decision\": gold_dec,\n",
    "            \"pred_decision\": pred_dec,\n",
    "            \"gold_text\": gold_text,\n",
    "            \"pred_text\": pred_text,\n",
    "            \"rag_context\": evid,\n",
    "        })\n",
    "    return records\n",
    "\n",
    "base_recs = eval_pubmedqa(test_df, \"base\", limit=LIMIT_TEST)\n",
    "rag_recs  = eval_pubmedqa(test_df, \"rag\",  limit=LIMIT_TEST)\n",
    "len(base_recs), len(rag_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6de57d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:base: 100%|██████████| 100/100 [00:02<00:00, 45.28it/s]\n",
      "eval:rag: 100%|██████████| 100/100 [00:02<00:00, 33.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE decision: {'acc': 0.5714285714285714, 'support': 98, 'format_errors': 2, 'confusion': {'yes->yes': 55, 'no->yes': 28, 'yes->no': 6, 'maybe->yes': 7, 'no->no': 1, 'maybe->no': 1}}\n",
      "RAG  decision: {'acc': 0.4186046511627907, 'support': 86, 'format_errors': 14, 'confusion': {'no->yes': 9, 'yes->no': 33, 'no->no': 17, 'yes->yes': 19, 'maybe->yes': 2, 'maybe->no': 6}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Decision ACC ----\n",
    "\n",
    "def decision_acc(records):\n",
    "    ok = 0\n",
    "    n = 0\n",
    "    fmt_err = 0\n",
    "    conf = {}\n",
    "    for r in records:\n",
    "        g = r[\"gold_decision\"]\n",
    "        p = r[\"pred_decision\"]\n",
    "        if p is None:\n",
    "            fmt_err += 1\n",
    "            continue\n",
    "        n += 1\n",
    "        conf[f\"{g}->{p}\"] = conf.get(f\"{g}->{p}\", 0) + 1\n",
    "        if g == p:\n",
    "            ok += 1\n",
    "    acc = ok / n if n else 0.0\n",
    "    return {\"acc\": acc, \"support\": n, \"format_errors\": fmt_err, \"confusion\": conf}\n",
    "\n",
    "LIMIT_TEST = 100\n",
    "base_recs = eval_pubmedqa(test_df, \"base\", limit=LIMIT_TEST)\n",
    "rag_recs  = eval_pubmedqa(test_df, \"rag\",  limit=LIMIT_TEST)\n",
    "\n",
    "print(\"BASE decision:\", decision_acc(base_recs))\n",
    "print(\"RAG  decision:\", decision_acc(rag_recs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4462debe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['pubid', 'question', 'context', 'long_answer', 'final_decision']\n",
      "final_decision example: ['yes', 'no', 'yes']\n",
      "0 q: Malnutrition, a new inducer for arterial calcification in hemodialysis patients?\n",
      "gold_decision(parsed): ''\n",
      "1 q: Should temperature be monitorized during kidney allograft preservation?\n",
      "gold_decision(parsed): ''\n",
      "2 q: Screening for gestational diabetes mellitus: are the criteria proposed by the in\n",
      "gold_decision(parsed): ''\n",
      "3 q: Is resected stomach volume related to weight loss after laparoscopic sleeve gast\n",
      "gold_decision(parsed): ''\n",
      "4 q: Body perception: do parents, their children, and their children's physicians per\n",
      "gold_decision(parsed): ''\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns:\", list(test_df.columns))\n",
    "\n",
    "for col in [\"answer\", \"final_decision\", \"decision\", \"label\", \"gold\", \"raw\"]:\n",
    "    if col in test_df.columns:\n",
    "        print(col, \"example:\", test_df[col].head(3).tolist())\n",
    "\n",
    "# 看前5条 gold_decision 实际解析结果\n",
    "for i in range(5):\n",
    "    r = test_df.iloc[i]\n",
    "    print(i, \"q:\", row_to_question(r)[:80])\n",
    "    print(\"gold_decision(parsed):\", repr(row_to_gold_decision(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5aab8b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c949aaa1e4f943319329b3dcd722cbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed56fb3304042e59d817fe6d3784cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2078d183e7274a92bae42e504b74b1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8251bfb8926d441383368b9968f3fc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29aacb23e924d2fb716bdb737e5f88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f829a218697b414a8a0df0b0accf9b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE BERTScore: {'precision': -0.12728960812091827, 'recall': 0.0066066887229681015, 'f1': -0.060143403708934784}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG  BERTScore: {'precision': -0.17148424685001373, 'recall': 0.026392295956611633, 'f1': -0.07353765517473221}\n"
     ]
    }
   ],
   "source": [
    "# ---- BERTScore (optional but matches你之前实验) ----\n",
    "# 如果没装： pip install bert-score\n",
    "from bert_score import score as bertscore\n",
    "\n",
    "def bertscore_mean(records):\n",
    "    refs = [r[\"gold_text\"] for r in records]\n",
    "    hyps = [r[\"pred_text\"] for r in records]\n",
    "    P, R, F1 = bertscore(hyps, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "    return {\"precision\": float(P.mean()), \"recall\": float(R.mean()), \"f1\": float(F1.mean())}\n",
    "\n",
    "print(\"BASE BERTScore:\", bertscore_mean(base_recs))\n",
    "print(\"RAG  BERTScore:\", bertscore_mean(rag_recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57c53822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: eval_out/pubmedqa_base_20251222_143546.jsonl\n",
      "Saved: eval_out/pubmedqa_rag_20251222_143546.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ---- Save results ----\n",
    "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_base = Path(\"./eval_out\") / f\"pubmedqa_base_{ts}.jsonl\"\n",
    "out_rag  = Path(\"./eval_out\") / f\"pubmedqa_rag_{ts}.jsonl\"\n",
    "out_base.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with out_base.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in base_recs:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "with out_rag.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rag_recs:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", out_base)\n",
    "print(\"Saved:\", out_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93edd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
