OS：Ubuntu 22.04

GPU：NVIDIA RTX 3080 Ti（12GB）

Python：3.10.x（3.10.12）

llama2-Chatv2.py：/home/mew/mev/llm/llama2-Chatv2.py

Llama model：/home/mew/mev/llm/llama2

prompt file：/home/mew/mev/llm/prompts/prompts_llama2.txt

work address：/home/mew/mev/llm/


2. 환경 설치 및 준비

1) GPU 및 CUDA 확인
nvidia-smi

2) Python 버전 확인
python3 --version
which python3

4) pip 경로 보정
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc

5) 필수 Python 패키지 설치
python3 -m pip install --user --upgrade pip
python3 -m pip uninstall -y transformers tokenizers || true
python3 -m pip install --user --no-cache-dir "transformers==4.44.2" "tokenizers==0.19.1" accelerate safetensors sentencepiece
python3 -m pip install --user --no-cache-dir bitsandbytes   # (int8 모드 사용 시)
python3 -m pip install --user jsonschema                    # (JSON 검증용 선택 설치)

3. 파일 구조 확인
ls -lh \
  /home/mew/mev/llm/llama2-Chatv2.py \
  /home/mew/mev/llm/prompts/prompts_llama2.txt \
  /home/mew/mev/llm/tests/input_fact.txt

#모델 필수 파일
ls -lh /home/mew/mev/llm/llama2 | egrep -i 'config\.json|tokenizer|generation_config|pytorch_model|safetensors' | sed -n '1,200p'

# Python 版本与 transformers 版本 / 파이썬 & 트랜스포머 버전
python3 - <<'PY'
import sys, transformers, torch
print("Python:", sys.version.split()[0])
print("Transformers:", transformers.__version__)
print("Torch:", torch.__version__, "CUDA:", torch.cuda.is_available())
PY

# GPU 可用性（int8 推荐）/ GPU 사용 가능 여부
command -v nvidia-smi >/dev/null && nvidia-smi || echo "No nvidia-smi"


4.# Llama2 모델 실행(prompt)
python3 /home/mew/mev/llm/llama2-Chatv2.py \
  --model_dir "/home/mew/mev/llm/llama2" \
  --mode int8 \
  --temperature 0.1 \
  --top_p 1.0 \
  --repetition_penalty 1.08 \
  --max_new_tokens 300 \
  --n_samples 1 \
  --system "$(cat /home/mew/mev/llm/prompts/prompts_llama2.txt)" \
  --question "From the provided material, give a two-sentence overview of aspirin’s mechanism of action and one major contraindication."
