{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcf7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MedMCQA RAG (E5 + FAISS + BM25 Hybrid) — FULL RUNNABLE NOTEBOOK CODE\n",
    "# - KB: train -> (Q-only for retrieval, store Q/A)  ✅ no ABCD problem text in KB\n",
    "# - Retrieval: Dense (E5) topK -> BM25 fuse -> near-dup rerank -> selective evidence (only add A when very similar)\n",
    "# - Scoring: default = letter logits (A/B/C/D). Optional = option_text_logprob (stronger but slower)\n",
    "# =========================\n",
    "\n",
    "# ---- Cell 0: installs (run once) ----\n",
    "# !pip install -q faiss-cpu rank-bm25 tqdm transformers\n",
    "\n",
    "import os, re, json, time, pickle, math, random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee376e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from rank-bm25) (1.26.4)\n",
      "Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c335e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cell 1: CONFIG ----\n",
    "\n",
    "# ========== Data paths ==========\n",
    "MEDMCQA_TRAIN_FILE = \"./data/medmcqa/train.json\"  # <-- change\n",
    "MEDMCQA_DEV_FILE   = \"./data/medmcqa/dev.json\"    # <-- change\n",
    "\n",
    "# ========== Model (for answering MCQ) ==========\n",
    "# Example: GPT-2\n",
    "BASE_MODEL_PATH = \"./gpt2\"  # or \"gpt2\" if you use HF hub (requires net/cache)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "# ========== Retrieval embedding model ==========\n",
    "E5_MODEL_NAME = \"intfloat/e5-base-v2\"  # you already built this successfully\n",
    "\n",
    "# ========== KB build outputs ==========\n",
    "KB_JSONL = \"./kb/medmcqa_q_only_kb_train.filtered.jsonl\"\n",
    "KB_DIR   = \"./rag_cache/medmcqa_q_only_kb_train.filtered_e5\"  # will contain kb.index, docs.pkl\n",
    "\n",
    "# ========== Retrieval hyperparams ==========\n",
    "K_VEC   = 400    # dense candidates\n",
    "K_FINAL = 50     # after fusion, keep this many for near-dup rerank\n",
    "ALPHA   = 0.35   # fusion: alpha*dense + (1-alpha)*bm25. smaller -> more BM25\n",
    "\n",
    "MIN_SIM_KEEP    = 0.18  # near-dup jaccard threshold to keep\n",
    "MIN_SIM_FOR_ANS = 0.22  # only include A in evidence if near-dup >= this\n",
    "\n",
    "EVID_MAX_ITEMS  = 3\n",
    "CTX_MAX_CHARS   = 600\n",
    "\n",
    "# ========== Scoring ==========\n",
    "# \"letter\" (fast): score A/B/C/D token logits\n",
    "# \"option_logprob\" (stronger but slower): score logP(option_text | prompt)\n",
    "SCORING = \"option_logprob\"  # change to \"option_logprob\" if needed\n",
    "\n",
    "# ========== Output ==========\n",
    "OUT_DIR = \"./eval_out\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0e939e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dev samples: 4183\n",
      "Example: Which of the following is not true for myelinated nerve fibers:\n"
     ]
    }
   ],
   "source": [
    "# ---- Cell 2: Helpers (dataset + normalization) ----\n",
    "\n",
    "BAD_ANS = {\n",
    "  \"all of the above\", \"none of the above\",\n",
    "  \"both a and b\", \"both b and c\", \"both a and c\",\n",
    "  \"a and b\", \"b and c\", \"a and c\",\n",
    "  \"a & b\", \"b & c\", \"a & c\",\n",
    "  \"a, b and c\", \"a, b, c\", \"a,b and c\",\n",
    "}\n",
    "\n",
    "def load_json_or_jsonl(path: str):\n",
    "    p = Path(path)\n",
    "    text = p.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        return obj\n",
    "    except json.JSONDecodeError:\n",
    "        rows = []\n",
    "        with p.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    rows.append(json.loads(line))\n",
    "        return rows\n",
    "\n",
    "def normalize_gold_cop(cop: int) -> str:\n",
    "    return \"ABCD\"[int(cop) - 1]\n",
    "\n",
    "def is_bad_answer(a: str) -> bool:\n",
    "    if a is None:\n",
    "        return True\n",
    "    x = \" \".join(str(a).lower().split())\n",
    "    if x in BAD_ANS:\n",
    "        return True\n",
    "    if len(x) <= 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def normalize_q_for_retrieval(q: str) -> str:\n",
    "    \"\"\"Make entities (drug/disease/material) more salient for retrieval.\"\"\"\n",
    "    q0 = \" \".join(str(q).strip().split())\n",
    "    ql = q0.lower().strip()\n",
    "\n",
    "    # mechanism of action of X:\n",
    "    m = re.match(r\"^mechanism of action of\\s+(.+?)[\\:\\?]?$\", ql)\n",
    "    if m:\n",
    "        x = m.group(1).strip()\n",
    "        return f\"{x} mechanism of action\"\n",
    "\n",
    "    # features/findings of X except\n",
    "    m = re.match(r\"^all of the following are (?:features|findings) of\\s+(.+?)\\s+except.*$\", ql)\n",
    "    if m:\n",
    "        x = m.group(1).strip()\n",
    "        return f\"{x} features except\"\n",
    "\n",
    "    return q0\n",
    "\n",
    "def load_medmcqa_split(path: str) -> List[dict]:\n",
    "    data = load_json_or_jsonl(path)\n",
    "    if isinstance(data, dict) and \"data\" in data and isinstance(data[\"data\"], list):\n",
    "        data = data[\"data\"]\n",
    "    assert isinstance(data, list), f\"Unexpected format in {path}\"\n",
    "\n",
    "    samples = []\n",
    "    for i, ex in enumerate(data):\n",
    "        q = ex.get(\"question\") or ex.get(\"ques\") or ex.get(\"query\")\n",
    "        if not q:\n",
    "            continue\n",
    "        opts = {\"A\": ex.get(\"opa\"), \"B\": ex.get(\"opb\"), \"C\": ex.get(\"opc\"), \"D\": ex.get(\"opd\")}\n",
    "        cop = ex.get(\"cop\")\n",
    "        if cop is None:\n",
    "            continue\n",
    "        gold = normalize_gold_cop(cop)\n",
    "        samples.append({\n",
    "            \"id\": ex.get(\"id\", i),\n",
    "            \"question\": \" \".join(str(q).strip().split()),\n",
    "            \"options\": {k: \" \".join(str(v).strip().split()) for k, v in opts.items()},\n",
    "            \"gold\": gold,\n",
    "            \"raw\": ex\n",
    "        })\n",
    "    return samples\n",
    "\n",
    "dev_samples = load_medmcqa_split(MEDMCQA_DEV_FILE)\n",
    "print(\"Loaded dev samples:\", len(dev_samples))\n",
    "print(\"Example:\", dev_samples[0][\"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba39a31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB JSONL exists: ./kb/medmcqa_q_only_kb_train.filtered.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ---- Cell 3: Build KB JSONL (train -> filtered Q-only retrieval docs) ----\n",
    "# Output format per line:\n",
    "# {\"id\":..., \"q\":..., \"a\":..., \"text\":...}  where text is used for retrieval (Q normalized), evidence uses q/a.\n",
    "\n",
    "def build_kb_jsonl(train_file: str, out_jsonl: str):\n",
    "    data = load_json_or_jsonl(train_file)\n",
    "    if isinstance(data, dict) and \"data\" in data and isinstance(data[\"data\"], list):\n",
    "        data = data[\"data\"]\n",
    "    assert isinstance(data, list), f\"Unexpected format in {train_file}\"\n",
    "\n",
    "    outp = Path(out_jsonl)\n",
    "    outp.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    kept = 0\n",
    "    skipped_bad = 0\n",
    "    skipped_missing = 0\n",
    "\n",
    "    with outp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i, ex in enumerate(data):\n",
    "            q = ex.get(\"question\") or ex.get(\"ques\") or ex.get(\"query\")\n",
    "            if not q:\n",
    "                skipped_missing += 1\n",
    "                continue\n",
    "\n",
    "            options = {\"A\": ex.get(\"opa\"), \"B\": ex.get(\"opb\"), \"C\": ex.get(\"opc\"), \"D\": ex.get(\"opd\")}\n",
    "            cop = ex.get(\"cop\")\n",
    "            if cop is None:\n",
    "                skipped_missing += 1\n",
    "                continue\n",
    "\n",
    "            gold = normalize_gold_cop(cop)\n",
    "            a = options.get(gold)\n",
    "\n",
    "            if not a or is_bad_answer(a):\n",
    "                skipped_bad += 1\n",
    "                continue\n",
    "\n",
    "            q_clean = \" \".join(str(q).strip().split())\n",
    "            a_clean = \" \".join(str(a).strip().split())\n",
    "\n",
    "            doc = {\n",
    "                \"id\": ex.get(\"id\", i),\n",
    "                \"q\": q_clean,\n",
    "                \"a\": a_clean,\n",
    "                \"text\": normalize_q_for_retrieval(q_clean),  # retrieval text only\n",
    "            }\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "            kept += 1\n",
    "\n",
    "    print(\"KB JSONL wrote:\", kept, \"->\", outp)\n",
    "    print(\"Skipped bad answers:\", skipped_bad, \"| skipped missing:\", skipped_missing)\n",
    "\n",
    "# Build if missing\n",
    "if not Path(KB_JSONL).exists():\n",
    "    build_kb_jsonl(MEDMCQA_TRAIN_FILE, KB_JSONL)\n",
    "else:\n",
    "    print(\"KB JSONL exists:\", KB_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d8737b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index exists: ./rag_cache/medmcqa_q_only_kb_train.filtered_e5\n"
     ]
    }
   ],
   "source": [
    "# ---- Cell 4: Build E5 FAISS index (passage: text) + docs.pkl ----\n",
    "\n",
    "def mean_pool(last_hidden, attn_mask):\n",
    "    mask = attn_mask.unsqueeze(-1).to(last_hidden.dtype)\n",
    "    summed = (last_hidden * mask).sum(dim=1)\n",
    "    denom = mask.sum(dim=1).clamp(min=1e-6)\n",
    "    return summed / denom\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_e5_texts(texts: List[str], tok, mdl, device, max_len=256, bs=64) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in tqdm(range(0, len(texts), bs), desc=\"embed(e5)\"):\n",
    "        batch = texts[i:i+bs]\n",
    "        t = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "        t = {k: v.to(device) for k, v in t.items()}\n",
    "        out = mdl(**t)\n",
    "        emb = mean_pool(out.last_hidden_state, t[\"attention_mask\"])\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "        vecs.append(emb.float().cpu().numpy())\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "def build_faiss_e5(kb_jsonl: str, out_dir: str, e5_name: str):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    docs = []\n",
    "    passages = []\n",
    "    with Path(kb_jsonl).open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "            docs.append(j)  # dict with q/a/text\n",
    "            passages.append(\"passage: \" + j[\"text\"])\n",
    "\n",
    "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tok = AutoTokenizer.from_pretrained(e5_name, use_fast=True)\n",
    "    mdl = AutoModel.from_pretrained(e5_name).to(dev).eval()\n",
    "    if dev == \"cuda\":\n",
    "        mdl.half()\n",
    "\n",
    "    X = encode_e5_texts(passages, tok, mdl, dev, max_len=256, bs=64).astype(np.float32)\n",
    "    dim = X.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(X)\n",
    "\n",
    "    faiss.write_index(index, str(out_dir / \"kb.index\"))\n",
    "    (out_dir / \"docs.pkl\").write_bytes(pickle.dumps(docs))\n",
    "\n",
    "    print(\"Saved:\", out_dir / \"kb.index\")\n",
    "    print(\"Saved:\", out_dir / \"docs.pkl\")\n",
    "    print(\"ntotal:\", index.ntotal)\n",
    "\n",
    "# Build if missing\n",
    "if not (Path(KB_DIR) / \"kb.index\").exists() or not (Path(KB_DIR) / \"docs.pkl\").exists():\n",
    "    build_faiss_e5(KB_JSONL, KB_DIR, E5_MODEL_NAME)\n",
    "else:\n",
    "    print(\"FAISS index exists:\", KB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec056f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded KB: 176438 docs\n",
      "BM25 ready\n"
     ]
    }
   ],
   "source": [
    "# ---- Cell 5: Load KB (FAISS + docs) + Build BM25 ----\n",
    "\n",
    "def load_kb(kb_dir: str):\n",
    "    kb_dir = Path(kb_dir)\n",
    "    index = faiss.read_index(str(kb_dir / \"kb.index\"))\n",
    "    docs = pickle.loads((kb_dir / \"docs.pkl\").read_bytes())\n",
    "    assert isinstance(docs, list) and isinstance(docs[0], dict), \"docs.pkl must be list[dict]\"\n",
    "    return index, docs\n",
    "\n",
    "index, kb_docs = load_kb(KB_DIR)\n",
    "print(\"Loaded KB:\", len(kb_docs), \"docs\")\n",
    "\n",
    "def bm25_tokenize(s: str):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    return [t for t in s.split() if len(t) >= 2]\n",
    "\n",
    "bm25 = BM25Okapi([bm25_tokenize(d[\"q\"]) for d in kb_docs])\n",
    "print(\"BM25 ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834877c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cell 6: Load E5 encoder for queries (must match index) ----\n",
    "\n",
    "embed_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "e5_tok = AutoTokenizer.from_pretrained(E5_MODEL_NAME, use_fast=True)\n",
    "e5_model = AutoModel.from_pretrained(E5_MODEL_NAME).to(embed_device).eval()\n",
    "if embed_device == \"cuda\":\n",
    "    e5_model.half()\n",
    "\n",
    "@torch.no_grad()\n",
    "def e5_encode_query(text: str) -> np.ndarray:\n",
    "    t = \"query: \" + text\n",
    "    tok = e5_tok(t, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    tok = {k: v.to(embed_device) for k, v in tok.items()}\n",
    "    out = e5_model(**tok)\n",
    "    emb = mean_pool(out.last_hidden_state, tok[\"attention_mask\"])\n",
    "    emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb[0].float().cpu().numpy().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03057e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Q: Retraction of mandible is achieved by:\n",
      "Gold: B\n",
      "Hit Q: Maximum amount of incisor retraction achieved is:\n",
      "Hit A: 7 mm\n",
      "========================================================================================================================\n",
      "Q: Technique of root coverage called as:\n",
      "Gold: B\n",
      "Hit Q: Given technique is called as:\n",
      "Hit A: Crown down technique\n",
      "========================================================================================================================\n",
      "Q: MTA barrier in open apex is made up to?\n",
      "Gold: B\n",
      "Hit Q: Barrier method\n",
      "Hit A: Condom\n",
      "========================================================================================================================\n",
      "Q: %lost radio-resistant cells in retina\n",
      "Gold: B\n",
      "Hit Q: Cells are most radio-resistant in\n",
      "Hit A: S phase\n",
      "========================================================================================================================\n",
      "Q: In symphyseal fracture with lag screw fixation?\n",
      "Gold: B\n",
      "Hit Q: Long bone fracture fixation done with -\n",
      "Hit A: Intramedullary nail\n",
      "========================================================================================================================\n",
      "Q: A lady delivered a normal vaginal delivery and was discharged. On third day she came back with fever, tachycardia and seizures. Fundus showed papilledema with no focal deficits. What is the most likely diagnosis?\n",
      "Gold: A\n",
      "Hit Q: A child from West Bengal presents with fever & unconsciousness for 1 day and pallor with no focal neurodeficit. What is the most probable diagnosis?\n",
      "Hit A: Cerebral malaria\n",
      "========================================================================================================================\n",
      "Q: Euphemism pudding paste is used for:\n",
      "Gold: D\n",
      "Hit Q: Tolvaptan is used for\n",
      "Hit A: SIADH\n",
      "========================================================================================================================\n",
      "Q: Which of the following increases callus formation:\n",
      "Gold: B\n",
      "Hit Q: Which of the following increases BMR?\n",
      "Hit A: Ingestion of food\n"
     ]
    }
   ],
   "source": [
    "# ---- Cell 7: Hybrid retrieval + near-dup rerank + selective evidence ----\n",
    "\n",
    "def toks_for_jaccard(s: str):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    return [t for t in s.split() if len(t) >= 3]\n",
    "\n",
    "def jaccard(a: List[str], b: List[str]) -> float:\n",
    "    A, B = set(a), set(b)\n",
    "    if not A or not B:\n",
    "        return 0.0\n",
    "    return len(A & B) / len(A | B)\n",
    "\n",
    "def near_dup_score(q: str, hit_q: str) -> float:\n",
    "    return jaccard(toks_for_jaccard(q), toks_for_jaccard(hit_q))\n",
    "\n",
    "def faiss_search(qvec: np.ndarray, k: int) -> Tuple[List[int], List[float]]:\n",
    "    D, I = index.search(qvec.reshape(1, -1), k)\n",
    "    return I[0].tolist(), D[0].tolist()\n",
    "\n",
    "def retrieve_topk_hybrid(question: str, k_vec=K_VEC, k_final=K_FINAL, alpha=ALPHA) -> List[int]:\n",
    "    q_norm = normalize_q_for_retrieval(question)\n",
    "    qvec = e5_encode_query(q_norm)\n",
    "    cand_ids, dense_scores = faiss_search(qvec, k_vec)\n",
    "\n",
    "    # BM25 scores over all, slice to candidates\n",
    "    qtok = bm25_tokenize(question)\n",
    "    bm25_scores_all = bm25.get_scores(qtok)\n",
    "    bm25_scores = np.array([bm25_scores_all[i] for i in cand_ids], dtype=np.float32)\n",
    "    dense_scores = np.array(dense_scores, dtype=np.float32)\n",
    "\n",
    "    def norm(x):\n",
    "        if float(x.max() - x.min()) < 1e-6:\n",
    "            return np.zeros_like(x)\n",
    "        return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "    fused = alpha * norm(dense_scores) + (1 - alpha) * norm(bm25_scores)\n",
    "    order = np.argsort(-fused)[:k_final]\n",
    "    return [cand_ids[i] for i in order]\n",
    "\n",
    "def rerank_and_filter(question: str, cand_ids: List[int], min_sim=MIN_SIM_KEEP) -> List[int]:\n",
    "    scored = []\n",
    "    for did in cand_ids:\n",
    "        s = near_dup_score(question, kb_docs[did][\"q\"])\n",
    "        scored.append((s, did))\n",
    "    scored.sort(reverse=True)\n",
    "    kept = [did for s, did in scored if s >= min_sim]\n",
    "    return kept if kept else [scored[0][1]]  # never empty\n",
    "\n",
    "def build_evidence(question: str, hit_ids: List[int],\n",
    "                   max_items=EVID_MAX_ITEMS, max_chars=CTX_MAX_CHARS,\n",
    "                   min_sim_for_A=MIN_SIM_FOR_ANS) -> str:\n",
    "    blocks = []\n",
    "    for did in hit_ids[:max_items]:\n",
    "        item = kb_docs[did]\n",
    "        sim = near_dup_score(question, item[\"q\"])\n",
    "        if sim >= min_sim_for_A:\n",
    "            blocks.append(f'Q: {item[\"q\"]}\\nA: {item[\"a\"]}')\n",
    "        else:\n",
    "            blocks.append(f'Related Q: {item[\"q\"]}')\n",
    "    ev = \"\\n\\n\".join(blocks)\n",
    "    return ev[:max_chars]\n",
    "\n",
    "def sanity_show_hits(samples: List[dict], n=10):\n",
    "    for s in random.sample(samples, n):\n",
    "        q = s[\"question\"]\n",
    "        cand = retrieve_topk_hybrid(q)\n",
    "        hit = rerank_and_filter(q, cand)\n",
    "        top = kb_docs[hit[0]]\n",
    "        print(\"=\"*120)\n",
    "        print(\"Q:\", q)\n",
    "        print(\"Gold:\", s[\"gold\"])\n",
    "        print(\"Hit Q:\", top[\"q\"][:200])\n",
    "        print(\"Hit A:\", top[\"a\"][:200])\n",
    "\n",
    "sanity_show_hits(dev_samples, n=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6443f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MCQ model: ./gpt2\n"
     ]
    }
   ],
   "source": [
    "# ---- Cell 8: Load MCQ Answering Model (base) ----\n",
    "\n",
    "def load_mcq_model(model_path: str, device: str, dtype: torch.dtype):\n",
    "    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    mdl.to(device)\n",
    "    mdl.eval()\n",
    "    if device == \"cuda\":\n",
    "        mdl = mdl.half()\n",
    "    return tok, mdl\n",
    "\n",
    "mcq_tok, mcq_model = load_mcq_model(BASE_MODEL_PATH, DEVICE, DTYPE)\n",
    "print(\"Loaded MCQ model:\", BASE_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d8d16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice token ids: {'A': [32, 317], 'B': [33, 347], 'C': [34, 327], 'D': [35, 360]}\n"
     ]
    }
   ],
   "source": [
    "# ---- Cell 9: Prompt + Scoring ----\n",
    "\n",
    "def build_prompt_base(question: str, opts: Dict[str, str]) -> str:\n",
    "    # Keep prompt minimal (too long hurts small models)\n",
    "    return (\n",
    "        \"Choose the correct option (A, B, C, or D).\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"A) {opts['A']}\\n\"\n",
    "        f\"B) {opts['B']}\\n\"\n",
    "        f\"C) {opts['C']}\\n\"\n",
    "        f\"D) {opts['D']}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "def build_prompt_rag(question: str, opts: Dict[str, str], evidence: str) -> str:\n",
    "    return (\n",
    "        \"Use the evidence to answer. Choose A, B, C, or D.\\n\\n\"\n",
    "        f\"Evidence:\\n{evidence}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"A) {opts['A']}\\n\"\n",
    "        f\"B) {opts['B']}\\n\"\n",
    "        f\"C) {opts['C']}\\n\"\n",
    "        f\"D) {opts['D']}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "def _choice_token_ids(tok, ch: str) -> List[int]:\n",
    "    # Score multiple possible tokenizations\n",
    "    candidates = [ch, \" \" + ch, \"\\n\" + ch]\n",
    "    ids = []\n",
    "    for s in candidates:\n",
    "        t = tok.encode(s, add_special_tokens=False)\n",
    "        if len(t) == 1:\n",
    "            ids.append(t[0])\n",
    "    return list(dict.fromkeys(ids))  # unique\n",
    "\n",
    "CHOICE_TOKEN_IDS = {c: _choice_token_ids(mcq_tok, c) for c in [\"A\",\"B\",\"C\",\"D\"]}\n",
    "print(\"Choice token ids:\", CHOICE_TOKEN_IDS)\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_letter_logits(prompt: str) -> Dict[str, float]:\n",
    "    t = mcq_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    t = {k: v.to(DEVICE) for k, v in t.items()}\n",
    "    out = mcq_model(**t)\n",
    "    logits = out.logits[0, -1]  # next-token logits\n",
    "    scores = {}\n",
    "    for c in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "        ids = CHOICE_TOKEN_IDS[c]\n",
    "        if not ids:\n",
    "            scores[c] = -1e9\n",
    "        else:\n",
    "            scores[c] = float(torch.max(logits[ids]).item())\n",
    "    return scores\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_option_logprob(prompt: str, option_text: str, max_len: int = 512) -> float:\n",
    "    # logP(option_text | prompt)\n",
    "    full = prompt + \" \" + option_text\n",
    "    tok_full = mcq_tok(full, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    tok_full = {k:v.to(DEVICE) for k,v in tok_full.items()}\n",
    "    input_ids = tok_full[\"input_ids\"]\n",
    "    attn = tok_full[\"attention_mask\"]\n",
    "\n",
    "    tok_p = mcq_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    prompt_len = tok_p[\"input_ids\"].shape[1]\n",
    "\n",
    "    out = mcq_model(input_ids=input_ids, attention_mask=attn)\n",
    "    logits = out.logits  # [1,T,V]\n",
    "    targets = input_ids[:, prompt_len:]\n",
    "    if targets.numel() == 0:\n",
    "        return -1e9\n",
    "\n",
    "    logp = 0.0\n",
    "    for j in range(targets.shape[1]):\n",
    "        tpos = prompt_len + j\n",
    "        if tpos == 0:\n",
    "            continue\n",
    "        lp = F.log_softmax(logits[0, tpos-1], dim=-1)[targets[0, j]].item()\n",
    "        logp += lp\n",
    "    return float(logp)\n",
    "\n",
    "def predict_one(sample: dict, mode: str, ctx_max_chars: int = CTX_MAX_CHARS) -> dict:\n",
    "    q = sample[\"question\"]\n",
    "    opts = sample[\"options\"]\n",
    "    gold = sample[\"gold\"]\n",
    "\n",
    "    if mode == \"base\":\n",
    "        prompt = build_prompt_base(q, opts)\n",
    "        evidence = \"\"\n",
    "    elif mode == \"rag_q\":\n",
    "        cand = retrieve_topk_hybrid(q)\n",
    "        hit = rerank_and_filter(q, cand)\n",
    "        evidence = build_evidence(q, hit, max_items=EVID_MAX_ITEMS, max_chars=ctx_max_chars)\n",
    "        prompt = build_prompt_rag(q, opts, evidence)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'base' or 'rag_q'\")\n",
    "\n",
    "    if SCORING == \"letter\":\n",
    "        scores = score_letter_logits(prompt)\n",
    "        pred = max(scores, key=scores.get)\n",
    "    elif SCORING == \"option_logprob\":\n",
    "        scores = {c: score_option_logprob(prompt, opts[c]) for c in [\"A\",\"B\",\"C\",\"D\"]}\n",
    "        pred = max(scores, key=scores.get)\n",
    "    else:\n",
    "        raise ValueError(\"SCORING must be 'letter' or 'option_logprob'\")\n",
    "\n",
    "    return {\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"gold\": gold,\n",
    "        \"pred\": pred,\n",
    "        \"scores\": scores,\n",
    "        \"rag_context\": evidence,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "318c5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cell 10: Metrics ----\n",
    "\n",
    "def confusion_matrix(records: List[dict]) -> np.ndarray:\n",
    "    idx = {c:i for i,c in enumerate([\"A\",\"B\",\"C\",\"D\"])}\n",
    "    cm = np.zeros((4,4), dtype=int)\n",
    "    for r in records:\n",
    "        cm[idx[r[\"gold\"]], idx[r[\"pred\"]]] += 1\n",
    "    return cm\n",
    "\n",
    "def precision_recall_f1(cm: np.ndarray):\n",
    "    # per-class\n",
    "    eps = 1e-9\n",
    "    tp = np.diag(cm).astype(np.float32)\n",
    "    pred_sum = cm.sum(axis=0).astype(np.float32)\n",
    "    gold_sum = cm.sum(axis=1).astype(np.float32)\n",
    "\n",
    "    prec = tp / (pred_sum + eps)\n",
    "    rec  = tp / (gold_sum + eps)\n",
    "    f1   = 2*prec*rec / (prec+rec+eps)\n",
    "    return prec, rec, f1, gold_sum\n",
    "\n",
    "def summarize(records: List[dict]) -> dict:\n",
    "    cm = confusion_matrix(records)\n",
    "    prec, rec, f1, support = precision_recall_f1(cm)\n",
    "\n",
    "    acc = float(np.trace(cm) / max(1, cm.sum()))\n",
    "    macro_p = float(np.mean(prec))\n",
    "    macro_r = float(np.mean(rec))\n",
    "    macro_f = float(np.mean(f1))\n",
    "    weighted_f = float(np.sum(f1 * support) / max(1, np.sum(support)))\n",
    "    weighted_p = float(np.sum(prec * support) / max(1, np.sum(support)))\n",
    "    weighted_r = float(np.sum(rec * support) / max(1, np.sum(support)))\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"macro_p\": macro_p, \"macro_r\": macro_r, \"macro_f1\": macro_f,\n",
    "        \"weighted_p\": weighted_p, \"weighted_r\": weighted_r, \"weighted_f1\": weighted_f,\n",
    "        \"cm\": cm,\n",
    "        \"per_class\": {\n",
    "            \"A\": {\"p\": float(prec[0]), \"r\": float(rec[0]), \"f1\": float(f1[0]), \"support\": int(support[0])},\n",
    "            \"B\": {\"p\": float(prec[1]), \"r\": float(rec[1]), \"f1\": float(f1[1]), \"support\": int(support[1])},\n",
    "            \"C\": {\"p\": float(prec[2]), \"r\": float(rec[2]), \"f1\": float(f1[2]), \"support\": int(support[2])},\n",
    "            \"D\": {\"p\": float(prec[3]), \"r\": float(rec[3]), \"f1\": float(f1[3]), \"support\": int(support[3])},\n",
    "        }\n",
    "    }\n",
    "\n",
    "def fix_hurt(base_records: List[dict], rag_records: List[dict]) -> dict:\n",
    "    base_map = {r[\"id\"]: r for r in base_records}\n",
    "    fix = hurt = same_correct = same_wrong = 0\n",
    "    for r in rag_records:\n",
    "        b = base_map[r[\"id\"]]\n",
    "        base_ok = (b[\"pred\"] == b[\"gold\"])\n",
    "        rag_ok = (r[\"pred\"] == r[\"gold\"])\n",
    "        if (not base_ok) and rag_ok:\n",
    "            fix += 1\n",
    "        elif base_ok and (not rag_ok):\n",
    "            hurt += 1\n",
    "        elif base_ok and rag_ok:\n",
    "            same_correct += 1\n",
    "        else:\n",
    "            same_wrong += 1\n",
    "    return {\"fix\": fix, \"hurt\": hurt, \"same_correct\": same_correct, \"same_wrong\": same_wrong, \"n\": len(rag_records)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da42204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:base: 100%|██████████| 10/10 [00:00<00:00, 22.77it/s]\n",
      "eval:rag_q: 100%|██████████| 10/10 [00:04<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke test ok.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Cell 11: Evaluate (Base vs RAG-Q) ----\n",
    "\n",
    "def eval_mode(samples: List[dict], mode: str, ctx_max_chars: int = CTX_MAX_CHARS, limit: Optional[int] = None) -> List[dict]:\n",
    "    out = []\n",
    "    it = samples[:limit] if limit else samples\n",
    "    for s in tqdm(it, desc=f\"eval:{mode}\"):\n",
    "        out.append(predict_one(s, mode=mode, ctx_max_chars=ctx_max_chars))\n",
    "    return out\n",
    "\n",
    "# Quick smoke test (10 samples)\n",
    "_ = eval_mode(dev_samples, \"base\", limit=10)\n",
    "_ = eval_mode(dev_samples, \"rag_q\", limit=10)\n",
    "print(\"Smoke test ok.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aa89353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:base:   1%|          | 33/4183 [00:01<02:08, 32.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m out_path_base \u001b[38;5;241m=\u001b[39m Path(OUT_DIR) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.base.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m out_path_rag  \u001b[38;5;241m=\u001b[39m Path(OUT_DIR) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ragq.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m base_records \u001b[38;5;241m=\u001b[39m \u001b[43meval_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_max_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCTX_MAX_CHARS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m ragq_records \u001b[38;5;241m=\u001b[39m eval_mode(dev_samples, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_q\u001b[39m\u001b[38;5;124m\"\u001b[39m, ctx_max_chars\u001b[38;5;241m=\u001b[39mCTX_MAX_CHARS)\n\u001b[1;32m     11\u001b[0m base_sum \u001b[38;5;241m=\u001b[39m summarize(base_records)\n",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m, in \u001b[0;36meval_mode\u001b[0;34m(samples, mode, ctx_max_chars, limit)\u001b[0m\n\u001b[1;32m      5\u001b[0m it \u001b[38;5;241m=\u001b[39m samples[:limit] \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;28;01melse\u001b[39;00m samples\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m tqdm(it, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpredict_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_max_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx_max_chars\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[13], line 102\u001b[0m, in \u001b[0;36mpredict_one\u001b[0;34m(sample, mode, ctx_max_chars)\u001b[0m\n\u001b[1;32m    100\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m SCORING \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption_logprob\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {c: score_option_logprob(prompt, opts[c]) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    103\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[13], line 102\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m SCORING \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption_logprob\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {c: \u001b[43mscore_option_logprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    103\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 67\u001b[0m, in \u001b[0;36mscore_option_logprob\u001b[0;34m(prompt, option_text, max_len)\u001b[0m\n\u001b[1;32m     64\u001b[0m tok_p \u001b[38;5;241m=\u001b[39m mcq_tok(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39mmax_len)\n\u001b[1;32m     65\u001b[0m prompt_len \u001b[38;5;241m=\u001b[39m tok_p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmcq_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m logits \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# [1,T,V]\u001b[39;00m\n\u001b[1;32m     69\u001b[0m targets \u001b[38;5;241m=\u001b[39m input_ids[:, prompt_len:]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1062\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    910\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    911\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    912\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         output_attentions,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:404\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    402\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    403\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 404\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    413\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:348\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    337\u001b[0m         query_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    347\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mattn_output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 348\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(attn_output)\n\u001b[1;32m    351\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (attn_output, present)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pytorch_utils.py:119\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    118\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 119\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---- Cell 12: Full run + Save ----\n",
    "\n",
    "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"medmcqa_rag_e5_hybrid_{SCORING}_{ts}\"\n",
    "out_path_base = Path(OUT_DIR) / f\"{run_name}.base.jsonl\"\n",
    "out_path_rag  = Path(OUT_DIR) / f\"{run_name}.ragq.jsonl\"\n",
    "\n",
    "base_records = eval_mode(dev_samples, \"base\", ctx_max_chars=CTX_MAX_CHARS)\n",
    "ragq_records = eval_mode(dev_samples, \"rag_q\", ctx_max_chars=CTX_MAX_CHARS)\n",
    "\n",
    "base_sum = summarize(base_records)\n",
    "rag_sum  = summarize(ragq_records)\n",
    "fh = fix_hurt(base_records, ragq_records)\n",
    "\n",
    "print(\"\\n==== RESULTS ====\")\n",
    "print(\"Base ACC:\", base_sum[\"acc\"], \"MacroF1:\", base_sum[\"macro_f1\"], \"WeightedF1:\", base_sum[\"weighted_f1\"])\n",
    "print(\"RAGQ ACC:\", rag_sum[\"acc\"],  \"MacroF1:\", rag_sum[\"macro_f1\"],  \"WeightedF1:\", rag_sum[\"weighted_f1\"])\n",
    "print(\"Fix/Hurt:\", fh)\n",
    "\n",
    "# Save JSONL\n",
    "with out_path_base.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in base_records:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with out_path_rag.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in ragq_records:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", out_path_base)\n",
    "print(\" -\", out_path_rag)\n",
    "\n",
    "print(\"\\nConfusion Matrix (rows=Gold, cols=Pred) RAGQ:\")\n",
    "print(rag_sum[\"cm\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db83f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING = \"option_logprob\"\n",
    "CTX_MAX_CHARS = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fee468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:base: 100%|██████████| 4183/4183 [02:06<00:00, 33.13it/s]\n",
      "eval:rag_q:  38%|███▊      | 1604/4183 [16:46<20:03,  2.14it/s]  "
     ]
    }
   ],
   "source": [
    "base_records = eval_mode(dev_samples, \"base\", ctx_max_chars=600)\n",
    "ragq_records = eval_mode(dev_samples, \"rag_q\", ctx_max_chars=600)\n",
    "\n",
    "print(\"Base ACC:\", summarize(base_records)[\"acc\"])\n",
    "print(\"RAGQ ACC:\", summarize(ragq_records)[\"acc\"])\n",
    "print(\"Fix/Hurt:\", fix_hurt(base_records, ragq_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a6772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SIM_FOR_ANS = 0.35   # 或 0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9654113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:rag_q: 100%|██████████| 4183/4183 [43:14<00:00,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGQ ACC: 0.31173798709060485\n",
      "Fix/Hurt: {'fix': 326, 'hurt': 225, 'same_correct': 978, 'same_wrong': 2654, 'n': 4183}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ragq_035 = eval_mode(dev_samples, \"rag_q\", ctx_max_chars=600)\n",
    "print(\"RAGQ ACC:\", summarize(ragq_035)[\"acc\"])\n",
    "print(\"Fix/Hurt:\", fix_hurt(base_records, ragq_035))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e276938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_evidence(question: str, hit_ids,\n",
    "                   max_items=3, max_chars=600,\n",
    "                   min_sim_for_A=0.22):\n",
    "    blocks = []\n",
    "    for did in hit_ids[:max_items]:\n",
    "        item = kb_docs[did]\n",
    "        sim = near_dup_score(question, item[\"q\"])\n",
    "        if sim >= min_sim_for_A:\n",
    "            blocks.append(f'Q: {item[\"q\"]}\\nA: {item[\"a\"]}')\n",
    "        else:\n",
    "            blocks.append(f'Related Q: {item[\"q\"]}')\n",
    "    ev = \"\\n\\n\".join(blocks)\n",
    "    return ev[:max_chars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f995c834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice token ids: {'A': [32, 317], 'B': [33, 347], 'C': [34, 327], 'D': [35, 360]}\n"
     ]
    }
   ],
   "source": [
    "# ---- Cell 9: Prompt + Scoring ----\n",
    "\n",
    "def build_prompt_base(question: str, opts: Dict[str, str]) -> str:\n",
    "    # Keep prompt minimal (too long hurts small models)\n",
    "    return (\n",
    "        \"Choose the correct option (A, B, C, or D).\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"A) {opts['A']}\\n\"\n",
    "        f\"B) {opts['B']}\\n\"\n",
    "        f\"C) {opts['C']}\\n\"\n",
    "        f\"D) {opts['D']}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "def build_prompt_rag(question: str, opts: Dict[str, str], evidence: str) -> str:\n",
    "    return (\n",
    "        \"Use the evidence to answer. Choose A, B, C, or D.\\n\\n\"\n",
    "        f\"Evidence:\\n{evidence}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"A) {opts['A']}\\n\"\n",
    "        f\"B) {opts['B']}\\n\"\n",
    "        f\"C) {opts['C']}\\n\"\n",
    "        f\"D) {opts['D']}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "def _choice_token_ids(tok, ch: str) -> List[int]:\n",
    "    # Score multiple possible tokenizations\n",
    "    candidates = [ch, \" \" + ch, \"\\n\" + ch]\n",
    "    ids = []\n",
    "    for s in candidates:\n",
    "        t = tok.encode(s, add_special_tokens=False)\n",
    "        if len(t) == 1:\n",
    "            ids.append(t[0])\n",
    "    return list(dict.fromkeys(ids))  # unique\n",
    "\n",
    "CHOICE_TOKEN_IDS = {c: _choice_token_ids(mcq_tok, c) for c in [\"A\",\"B\",\"C\",\"D\"]}\n",
    "print(\"Choice token ids:\", CHOICE_TOKEN_IDS)\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_letter_logits(prompt: str) -> Dict[str, float]:\n",
    "    t = mcq_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    t = {k: v.to(DEVICE) for k, v in t.items()}\n",
    "    out = mcq_model(**t)\n",
    "    logits = out.logits[0, -1]  # next-token logits\n",
    "    scores = {}\n",
    "    for c in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "        ids = CHOICE_TOKEN_IDS[c]\n",
    "        if not ids:\n",
    "            scores[c] = -1e9\n",
    "        else:\n",
    "            scores[c] = float(torch.max(logits[ids]).item())\n",
    "    return scores\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_option_logprob(prompt: str, option_text: str, max_len: int = 512) -> float:\n",
    "    # logP(option_text | prompt)\n",
    "    full = prompt + \" \" + option_text\n",
    "    tok_full = mcq_tok(full, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    tok_full = {k:v.to(DEVICE) for k,v in tok_full.items()}\n",
    "    input_ids = tok_full[\"input_ids\"]\n",
    "    attn = tok_full[\"attention_mask\"]\n",
    "\n",
    "    tok_p = mcq_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    prompt_len = tok_p[\"input_ids\"].shape[1]\n",
    "\n",
    "    out = mcq_model(input_ids=input_ids, attention_mask=attn)\n",
    "    logits = out.logits  # [1,T,V]\n",
    "    targets = input_ids[:, prompt_len:]\n",
    "    if targets.numel() == 0:\n",
    "        return -1e9\n",
    "\n",
    "    logp = 0.0\n",
    "    for j in range(targets.shape[1]):\n",
    "        tpos = prompt_len + j\n",
    "        if tpos == 0:\n",
    "            continue\n",
    "        lp = F.log_softmax(logits[0, tpos-1], dim=-1)[targets[0, j]].item()\n",
    "        logp += lp\n",
    "    return float(logp)\n",
    "\n",
    "def predict_one(sample: dict, mode: str, ctx_max_chars: int = CTX_MAX_CHARS) -> dict:\n",
    "    q = sample[\"question\"]\n",
    "    opts = sample[\"options\"]\n",
    "    gold = sample[\"gold\"]\n",
    "\n",
    "    if mode == \"base\":\n",
    "        prompt = build_prompt_base(q, opts)\n",
    "        evidence = \"\"\n",
    "    elif mode == \"rag_q\":\n",
    "        cand = retrieve_topk_hybrid(q)\n",
    "        hit = rerank_and_filter(q, cand)\n",
    "        evidence = build_evidence(q, hit, max_items=EVID_MAX_ITEMS, max_chars=ctx_max_chars, min_sim_for_A=MIN_SIM_FOR_ANS)\n",
    "        prompt = build_prompt_rag(q, opts, evidence)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'base' or 'rag_q'\")\n",
    "\n",
    "    if SCORING == \"letter\":\n",
    "        scores = score_letter_logits(prompt)\n",
    "        pred = max(scores, key=scores.get)\n",
    "    elif SCORING == \"option_logprob\":\n",
    "        scores = {c: score_option_logprob(prompt, opts[c]) for c in [\"A\",\"B\",\"C\",\"D\"]}\n",
    "        pred = max(scores, key=scores.get)\n",
    "    else:\n",
    "        raise ValueError(\"SCORING must be 'letter' or 'option_logprob'\")\n",
    "\n",
    "    return {\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"gold\": gold,\n",
    "        \"pred\": pred,\n",
    "        \"scores\": scores,\n",
    "        \"rag_context\": evidence,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a62ed729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:base: 100%|██████████| 500/500 [00:14<00:00, 33.72it/s]\n",
      "eval:rag_q:   0%|          | 2/500 [00:01<06:35,  1.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m cands:\n\u001b[0;32m---> 17\u001b[0m     acc, fh \u001b[38;5;241m=\u001b[39m \u001b[43meval_ragq_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_500\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_max_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((th, acc, fh[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfix\u001b[39m\u001b[38;5;124m\"\u001b[39m], fh[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhurt\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mth\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  fix=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfh[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfix\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  hurt=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfh[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhurt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m, in \u001b[0;36meval_ragq_with_params\u001b[0;34m(samples, base_records_subset, min_sim_for_ans, ctx_max_chars, limit)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m MIN_SIM_FOR_ANS\n\u001b[1;32m      4\u001b[0m MIN_SIM_FOR_ANS \u001b[38;5;241m=\u001b[39m min_sim_for_ans\n\u001b[0;32m----> 6\u001b[0m rag \u001b[38;5;241m=\u001b[39m \u001b[43meval_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrag_q\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_max_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx_max_chars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m acc \u001b[38;5;241m=\u001b[39m summarize(rag)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m fh \u001b[38;5;241m=\u001b[39m fix_hurt(base_records_subset, rag)\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36meval_mode\u001b[0;34m(samples, mode, ctx_max_chars, limit)\u001b[0m\n\u001b[1;32m      5\u001b[0m it \u001b[38;5;241m=\u001b[39m samples[:limit] \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;28;01melse\u001b[39;00m samples\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m tqdm(it, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpredict_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_max_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx_max_chars\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[15], line 91\u001b[0m, in \u001b[0;36mpredict_one\u001b[0;34m(sample, mode, ctx_max_chars)\u001b[0m\n\u001b[1;32m     89\u001b[0m     evidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_q\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     cand \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_topk_hybrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     hit \u001b[38;5;241m=\u001b[39m rerank_and_filter(q, cand)\n\u001b[1;32m     93\u001b[0m     evidence \u001b[38;5;241m=\u001b[39m build_evidence(q, hit, max_items\u001b[38;5;241m=\u001b[39mEVID_MAX_ITEMS, max_chars\u001b[38;5;241m=\u001b[39mctx_max_chars, min_sim_for_A\u001b[38;5;241m=\u001b[39mMIN_SIM_FOR_ANS)\n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mretrieve_topk_hybrid\u001b[0;34m(question, k_vec, k_final, alpha)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# BM25 scores over all, slice to candidates\u001b[39;00m\n\u001b[1;32m     27\u001b[0m qtok \u001b[38;5;241m=\u001b[39m bm25_tokenize(question)\n\u001b[0;32m---> 28\u001b[0m bm25_scores_all \u001b[38;5;241m=\u001b[39m \u001b[43mbm25\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqtok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m bm25_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([bm25_scores_all[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m cand_ids], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     30\u001b[0m dense_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(dense_scores, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/rank_bm25.py:118\u001b[0m, in \u001b[0;36mBM25Okapi.get_scores\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    116\u001b[0m doc_len \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_len)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[0;32m--> 118\u001b[0m     q_freq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([(doc\u001b[38;5;241m.\u001b[39mget(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_freqs])\n\u001b[1;32m    119\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf\u001b[38;5;241m.\u001b[39mget(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m (q_freq \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\n\u001b[1;32m    120\u001b[0m                                        (q_freq \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m*\u001b[39m doc_len \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgdl)))\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/rank_bm25.py:118\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m doc_len \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_len)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[0;32m--> 118\u001b[0m     q_freq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_freqs])\n\u001b[1;32m    119\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf\u001b[38;5;241m.\u001b[39mget(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m (q_freq \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\n\u001b[1;32m    120\u001b[0m                                        (q_freq \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m*\u001b[39m doc_len \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgdl)))\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def eval_ragq_with_params(samples, base_records_subset, min_sim_for_ans, ctx_max_chars=600, limit=500):\n",
    "    # 临时改全局阈值（predict_one 会读它）\n",
    "    global MIN_SIM_FOR_ANS\n",
    "    MIN_SIM_FOR_ANS = min_sim_for_ans\n",
    "\n",
    "    rag = eval_mode(samples, \"rag_q\", ctx_max_chars=ctx_max_chars, limit=limit)\n",
    "    acc = summarize(rag)[\"acc\"]\n",
    "    fh = fix_hurt(base_records_subset, rag)\n",
    "    return acc, fh\n",
    "\n",
    "limit = 500\n",
    "base_500 = eval_mode(dev_samples, \"base\", ctx_max_chars=600, limit=limit)\n",
    "\n",
    "cands = [0.22, 0.28, 0.32, 0.35, 0.38, 0.42]\n",
    "results = []\n",
    "for th in cands:\n",
    "    acc, fh = eval_ragq_with_params(dev_samples, base_500, th, ctx_max_chars=600, limit=limit)\n",
    "    results.append((th, acc, fh[\"fix\"], fh[\"hurt\"]))\n",
    "    print(f\"th={th:.2f}  acc={acc:.4f}  fix={fh['fix']}  hurt={fh['hurt']}\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ca488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:rag_q: 100%|██████████| 4183/4183 [42:38<00:00,  1.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGQ(0.28) ACC: 0.31197704996414055\n",
      "Fix/Hurt: {'fix': 310, 'hurt': 208, 'same_correct': 995, 'same_wrong': 2670, 'n': 4183}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MIN_SIM_FOR_ANS = 0.28\n",
    "\n",
    "ragq_records_028 = eval_mode(dev_samples, \"rag_q\", ctx_max_chars=600)\n",
    "print(\"RAGQ(0.28) ACC:\", summarize(ragq_records_028)[\"acc\"])\n",
    "print(\"Fix/Hurt:\", fix_hurt(base_records, ragq_records_028))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9052c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:base:   0%|          | 0/4183 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:base: 100%|██████████| 4183/4183 [02:22<00:00, 29.34it/s]\n",
      "eval:rag_q: 100%|██████████| 4183/4183 [36:05<00:00,  1.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base ACC: 0.2878316997370308\n",
      "RAGQ(0.28) ACC: 0.31125986134353334\n",
      "Fix/Hurt: {'fix': 305, 'hurt': 207, 'same_correct': 997, 'same_wrong': 2674, 'n': 4183}\n",
      "[Warn] save_real_records failed -> fallback save. Error: NameError: name 'save_real_records' is not defined\n",
      "[Warn] save_real_records failed -> fallback save. Error: NameError: name 'save_real_records' is not defined\n",
      "\n",
      "[Saved] base: {'jsonl': 'eval_out_notebook/ragq_threshold_sweeps/base_closedbook_20251223_135115.jsonl', 'csv': 'eval_out_notebook/ragq_threshold_sweeps/base_closedbook_20251223_135115.csv', 'summary': 'eval_out_notebook/ragq_threshold_sweeps/base_closedbook_20251223_135115_summary.json'}\n",
      "[Saved] ragq: {'jsonl': 'eval_out_notebook/ragq_threshold_sweeps/ragq_sim028_20251223_135115.jsonl', 'csv': 'eval_out_notebook/ragq_threshold_sweeps/ragq_sim028_20251223_135115.csv', 'summary': 'eval_out_notebook/ragq_threshold_sweeps/ragq_sim028_20251223_135115_summary.json'}\n",
      "\n",
      "Gain: +0.0234\n"
     ]
    }
   ],
   "source": [
    "MIN_SIM_FOR_ANS = 0.28\n",
    "\n",
    "# 0) 如果你的 eval_mode 里是直接用全局变量 MIN_SIM_FOR_ANS，这句能保证它读到最新值\n",
    "globals()[\"MIN_SIM_FOR_ANS\"] = MIN_SIM_FOR_ANS\n",
    "\n",
    "import os\n",
    "import json\n",
    "import inspect\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _call_eval_mode(samples, mode: str, ctx_max_chars: int, min_sim: float):\n",
    "    \"\"\"\n",
    "    兼容不同 eval_mode 签名：\n",
    "    - 有的写 eval_mode(samples, mode, ctx_max_chars=..., min_sim_for_ans=...)\n",
    "    - 有的写 eval_mode(samples, mode, ctx_max_chars=..., MIN_SIM_FOR_ANS=...)\n",
    "    - 有的根本不收阈值参数，只能靠全局 MIN_SIM_FOR_ANS\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(eval_mode)\n",
    "    kw = {}\n",
    "    if \"ctx_max_chars\" in sig.parameters:\n",
    "        kw[\"ctx_max_chars\"] = ctx_max_chars\n",
    "\n",
    "    # 尝试把阈值作为参数传进去（优先常见名字）\n",
    "    for name in [\"min_sim_for_ans\", \"min_sim\", \"sim_threshold\", \"MIN_SIM_FOR_ANS\"]:\n",
    "        if name in sig.parameters:\n",
    "            kw[name] = min_sim\n",
    "            break\n",
    "\n",
    "    return eval_mode(samples, mode, **kw)\n",
    "\n",
    "\n",
    "def _jsonable(x):\n",
    "    \"\"\"把 numpy/torch 等对象转成 json 可序列化类型。\"\"\"\n",
    "    # numpy 标量\n",
    "    if isinstance(x, (np.integer, np.int64, np.int32)):\n",
    "        return int(x)\n",
    "    if isinstance(x, (np.floating, np.float32, np.float64)):\n",
    "        return float(x)\n",
    "    if isinstance(x, (np.bool_,)):\n",
    "        return bool(x)\n",
    "    # numpy 数组\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "\n",
    "    # torch tensor（如果你 record 里塞了）\n",
    "    try:\n",
    "        import torch\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return x.detach().cpu().tolist()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # dict / list 递归\n",
    "    if isinstance(x, dict):\n",
    "        return {str(k): _jsonable(v) for k, v in x.items()}\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [_jsonable(v) for v in x]\n",
    "\n",
    "    # 其它：保持原样（json dump 失败时会再抛）\n",
    "    return x\n",
    "\n",
    "\n",
    "def _fallback_save(records, out_dir: str, prefix: str, extra_meta: dict | None = None):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base = f\"{prefix}_{ts}\"\n",
    "\n",
    "    jsonl_path = os.path.join(out_dir, base + \".jsonl\")\n",
    "    csv_path   = os.path.join(out_dir, base + \".csv\")\n",
    "    sum_path   = os.path.join(out_dir, base + \"_summary.json\")\n",
    "\n",
    "    # jsonl\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(_jsonable(r), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # csv（尽量展开）\n",
    "    try:\n",
    "        df = pd.DataFrame([_jsonable(r) for r in records])\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    except Exception:\n",
    "        csv_path = None\n",
    "\n",
    "    # summary\n",
    "    summ = summarize(records) if callable(summarize) else {}\n",
    "    payload = {\"summary\": _jsonable(summ), \"meta\": _jsonable(extra_meta or {})}\n",
    "    with open(sum_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return {\"jsonl\": jsonl_path, \"csv\": csv_path, \"summary\": sum_path}\n",
    "\n",
    "\n",
    "def safe_save_real_records(records, out_dir: str, prefix: str, extra_meta: dict | None = None):\n",
    "    \"\"\"\n",
    "    先用你原来的 save_real_records；\n",
    "    如果炸（最常见就是 ndarray 不可序列化），就自动 fallback 保存。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return save_real_records(records, out_dir=out_dir, prefix=prefix, extra_meta=extra_meta)\n",
    "    except Exception as e:\n",
    "        print(f\"[Warn] save_real_records failed -> fallback save. Error: {type(e).__name__}: {str(e)[:200]}\")\n",
    "        return _fallback_save(records, out_dir=out_dir, prefix=prefix, extra_meta=extra_meta)\n",
    "\n",
    "\n",
    "# 1) 先算 base（闭卷）\n",
    "base_records = _call_eval_mode(dev_samples, \"base\", ctx_max_chars=600, min_sim=MIN_SIM_FOR_ANS)\n",
    "\n",
    "# 2) 再算 rag_q（检索 + 阈值）\n",
    "ragq_records_028 = _call_eval_mode(dev_samples, \"rag_q\", ctx_max_chars=600, min_sim=MIN_SIM_FOR_ANS)\n",
    "\n",
    "# 3) 打印指标\n",
    "base_acc = summarize(base_records)[\"acc\"]\n",
    "rag_acc  = summarize(ragq_records_028)[\"acc\"]\n",
    "fix_hurt_stats = fix_hurt(base_records, ragq_records_028)\n",
    "\n",
    "print(\"Base ACC:\", base_acc)\n",
    "print(f\"RAGQ({MIN_SIM_FOR_ANS}) ACC:\", rag_acc)\n",
    "print(\"Fix/Hurt:\", fix_hurt_stats)\n",
    "\n",
    "# 4) 保存结果（同时保存 base + ragq）\n",
    "meta = {\"MIN_SIM_FOR_ANS\": float(MIN_SIM_FOR_ANS), \"mode\": \"rag_q\", \"ctx_max_chars\": 600}\n",
    "\n",
    "save_base = safe_save_real_records(\n",
    "    base_records,\n",
    "    out_dir=\"eval_out_notebook/ragq_threshold_sweeps\",\n",
    "    prefix=\"base_closedbook\",\n",
    "    extra_meta={**meta, \"mode\": \"base\"}\n",
    ")\n",
    "\n",
    "save_ragq = safe_save_real_records(\n",
    "    ragq_records_028,\n",
    "    out_dir=\"eval_out_notebook/ragq_threshold_sweeps\",\n",
    "    prefix=f\"ragq_sim{str(MIN_SIM_FOR_ANS).replace('.','')}\",\n",
    "    extra_meta=meta\n",
    ")\n",
    "\n",
    "print(\"\\n[Saved] base:\", save_base)\n",
    "print(\"[Saved] ragq:\", save_ragq)\n",
    "print(f\"\\nGain: {rag_acc - base_acc:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32fdf9b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type ndarray is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msave_eval_records\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mragq_records_028\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_out_notebook/ragq_threshold_sweeps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mragq_sim028\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMIN_SIM_FOR_ANS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMIN_SIM_FOR_ANS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrag_q\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mctx_max_chars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 58\u001b[0m, in \u001b[0;36msave_eval_records\u001b[0;34m(records, out_dir, prefix, extra_meta)\u001b[0m\n\u001b[1;32m     55\u001b[0m     summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_meta\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(sum_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Saved]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  JSONL :\u001b[39m\u001b[38;5;124m\"\u001b[39m, jsonl_path)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type ndarray is not JSON serializable"
     ]
    }
   ],
   "source": [
    "save_eval_records(\n",
    "    ragq_records_028,\n",
    "    out_dir=\"eval_out_notebook/ragq_threshold_sweeps\",\n",
    "    prefix=\"ragq_sim028\",\n",
    "    extra_meta={\"MIN_SIM_FOR_ANS\": MIN_SIM_FOR_ANS, \"mode\": \"rag_q\", \"ctx_max_chars\": 600}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53905fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9e859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
