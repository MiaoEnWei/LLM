{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8417f5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "OUT_DIR: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/eval_out/medqa_train_letter_rag\n",
      "FT_OUT_DIR: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/out_gpt2_medqa_rag_letter\n",
      "KB: medalpaca/medical_meadow_wikidoc MAX_DOCS: 200000\n",
      "RAG: K= 2 tau= 0.25 P_RAG_TRAIN= 0.5\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, pickle, random, math\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForCausalLM,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"缺少 faiss：pip install faiss-cpu 或 faiss-gpu\") from e\n",
    "\n",
    "import torch\n",
    "\n",
    "# 给所有 torch 优化器补上 train/eval（某些 transformers 版本会调用）\n",
    "if not hasattr(torch.optim.Optimizer, \"train\"):\n",
    "    torch.optim.Optimizer.train = lambda self: None\n",
    "if not hasattr(torch.optim.Optimizer, \"eval\"):\n",
    "    torch.optim.Optimizer.eval = lambda self: None\n",
    "\n",
    "\n",
    "# ========= 路径 =========\n",
    "BASE_DIR = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM\"\n",
    "OUT_DIR  = f\"{BASE_DIR}/eval_out/medqa_train_letter_rag\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 训练输出目录（新模型）\n",
    "FT_OUT_DIR = f\"{BASE_DIR}/out_gpt2_medqa_rag_letter\"\n",
    "os.makedirs(FT_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ========= 两个模型（对比用） =========\n",
    "MODEL_BASE = f\"{BASE_DIR}/gpt2\"\n",
    "MODEL_OLD_FT = f\"{BASE_DIR}/gpt2-medmcqa-raft-masked\"  # 旧的（跨数据集会掉点正常）\n",
    "# 你将训练的新模型：FT_OUT_DIR\n",
    "\n",
    "# ========= Embedding 模型（建库/检索一致） =========\n",
    "EMB_MODEL = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "# ========= KB（WikiDoc） =========\n",
    "KB_NAME  = \"medalpaca/medical_meadow_wikidoc\"\n",
    "MAX_DOCS = 200000  # 先 20w（你机器够再加）\n",
    "SAVE_DIR = f\"{BASE_DIR}/rag_cache/medqa_{KB_NAME.replace('/','_')}_{MAX_DOCS}\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "RAG_INDEX_PATH = f\"{SAVE_DIR}/kb.index\"\n",
    "RAG_DOCS_PATH  = f\"{SAVE_DIR}/docs.pkl\"\n",
    "\n",
    "# ========= 设备/随机种子 =========\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ========= Prompt/Token 上限 =========\n",
    "MAX_INPUT_TOKENS = 512   # 和你之前保持一致\n",
    "EVID_MAX_CHARS   = 180   # 证据更短更稳\n",
    "EVID_KEEP_SENTS  = 1\n",
    "\n",
    "# ========= RAG 参数 =========\n",
    "RAG_K = 2               # question-only: top2\n",
    "RAG_TAU = 0.25          # 门控阈值（后续可以扫 0.2/0.25/0.3）\n",
    "P_RAG_TRAIN = 0.5       # 训练样本中，有证据比例（关键！）\n",
    "\n",
    "# ========= 训练参数（GPT2 很小，别训太猛） =========\n",
    "TRAIN_MAX = None        # 想快速试跑：比如 20000\n",
    "VAL_SIZE = 2000         # 从 train 切出验证集\n",
    "EPOCHS = 1              # 建议先 1，再看趋势\n",
    "LR = 5e-5               # 小一点更稳\n",
    "TRAIN_BS = 8\n",
    "EVAL_BS  = 16\n",
    "GRAD_ACC = 4\n",
    "WARMUP_RATIO = 0.03\n",
    "\n",
    "CHOICE_LETTERS = [\"A\",\"B\",\"C\",\"D\"]\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "print(\"FT_OUT_DIR:\", FT_OUT_DIR)\n",
    "print(\"KB:\", KB_NAME, \"MAX_DOCS:\", MAX_DOCS)\n",
    "print(\"RAG: K=\", RAG_K, \"tau=\", RAG_TAU, \"P_RAG_TRAIN=\", P_RAG_TRAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd3ac3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: dict_keys(['train', 'test'])\n",
      "Train example keys: dict_keys(['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'])\n",
      "Train size: 8178 Val size: 2000 Test size: 1273\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
    "print(\"Splits:\", ds.keys())\n",
    "print(\"Train example keys:\", ds[\"train\"][0].keys())\n",
    "\n",
    "def _label_from_any(x):\n",
    "    if x is None: \n",
    "        return None\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        v = int(x)\n",
    "        if 0 <= v <= 3: return v\n",
    "        if 1 <= v <= 4: return v-1\n",
    "        return None\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().upper()\n",
    "        if s in CHOICE_LETTERS: return CHOICE_LETTERS.index(s)\n",
    "        if s.isdigit():\n",
    "            v = int(s)\n",
    "            if 0 <= v <= 3: return v\n",
    "            if 1 <= v <= 4: return v-1\n",
    "    return None\n",
    "\n",
    "def normalize_medqa_example(ex: Dict[str, Any], idx: int) -> Dict[str, Any]:\n",
    "    q = ex[\"question\"]\n",
    "    od = ex[\"options\"]\n",
    "    A,B,C,D = od.get(\"A\",\"\"), od.get(\"B\",\"\"), od.get(\"C\",\"\"), od.get(\"D\",\"\")\n",
    "    y = _label_from_any(ex.get(\"answer_idx\", None))\n",
    "    if y is None:\n",
    "        y = _label_from_any(ex.get(\"answer\", None))\n",
    "    if y is None:\n",
    "        raise KeyError(f\"无法解析 label：keys={list(ex.keys())[:20]}\")\n",
    "    uid = ex.get(\"id\", ex.get(\"qid\", ex.get(\"question_id\", f\"idx-{idx}\")))\n",
    "    return {\"id\": str(uid), \"question\": q, \"opa\": A, \"opb\": B, \"opc\": C, \"opd\": D, \"label\": int(y)}\n",
    "\n",
    "# 生成 train/val（MedQA 只有 train/test，这里从 train 切 val）\n",
    "train_raw = ds[\"train\"]\n",
    "idxs = list(range(len(train_raw)))\n",
    "random.shuffle(idxs)\n",
    "\n",
    "val_idxs = idxs[:VAL_SIZE]\n",
    "trn_idxs = idxs[VAL_SIZE:]\n",
    "\n",
    "if TRAIN_MAX is not None:\n",
    "    trn_idxs = trn_idxs[:TRAIN_MAX]\n",
    "\n",
    "print(\"Train size:\", len(trn_idxs), \"Val size:\", len(val_idxs), \"Test size:\", len(ds[\"test\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a914e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SENT_SPLIT = re.compile(r'(?<=[\\.\\?\\!])\\s+|\\n+')\n",
    "\n",
    "def _normalize(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (text or \"\").strip())\n",
    "\n",
    "def _tokenize(text: str):\n",
    "    text = (text or \"\").lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s\\-]\", \" \", text)\n",
    "    return [t for t in text.split() if len(t) >= 3]\n",
    "\n",
    "def _jaccard(a, b):\n",
    "    sa, sb = set(a), set(b)\n",
    "    if not sa or not sb: return 0.0\n",
    "    return len(sa & sb) / max(1, len(sa | sb))\n",
    "\n",
    "def compress_context_by_overlap(question: str, options: List[str], context: str,\n",
    "                                keep_sents: int=1, max_chars: int=180, min_sent_chars: int=30) -> str:\n",
    "    ctx = _normalize(context)\n",
    "    if not ctx: return \"\"\n",
    "\n",
    "    query = _normalize(question) + \" \" + \" \".join([_normalize(x) for x in options if x])\n",
    "    q_toks = _tokenize(query)\n",
    "    if not q_toks:\n",
    "        return ctx[:max_chars] if len(ctx) > max_chars else ctx\n",
    "\n",
    "    sents = [s.strip() for s in _SENT_SPLIT.split(ctx) if s and len(s.strip()) >= min_sent_chars]\n",
    "    if not sents:\n",
    "        return ctx[:max_chars] if len(ctx) > max_chars else ctx\n",
    "\n",
    "    scored = []\n",
    "    for s in sents:\n",
    "        s_toks = _tokenize(s)\n",
    "        if not s_toks:\n",
    "            continue\n",
    "        jac = _jaccard(q_toks, s_toks)\n",
    "        overlap = len(set(q_toks) & set(s_toks))\n",
    "        score = jac * 2.0 + overlap * 0.05\n",
    "        scored.append((score, s))\n",
    "\n",
    "    if not scored:\n",
    "        return ctx[:max_chars] if len(ctx) > max_chars else ctx\n",
    "\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    picked = [s for _, s in scored[:keep_sents]]\n",
    "\n",
    "    # 保持原顺序\n",
    "    order = {s: i for i, s in enumerate(sents)}\n",
    "    picked.sort(key=lambda s: order.get(s, 10**9))\n",
    "\n",
    "    evidence = _normalize(\" \".join(picked))\n",
    "    return evidence[:max_chars] if len(evidence) > max_chars else evidence\n",
    "\n",
    "def extract_text_from_kb_example(ex: Dict[str, Any]) -> str:\n",
    "    # 尽量从常见字段取正文\n",
    "    for k in [\"text\",\"content\",\"passage\",\"snippet\",\"abstract\",\"output\"]:\n",
    "        if k in ex and isinstance(ex[k], str) and ex[k].strip():\n",
    "            return ex[k].strip()\n",
    "    parts = []\n",
    "    for k in [\"title\",\"instruction\",\"input\",\"question\"]:\n",
    "        if k in ex and isinstance(ex[k], str) and ex[k].strip():\n",
    "            parts.append(ex[k].strip())\n",
    "    for k in [\"output\",\"answer\",\"context\"]:\n",
    "        if k in ex and isinstance(ex[k], str) and ex[k].strip():\n",
    "            parts.append(ex[k].strip())\n",
    "    return _normalize(\" \".join(parts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1b4620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现 KB 缓存，跳过建库。\n",
      "KB loaded. docs: 10000 dim: 768\n"
     ]
    }
   ],
   "source": [
    "def build_kb_if_needed():\n",
    "    if os.path.exists(RAG_INDEX_PATH) and os.path.exists(RAG_DOCS_PATH):\n",
    "        print(\"发现 KB 缓存，跳过建库。\")\n",
    "        return\n",
    "\n",
    "    print(\"开始建 KB（首次会较久）...\")\n",
    "    emb_tok = AutoTokenizer.from_pretrained(EMB_MODEL, use_fast=True)\n",
    "    emb_model = AutoModel.from_pretrained(EMB_MODEL).to(DEVICE)\n",
    "    emb_model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_texts(texts: List[str], batch_size: int=64) -> np.ndarray:\n",
    "        all_vecs = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = emb_tok(batch, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "            enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "            out = emb_model(**enc).last_hidden_state\n",
    "            mask = enc[\"attention_mask\"].unsqueeze(-1).float()\n",
    "            pooled = (out * mask).sum(dim=1) / torch.clamp(mask.sum(dim=1), min=1e-6)\n",
    "            vec = pooled.detach().cpu().numpy().astype(\"float32\")\n",
    "            vec /= (np.linalg.norm(vec, axis=1, keepdims=True) + 1e-12)\n",
    "            all_vecs.append(vec)\n",
    "        return np.concatenate(all_vecs, axis=0)\n",
    "\n",
    "    dim = 768\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    docs_text = []\n",
    "    buf = []\n",
    "    BATCH = 64\n",
    "\n",
    "    stream_ds = load_dataset(KB_NAME, split=\"train\", streaming=True)\n",
    "\n",
    "    cnt = 0\n",
    "    for ex in stream_ds:\n",
    "        text = extract_text_from_kb_example(ex)\n",
    "        text = _normalize(text)\n",
    "        if not text:\n",
    "            continue\n",
    "        docs_text.append(text)\n",
    "        buf.append(text)\n",
    "        cnt += 1\n",
    "\n",
    "        if len(buf) >= BATCH:\n",
    "            vecs = encode_texts(buf, batch_size=BATCH)\n",
    "            index.add(vecs)\n",
    "            buf = []\n",
    "\n",
    "        if cnt >= MAX_DOCS:\n",
    "            break\n",
    "\n",
    "    if buf:\n",
    "        vecs = encode_texts(buf, batch_size=BATCH)\n",
    "        index.add(vecs)\n",
    "\n",
    "    print(\"Built docs:\", len(docs_text), \"FAISS ntotal:\", index.ntotal)\n",
    "\n",
    "    faiss.write_index(index, RAG_INDEX_PATH)\n",
    "    with open(RAG_DOCS_PATH, \"wb\") as f:\n",
    "        pickle.dump(docs_text, f)\n",
    "\n",
    "    print(\"Saved index:\", RAG_INDEX_PATH)\n",
    "    print(\"Saved docs :\", RAG_DOCS_PATH)\n",
    "\n",
    "build_kb_if_needed()\n",
    "\n",
    "# --- 加载 KB ---\n",
    "index = faiss.read_index(RAG_INDEX_PATH)\n",
    "with open(RAG_DOCS_PATH, \"rb\") as f:\n",
    "    docs_text = pickle.load(f)\n",
    "print(\"KB loaded. docs:\", len(docs_text), \"dim:\", index.d)\n",
    "\n",
    "# --- 加载 embedding 模型（用于检索 query）---\n",
    "emb_tok = AutoTokenizer.from_pretrained(EMB_MODEL, use_fast=True)\n",
    "emb_model = AutoModel.from_pretrained(EMB_MODEL).to(DEVICE)\n",
    "emb_model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_query(text: str) -> np.ndarray:\n",
    "    enc = emb_tok([text], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "    out = emb_model(**enc).last_hidden_state\n",
    "    mask = enc[\"attention_mask\"].unsqueeze(-1).float()\n",
    "    pooled = (out * mask).sum(dim=1) / torch.clamp(mask.sum(dim=1), min=1e-6)\n",
    "    vec = pooled.detach().cpu().numpy().astype(\"float32\")\n",
    "    vec /= (np.linalg.norm(vec, axis=1, keepdims=True) + 1e-12)\n",
    "    return vec\n",
    "\n",
    "def retrieve_topk_with_scores(query: str, k: int=2):\n",
    "    qv = encode_query(query)\n",
    "    D, I = index.search(qv, k)\n",
    "    out = []\n",
    "    for s, j in zip(D[0].tolist(), I[0].tolist()):\n",
    "        if 0 <= j < len(docs_text):\n",
    "            out.append((float(s), docs_text[j]))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ac8506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单缓存：训练集很大，避免同一题多次 encode_query（尤其你调参时）\n",
    "_evidence_cache = {}\n",
    "\n",
    "def get_rag_evidence(ex: Dict[str, Any]) -> str:\n",
    "    q = _normalize(ex[\"question\"])\n",
    "    if q in _evidence_cache:\n",
    "        return _evidence_cache[q]\n",
    "\n",
    "    opts = [ex[\"opa\"], ex[\"opb\"], ex[\"opc\"], ex[\"opd\"]]\n",
    "    got = retrieve_topk_with_scores(q, k=RAG_K)\n",
    "\n",
    "    lines = []\n",
    "    for score, doc in got:\n",
    "        if score < RAG_TAU:\n",
    "            continue\n",
    "        raw = doc[:1200]\n",
    "        short = compress_context_by_overlap(q, opts, raw, keep_sents=EVID_KEEP_SENTS, max_chars=EVID_MAX_CHARS)\n",
    "        if short:\n",
    "            lines.append(short)\n",
    "\n",
    "    # 最多拼 1~2 条（这里建议 1 条更稳）\n",
    "    ev = _normalize(\" \".join(lines[:1]))\n",
    "    _evidence_cache[q] = ev\n",
    "    return ev\n",
    "\n",
    "def build_prompt_base_letter(ex: Dict[str, Any]) -> str:\n",
    "    q = ex[\"question\"]\n",
    "    A,B,C,D = ex[\"opa\"], ex[\"opb\"], ex[\"opc\"], ex[\"opd\"]\n",
    "    return (\n",
    "        \"You are a medical exam solver. Choose the single best option.\\n\"\n",
    "        \"Reply with ONLY one letter: A, B, C, or D.\\n\\n\"\n",
    "        f\"Question:\\n{q}\\n\\n\"\n",
    "        f\"Options:\\nA) {A}\\nB) {B}\\nC) {C}\\nD) {D}\\n\\n\"\n",
    "        \"Answer (A, B, C, or D):\"\n",
    "    )\n",
    "\n",
    "def build_prompt_rag_letter(ex: Dict[str, Any], tok) -> Tuple[str, str]:\n",
    "    base = build_prompt_base_letter(ex)\n",
    "    cut = \"Answer (A, B, C, or D):\"\n",
    "    assert base.endswith(cut)\n",
    "    prefix = base[:-len(cut)]\n",
    "\n",
    "    ev = get_rag_evidence(ex)\n",
    "    evidence = \"\"\n",
    "    if ev:\n",
    "        # 控制 evidence token budget：保证不会挤爆 MAX_INPUT_TOKENS\n",
    "        ev_prefix = \"Evidence:\\n\"\n",
    "        base_len = len(tok(base, add_special_tokens=False).input_ids)\n",
    "        overhead = len(tok(ev_prefix + \"\\n\", add_special_tokens=False).input_ids)\n",
    "        budget = max(0, MAX_INPUT_TOKENS - base_len - overhead - 8)\n",
    "        if budget > 0:\n",
    "            evid_ids = tok(ev, add_special_tokens=False).input_ids[:budget]\n",
    "            evidence = tok.decode(evid_ids)\n",
    "\n",
    "    ev_block = (f\"Evidence:\\n{evidence}\\n\\n\" if evidence else \"\")\n",
    "    prompt = prefix + ev_block + cut\n",
    "    return prompt, evidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80e12d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds: 8178 val_ds: 2000\n"
     ]
    }
   ],
   "source": [
    "def load_reader_tokenizer(model_path: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.truncation_side = \"left\"\n",
    "    tok.padding_side = \"left\"\n",
    "    return tok\n",
    "\n",
    "tok_train = load_reader_tokenizer(MODEL_BASE)\n",
    "\n",
    "def pack_one_example(ex: Dict[str, Any], use_rag: bool, tok) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    生成 (input_ids, attention_mask, labels)：\n",
    "    - prompt 部分 labels = -100（不算 loss）\n",
    "    - answer 字母部分 labels = token ids（算 loss）\n",
    "    \"\"\"\n",
    "    if not use_rag:\n",
    "        prompt = build_prompt_base_letter(ex)\n",
    "        evidence = \"\"\n",
    "    else:\n",
    "        prompt, evidence = build_prompt_rag_letter(ex, tok)\n",
    "\n",
    "    # 目标：只输出字母（训练时给 \" A\" 这种形式更稳）\n",
    "    y = ex[\"label\"]\n",
    "    target = \" \" + CHOICE_LETTERS[y]\n",
    "\n",
    "    prompt_ids = tok(prompt, add_special_tokens=False).input_ids\n",
    "    target_ids = tok(target, add_special_tokens=False).input_ids\n",
    "\n",
    "    # 保证答案不被截掉：只截 prompt（左截）\n",
    "    max_prompt_len = MAX_INPUT_TOKENS - len(target_ids)\n",
    "    if max_prompt_len < 1:\n",
    "        # 极端情况：目标都放不下\n",
    "        prompt_ids = prompt_ids[-1:]\n",
    "        max_prompt_len = MAX_INPUT_TOKENS - len(target_ids)\n",
    "\n",
    "    if len(prompt_ids) > max_prompt_len:\n",
    "        prompt_ids = prompt_ids[-max_prompt_len:]\n",
    "\n",
    "    input_ids = prompt_ids + target_ids\n",
    "    labels = [-100]*len(prompt_ids) + target_ids\n",
    "    attention_mask = [1]*len(input_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"use_rag\": int(use_rag),\n",
    "        \"evidence_preview\": evidence[:120] if evidence else \"\"\n",
    "    }\n",
    "\n",
    "class MedQARagTrainDataset(Dataset):\n",
    "    def __init__(self, raw_ds, indices: List[int], tok, p_rag: float):\n",
    "        self.raw_ds = raw_ds\n",
    "        self.indices = indices\n",
    "        self.tok = tok\n",
    "        self.p_rag = p_rag\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i]\n",
    "        ex = normalize_medqa_example(self.raw_ds[idx], idx=idx)\n",
    "        use_rag = (random.random() < self.p_rag)\n",
    "        item = pack_one_example(ex, use_rag=use_rag, tok=self.tok)\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch, pad_token_id: int):\n",
    "    # 左 padding（与 tok.padding_side 一致）\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    input_ids, attn, labels = [], [], []\n",
    "    use_rag = []\n",
    "    for x in batch:\n",
    "        L = len(x[\"input_ids\"])\n",
    "        pad = max_len - L\n",
    "        input_ids.append([pad_token_id]*pad + x[\"input_ids\"])\n",
    "        attn.append([0]*pad + x[\"attention_mask\"])\n",
    "        labels.append([-100]*pad + x[\"labels\"])\n",
    "        use_rag.append(x.get(\"use_rag\", 0))\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        \"use_rag\": torch.tensor(use_rag, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "train_ds = MedQARagTrainDataset(train_raw, trn_idxs, tok_train, p_rag=P_RAG_TRAIN)\n",
    "val_ds   = MedQARagTrainDataset(train_raw, val_idxs, tok_train, p_rag=P_RAG_TRAIN)\n",
    "\n",
    "print(\"train_ds:\", len(train_ds), \"val_ds:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4903ed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoen/.local/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 03:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/out_gpt2_medqa_rag_letter\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_BASE,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=None\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "model.train()\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=FT_OUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    logging_steps=50,\n",
    "    report_to=[],\n",
    "    dataloader_num_workers=0,\n",
    "\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=lambda b: collate_fn(b, pad_token_id=tok_train.pad_token_id),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(FT_OUT_DIR)\n",
    "tok_train.save_pretrained(FT_OUT_DIR)\n",
    "print(\"Saved to:\", FT_OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c0e48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt2_base | test | NO-RAG | letter_logits]\n",
      "total=1273 acc=0.2773 trunc_rate=0.0086 ev_nonempty=0/1273\n",
      "prompt_len_stats: {'mean': 261.9858601728201, 'p50': 250.0, 'p90': 363.0, 'p99': 499.51999999999975, 'max': 945}\n",
      " -> /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/eval_out/medqa_train_letter_rag/gpt2_base_test_no_rag_letter_20251216_175526.jsonl\n",
      "[gpt2_base | test | RAG | letter_logits]\n",
      "total=1273 acc=0.2773 trunc_rate=0.0086 ev_nonempty=1260/1273\n",
      "prompt_len_stats: {'mean': 296.53731343283584, 'p50': 284.0, 'p90': 399.0, 'p99': 506.0, 'max': 945}\n",
      " -> /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/eval_out/medqa_train_letter_rag/gpt2_base_test_rag_letter_20251216_175637.jsonl\n",
      "[gpt2_medqa_ft | test | NO-RAG | letter_logits]\n",
      "total=1273 acc=0.2584 trunc_rate=0.0086 ev_nonempty=0/1273\n",
      "prompt_len_stats: {'mean': 261.9858601728201, 'p50': 250.0, 'p90': 363.0, 'p99': 499.51999999999975, 'max': 945}\n",
      " -> /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/eval_out/medqa_train_letter_rag/gpt2_medqa_ft_test_no_rag_letter_20251216_175810.jsonl\n",
      "[gpt2_medqa_ft | test | RAG | letter_logits]\n",
      "total=1273 acc=0.2427 trunc_rate=0.0086 ev_nonempty=1260/1273\n",
      "prompt_len_stats: {'mean': 296.53731343283584, 'p50': 284.0, 'p90': 399.0, 'p99': 506.0, 'max': 945}\n",
      " -> /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/eval_out/medqa_train_letter_rag/gpt2_medqa_ft_test_rag_letter_20251216_175921.jsonl\n",
      "\n",
      "=== SUMMARY ===\n",
      "base  no-rag: 0.27729772191673213 rag: 0.27729772191673213 gain: 0.0\n",
      "newFT no-rag: 0.2584446190102121 rag: 0.24273369992144542 gain: -0.01571091908876668\n"
     ]
    }
   ],
   "source": [
    "def load_reader(model_path: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.truncation_side = \"left\"\n",
    "    tok.padding_side = \"left\"\n",
    "    m = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    m.eval()\n",
    "    return tok, m\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_continuation(tok, model, prompt_ids, cont_ids):\n",
    "    input_ids = torch.cat([prompt_ids, cont_ids], dim=1)\n",
    "    out = model(input_ids=input_ids)\n",
    "    logits = out.logits\n",
    "    prompt_len = prompt_ids.size(1)\n",
    "    lp = 0.0\n",
    "    for i in range(cont_ids.size(1)):\n",
    "        pos = prompt_len + i - 1\n",
    "        logp = F.log_softmax(logits[0, pos], dim=-1)\n",
    "        tid = cont_ids[0, i].item()\n",
    "        lp += float(logp[tid].item())\n",
    "    return lp\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_by_letter_logits(tok, model, prompt: str):\n",
    "    enc = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_TOKENS)\n",
    "    prompt_ids = enc[\"input_ids\"].to(model.device)\n",
    "\n",
    "    scores = []\n",
    "    for L in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "        c1 = tok.encode(\" \" + L, add_special_tokens=False)\n",
    "        c2 = tok.encode(L, add_special_tokens=False)\n",
    "        t1 = torch.tensor([c1], device=model.device, dtype=torch.long) if c1 else None\n",
    "        t2 = torch.tensor([c2], device=model.device, dtype=torch.long) if c2 else None\n",
    "        s1 = score_continuation(tok, model, prompt_ids, t1) if t1 is not None else -1e9\n",
    "        s2 = score_continuation(tok, model, prompt_ids, t2) if t2 is not None else -1e9\n",
    "        scores.append(max(s1, s2))\n",
    "    pred = int(np.argmax(scores))\n",
    "    return pred, scores\n",
    "\n",
    "def run_eval_letter(model_path: str, name: str, use_rag: bool, limit=None):\n",
    "    tok, model = load_reader(model_path)\n",
    "    test_ds = ds[\"test\"]\n",
    "    n = len(test_ds) if limit is None else min(limit, len(test_ds))\n",
    "\n",
    "    total=0; correct=0; trunc=0; lens=[]; ev_nonempty=0\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_jsonl = os.path.join(\n",
    "        OUT_DIR,\n",
    "        f\"{name}_test_{'rag' if use_rag else 'no_rag'}_letter_{ts}.jsonl\"\n",
    "    )\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as wf:\n",
    "        for i in range(n):\n",
    "            ex = normalize_medqa_example(test_ds[i], idx=i)\n",
    "\n",
    "            if not use_rag:\n",
    "                prompt = build_prompt_base_letter(ex)\n",
    "                evidence = \"\"\n",
    "            else:\n",
    "                prompt, evidence = build_prompt_rag_letter(ex, tok)\n",
    "\n",
    "            L = len(tok(prompt, add_special_tokens=False).input_ids)\n",
    "            lens.append(L)\n",
    "            if L > MAX_INPUT_TOKENS: trunc += 1\n",
    "            if (evidence or \"\").strip(): ev_nonempty += 1\n",
    "\n",
    "            pred, scores = predict_by_letter_logits(tok, model, prompt)\n",
    "            ok = (pred == ex[\"label\"])\n",
    "\n",
    "            total += 1\n",
    "            correct += int(ok)\n",
    "\n",
    "            wf.write(json.dumps({\n",
    "                \"idx\": i,\n",
    "                \"gt\": ex[\"label\"],\n",
    "                \"pred\": pred,\n",
    "                \"is_correct\": bool(ok),\n",
    "                \"use_rag\": bool(use_rag),\n",
    "                \"prompt_len\": L,\n",
    "                \"evidence_preview\": (evidence[:200] if evidence else \"\"),\n",
    "                \"scores\": scores,\n",
    "            }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    acc = correct/total\n",
    "    trunc_rate = trunc/total\n",
    "    arr = np.array(lens)\n",
    "    stats = {\"mean\": float(arr.mean()), \"p50\": float(np.percentile(arr,50)), \"p90\": float(np.percentile(arr,90)), \"p99\": float(np.percentile(arr,99)), \"max\": int(arr.max())}\n",
    "\n",
    "    print(f\"[{name} | test | {'RAG' if use_rag else 'NO-RAG'} | letter_logits]\")\n",
    "    print(f\"total={total} acc={acc:.4f} trunc_rate={trunc_rate:.4f} ev_nonempty={ev_nonempty}/{total}\")\n",
    "    print(\"prompt_len_stats:\", stats)\n",
    "    print(\" ->\", out_jsonl)\n",
    "    return acc\n",
    "\n",
    "# ========== 4 组对比 ==========\n",
    "acc_base_no = run_eval_letter(MODEL_BASE, \"gpt2_base\", use_rag=False)\n",
    "acc_base_rg = run_eval_letter(MODEL_BASE, \"gpt2_base\", use_rag=True)\n",
    "\n",
    "acc_new_no  = run_eval_letter(FT_OUT_DIR, \"gpt2_medqa_ft\", use_rag=False)\n",
    "acc_new_rg  = run_eval_letter(FT_OUT_DIR, \"gpt2_medqa_ft\", use_rag=True)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(\"base  no-rag:\", acc_base_no, \"rag:\", acc_base_rg, \"gain:\", acc_base_rg-acc_base_no)\n",
    "print(\"newFT no-rag:\", acc_new_no,  \"rag:\", acc_new_rg,  \"gain:\", acc_new_rg-acc_new_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b3207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
