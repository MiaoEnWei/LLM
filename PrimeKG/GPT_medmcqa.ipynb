{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c57b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbabbf",
   "metadata": {},
   "source": [
    "gpt_medmcqa(pubmedqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ce91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Data & Model Paths\n",
    "MEDMCQA_VAL = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/medmcqa/test.json\"\n",
    "\n",
    "MODEL_A = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/gpt2\"\n",
    "MODEL_B = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/gpt2-medmcqa-raft-masked\"\n",
    "\n",
    "# Your RAG Index & Documents\n",
    "RAG_INDEX = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/pubmed_qa_pubmedbert.index\"\n",
    "RAG_DOCS  = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/pubmed_documents.pkl\"\n",
    "\n",
    "# Retrieval embedding model (HF name or local path)\n",
    "RAG_EMB_MODEL = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "# Output Directory\n",
    "OUT_DIR = \"./eval_out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Run Parameters\n",
    "SEED = 42\n",
    "LIMIT = None          # None = Full dataset; e.g., 200 for a quick test run\n",
    "RAG_K = 3\n",
    "MAX_INPUT_LEN = 512   # Max tokens for the GPT-2 prompt\n",
    "MAX_NEW_TOKENS = 8    # Just output a single letter, prevent the model from rambling\n",
    "BATCH_EMB = 16        # Query embedding batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18fa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MedMCQA val: 6150\n",
      "Sample keys: dict_keys(['question', 'opa', 'opb', 'opc', 'opd', 'subject_name', 'topic_name', 'id', 'choice_type'])\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    assert os.path.exists(path), f\"can't find file: {path}\"\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "medmcqa_val = load_jsonl(MEDMCQA_VAL)\n",
    "if LIMIT is not None:\n",
    "    medmcqa_val = medmcqa_val[:LIMIT]\n",
    "print(\"Loaded MedMCQA val:\", len(medmcqa_val))\n",
    "print(\"Sample keys:\", medmcqa_val[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d505b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cop_to_letter(cop) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    MedMCQA: A/B/C/D。\n",
    "    \"\"\"\n",
    "    if cop is None:\n",
    "        return None\n",
    "    s = str(cop).strip()\n",
    "    if s in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "        return s\n",
    "    if s.isdigit():\n",
    "        v = int(s)\n",
    "        if 1 <= v <= 4:\n",
    "            return chr(ord(\"A\") + v - 1)\n",
    "        if 0 <= v <= 3:\n",
    "            return chr(ord(\"A\") + v)\n",
    "    return None\n",
    "\n",
    "def build_mc_prompt(ex: Dict[str, Any], evidence: str = \"\") -> str:\n",
    "    q = ex[\"question\"]\n",
    "    A = ex.get(\"opa\",\"\")\n",
    "    B = ex.get(\"opb\",\"\")\n",
    "    C = ex.get(\"opc\",\"\")\n",
    "    D = ex.get(\"opd\",\"\")\n",
    "\n",
    "    instr = (\n",
    "        \"You are a medical exam solver. Choose the single best option.\\n\"\n",
    "        \"Reply with ONLY one letter: A, B, C, or D.\\n\"\n",
    "    )\n",
    "\n",
    "    ev_block = \"\"\n",
    "    if evidence and evidence.strip():\n",
    "        ev_block = f\"\\nRetrieved evidence:\\n{evidence.strip()}\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"{instr}\"\n",
    "        f\"{ev_block}\"\n",
    "        f\"\\nQuestion:\\n{q}\\n\"\n",
    "        f\"\\nOptions:\\nA) {A}\\nB) {B}\\nC) {C}\\nD) {D}\\n\"\n",
    "        f\"\\nAnswer (A, B, C, or D):\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "_letter_pat = re.compile(r\"\\b([ABCD])\\b\")\n",
    "\n",
    "def extract_letter(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    output A/B/C/D.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    text = text.strip().upper()\n",
    "    m = _letter_pat.search(text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    # \"ANSWER: A\"\n",
    "    for ch in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "        if ch in text[:5]:\n",
    "            return ch\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt2(model_path: str):\n",
    "    assert os.path.exists(model_path), f\"model path not found: {model_path}\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    # GPT2：fp16 else fp32\n",
    "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    try:\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "    except Exception:\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=dtype).to(DEVICE)\n",
    "\n",
    "    mdl.eval()\n",
    "    return tok, mdl\n",
    "\n",
    "@torch.no_grad()\n",
    "def gpt2_answer_letter(tokenizer, model, prompt: str) -> Tuple[str, Optional[str]]:\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        padding=False\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    gen = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    gen_suffix = gen[len(prompt):] if len(gen) > len(prompt) else gen\n",
    "    letter = extract_letter(gen_suffix)\n",
    "    return gen_suffix, letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f705acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG index loaded, ntotal= 800\n",
      "RAG docs size= 800\n"
     ]
    }
   ],
   "source": [
    "def load_rag_resources(index_path: str, docs_path: str):\n",
    "    assert os.path.exists(index_path), f\"RAG index 不存在: {index_path}\"\n",
    "    assert os.path.exists(docs_path),  f\"RAG docs 不存在: {docs_path}\"\n",
    "\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    with open(docs_path, \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "\n",
    "    # docs maybe list[str] or list[dict]，\n",
    "    return index, docs\n",
    "\n",
    "def docs_to_text(d) -> str:\n",
    "    if isinstance(d, str):\n",
    "        return d\n",
    "    if isinstance(d, dict):\n",
    "        # Common field bottom stitching\n",
    "        for key in [\"text\",\"abstract\",\"content\",\"body\"]:\n",
    "            if key in d and isinstance(d[key], str) and d[key].strip():\n",
    "                return d[key]\n",
    "        # miss it all dict dump\n",
    "        return json.dumps(d, ensure_ascii=False)\n",
    "    return str(d)\n",
    "\n",
    "def load_embedder(model_name_or_path: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    mdl = AutoModel.from_pretrained(model_name_or_path)\n",
    "    mdl.to(DEVICE)\n",
    "    mdl.eval()\n",
    "    return tok, mdl\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_texts(embed_tok, embed_model, texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    mean pooling sentence vector, output float32 numpy(N, dim)\n",
    "    \"\"\"\n",
    "    all_vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = embed_tok(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        out = embed_model(**enc).last_hidden_state  # (B, T, H)\n",
    "        attn = enc[\"attention_mask\"].unsqueeze(-1)  # (B, T, 1)\n",
    "        masked = out * attn\n",
    "        summed = masked.sum(dim=1)\n",
    "        denom = attn.sum(dim=1).clamp(min=1)\n",
    "        vec = (summed / denom)  # (B, H)\n",
    "        vec = vec.detach().cpu().float().numpy()\n",
    "        all_vecs.append(vec)\n",
    "\n",
    "    vecs = np.vstack(all_vecs).astype(np.float32)\n",
    "    return vecs\n",
    "\n",
    "def rag_retrieve(index, docs, embed_tok, embed_model, query: str, k: int = 3) -> Tuple[List[int], List[str]]:\n",
    "    qvec = embed_texts(embed_tok, embed_model, [query], batch_size=1)\n",
    "    # faiss expects float32\n",
    "    D, I = index.search(qvec, k)\n",
    "    idxs = I[0].tolist()\n",
    "    texts = []\n",
    "    for j in idxs:\n",
    "        if j < 0 or j >= len(docs):\n",
    "            continue\n",
    "        texts.append(docs_to_text(docs[j]))\n",
    "    return idxs, texts\n",
    "\n",
    "# load\n",
    "rag_index, rag_docs = load_rag_resources(RAG_INDEX, RAG_DOCS)\n",
    "emb_tok, emb_model = load_embedder(RAG_EMB_MODEL)\n",
    "\n",
    "print(\"RAG index loaded, ntotal=\", rag_index.ntotal)\n",
    "print(\"RAG docs size=\", len(rag_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_no_rag(tokenizer, model, ex: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    prompt = build_mc_prompt(ex, evidence=\"\")\n",
    "    gen_text, pred = gpt2_answer_letter(tokenizer, model, prompt)\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"gen\": gen_text,\n",
    "        \"pred\": pred,\n",
    "        \"evidence\": \"\"\n",
    "    }\n",
    "\n",
    "def qa_with_rag(tokenizer, model, ex: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Search using question (which can also be spelled as subject/topic)\n",
    "    query = ex[\"question\"]\n",
    "    idxs, ev_texts = rag_retrieve(rag_index, rag_docs, emb_tok, emb_model, query, k=RAG_K)\n",
    "    evidence = \"\\n\\n\".join([f\"- {t}\" for t in ev_texts[:RAG_K]])\n",
    "\n",
    "    prompt = build_mc_prompt(ex, evidence=evidence)\n",
    "    gen_text, pred = gpt2_answer_letter(tokenizer, model, prompt)\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"gen\": gen_text,\n",
    "        \"pred\": pred,\n",
    "        \"evidence\": evidence,\n",
    "        \"rag_idxs\": idxs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_on_medmcqa(\n",
    "    model_path: str,\n",
    "    dataset: List[Dict[str, Any]],\n",
    "    run_tag: str,\n",
    "    use_rag: bool,\n",
    "    save_jsonl: Optional[str] = None,\n",
    "    print_every: int = 200\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    tok, mdl = load_gpt2(model_path)\n",
    "\n",
    "    if save_jsonl is None:\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        save_jsonl = os.path.join(OUT_DIR, f\"{run_tag}_{'rag' if use_rag else 'no_rag'}_{ts}.jsonl\")\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with open(save_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, ex in enumerate(dataset):\n",
    "            gt = cop_to_letter(ex.get(\"cop\"))\n",
    "            if gt is None:\n",
    "                # Unable to recognize GT sample skipping\n",
    "                continue\n",
    "\n",
    "            if use_rag:\n",
    "                out = qa_with_rag(tok, mdl, ex)\n",
    "            else:\n",
    "                out = qa_no_rag(tok, mdl, ex)\n",
    "\n",
    "            pred = out[\"pred\"]\n",
    "            is_ok = (pred == gt)\n",
    "\n",
    "            rec = {\n",
    "                \"idx\": i,\n",
    "                \"id\": ex.get(\"id\",\"\"),\n",
    "                \"subject\": ex.get(\"subject_name\",\"\"),\n",
    "                \"topic\": ex.get(\"topic_name\",\"\"),\n",
    "                \"question\": ex.get(\"question\",\"\"),\n",
    "                \"gt\": gt,\n",
    "                \"pred\": pred,\n",
    "                \"is_correct\": bool(is_ok),\n",
    "                \"use_rag\": use_rag,\n",
    "                \"model_path\": model_path,\n",
    "                \"run_tag\": run_tag,\n",
    "                \"rag_k\": RAG_K if use_rag else 0,\n",
    "                \"rag_context\": out.get(\"evidence\",\"\") if use_rag else \"\",\n",
    "                \"gen\": out.get(\"gen\",\"\"),\n",
    "            }\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            total += 1\n",
    "            correct += int(is_ok)\n",
    "\n",
    "            if print_every and total % print_every == 0:\n",
    "                print(f\"[{run_tag} | {'RAG' if use_rag else 'NO-RAG'}] processed={total} acc={correct/total:.3f}\")\n",
    "\n",
    "    acc = correct / total if total else 0.0\n",
    "    summary = {\"run_tag\": run_tag, \"use_rag\": use_rag, \"total\": total, \"acc\": acc, \"save_jsonl\": save_jsonl}\n",
    "    print(\"DONE:\", summary)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def compare_fix_hurt(no_rag_jsonl: str, rag_jsonl: str) -> Dict[str, Any]:\n",
    "    def read_map(path):\n",
    "        m = {}\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                r = json.loads(line)\n",
    "                m[r[\"idx\"]] = r\n",
    "        return m\n",
    "\n",
    "    a = read_map(no_rag_jsonl)\n",
    "    b = read_map(rag_jsonl)\n",
    "\n",
    "    keys = sorted(set(a.keys()) & set(b.keys()))\n",
    "    fix = hurt = both_ok = both_wrong = 0\n",
    "\n",
    "    for k in keys:\n",
    "        ca = bool(a[k][\"is_correct\"])\n",
    "        cb = bool(b[k][\"is_correct\"])\n",
    "        if (not ca) and cb:\n",
    "            fix += 1\n",
    "        elif ca and (not cb):\n",
    "            hurt += 1\n",
    "        elif ca and cb:\n",
    "            both_ok += 1\n",
    "        else:\n",
    "            both_wrong += 1\n",
    "\n",
    "    out = {\n",
    "        \"n\": len(keys),\n",
    "        \"fix\": fix,\n",
    "        \"hurt\": hurt,\n",
    "        \"both_ok\": both_ok,\n",
    "        \"both_wrong\": both_wrong\n",
    "    }\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea55f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) gpt2 base\n",
    "sum_a_no = eval_model_on_medmcqa(\n",
    "    model_path=MODEL_A,\n",
    "    dataset=medmcqa_val,\n",
    "    run_tag=\"gpt2_base\",\n",
    "    use_rag=False\n",
    ")\n",
    "\n",
    "sum_a_rag = eval_model_on_medmcqa(\n",
    "    model_path=MODEL_A,\n",
    "    dataset=medmcqa_val,\n",
    "    run_tag=\"gpt2_base\",\n",
    "    use_rag=True\n",
    ")\n",
    "\n",
    "fixhurt_a = compare_fix_hurt(sum_a_no[\"save_jsonl\"], sum_a_rag[\"save_jsonl\"])\n",
    "print(\"gpt2_base Fix/Hurt:\", fixhurt_a)\n",
    "\n",
    "# 2) gpt2-medmcqa-raft-masked\n",
    "sum_b_no = eval_model_on_medmcqa(\n",
    "    model_path=MODEL_B,\n",
    "    dataset=medmcqa_val,\n",
    "    run_tag=\"gpt2_raft_masked\",\n",
    "    use_rag=False\n",
    ")\n",
    "\n",
    "sum_b_rag = eval_model_on_medmcqa(\n",
    "    model_path=MODEL_B,\n",
    "    dataset=medmcqa_val,\n",
    "    run_tag=\"gpt2_raft_masked\",\n",
    "    use_rag=True\n",
    ")\n",
    "\n",
    "fixhurt_b = compare_fix_hurt(sum_b_no[\"save_jsonl\"], sum_b_rag[\"save_jsonl\"])\n",
    "print(\"gpt2_raft_masked Fix/Hurt:\", fixhurt_b)\n",
    "\n",
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(\"A no_rag:\", sum_a_no)\n",
    "print(\"A rag   :\", sum_a_rag)\n",
    "print(\"B no_rag:\", sum_b_no)\n",
    "print(\"B rag   :\", sum_b_rag)\n",
    "\n",
    "print(\"\\n=== KEY COMPARISONS ===\")\n",
    "print(f\"Base RAG gain: {sum_a_rag['acc'] - sum_a_no['acc']:+.4f}\")\n",
    "print(f\"FT   RAG gain: {sum_b_rag['acc'] - sum_b_no['acc']:+.4f}\")\n",
    "print(f\"RAG condition: FT - Base: {sum_b_rag['acc'] - sum_a_rag['acc']:+.4f}\")\n",
    "print(f\"NoRAG condition: FT - Base: {sum_b_no['acc'] - sum_a_no['acc']:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833e980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c61c8414",
   "metadata": {},
   "source": [
    "gpt_medmcqa(pubmedqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564cb4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK. DEVICE = cuda\n",
      "Loading GPT-2 from /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/gpt2 ...\n",
      "GPT-2 loaded.\n",
      "Loading Embedding Model: all-MiniLM-L6-v2 ...\n",
      "Loading FAISS Index ...\n",
      "Loading Documents ...\n",
      "Knowledge Base Loaded! Index size: 800, Docs count: 800\n"
     ]
    }
   ],
   "source": [
    "# Layer 1: Basic Imports & Configuration\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "# [Configuration Area] Please modify variables below according to your actual paths\n",
    "GPT2_PATH           = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/gpt2\"\n",
    "MEDMCQA_FILE        = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/medmcqa/dev.json\"\n",
    "\n",
    "# Knowledge Base File Paths (Ensure these files are in the current directory, or use absolute paths)\n",
    "FAISS_INDEX_PATH    = \"pubmed_qa.index\"\n",
    "DOCS_PKL_PATH       = \"pubmed_documents.pkl\"\n",
    "\n",
    "# Embedding Model (Must match the model used when building the index)\n",
    "EMBED_MODEL_NAME    = \"all-MiniLM-L6-v2\" \n",
    "\n",
    "DEVICE              = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# RAG Parameters\n",
    "TOP_K_DOCS          = 2     # Retrieve the top 2 most relevant abstracts\n",
    "MAX_CTX_CHARS       = 2000  # Maximum context characters (Prevent GPT-2 memory overflow/context limit issues)\n",
    "\n",
    "print(f\"Config OK. DEVICE = {DEVICE}\")\n",
    "\n",
    "\n",
    "# Layer 2: Model Layer (GPT-2 Loading & Fixed Generation Function)\n",
    "def load_gpt2(model_path: str = GPT2_PATH):\n",
    "    print(f\"Loading GPT-2 from {model_path} ...\")\n",
    "    try:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_path).to(DEVICE)\n",
    "        print(f\"GPT-2 loaded.\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GPT-2: {e}\")\n",
    "        return None, None\n",
    "\n",
    "tokenizer, model = load_gpt2()\n",
    "\n",
    "def gpt2_generate(prompt: str, max_new_tokens: int = 120, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
    "    \"\"\"\n",
    "    General generation function, UserWarning issues fixed.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    # Input length truncation protection (GPT-2 context is usually 1024)\n",
    "    if inputs.shape[1] > 900:\n",
    "        inputs = inputs[:, -900:]\n",
    "        \n",
    "    attention_mask = torch.ones_like(inputs)\n",
    "\n",
    "    # Dynamically build parameters to avoid errors when passing temperature with do_sample=False\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"top_p\": top_p,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "    # Only pass temperature when sampling is enabled\n",
    "    if do_sample:\n",
    "        gen_kwargs[\"temperature\"] = temperature\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, **gen_kwargs)\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Layer 3: Knowledge Base Layer (Load FAISS & Documents)\n",
    "def load_retrieval_system():\n",
    "    if not os.path.exists(FAISS_INDEX_PATH) or not os.path.exists(DOCS_PKL_PATH):\n",
    "        print(f\"Error: Knowledge base files not found. Please check if {FAISS_INDEX_PATH} and {DOCS_PKL_PATH} exist.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"Loading Embedding Model: {EMBED_MODEL_NAME} ...\")\n",
    "    embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "    \n",
    "    print(f\"Loading FAISS Index ...\")\n",
    "    index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "    \n",
    "    print(f\"Loading Documents ...\")\n",
    "    with open(DOCS_PKL_PATH, \"rb\") as f:\n",
    "        documents = pickle.load(f)\n",
    "        \n",
    "    print(f\"Knowledge Base Loaded! Index size: {index.ntotal}, Docs count: {len(documents)}\")\n",
    "    return embed_model, index, documents\n",
    "\n",
    "# Initialize global variables\n",
    "embed_model, faiss_index, doc_store = load_retrieval_system()\n",
    "\n",
    "\n",
    "# Layer 4: Vector Retrieval RAG -- Semantic Search Core Logic\n",
    "def get_pubmed_context(question_text: str, top_k: int = TOP_K_DOCS) -> str:\n",
    "    \"\"\"\n",
    "    1. Question to Vector\n",
    "    2. FAISS Search for Similar Document IDs\n",
    "    3. Extract Text and Concatenate\n",
    "    \"\"\"\n",
    "    if faiss_index is None:\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Encode\n",
    "    q_emb = embed_model.encode([question_text], convert_to_numpy=True)\n",
    "    \n",
    "    # 2. Search\n",
    "    distances, indices = faiss_index.search(q_emb, top_k)\n",
    "    \n",
    "    # 3. Fetch Text\n",
    "    retrieved_texts = []\n",
    "    current_chars = 0\n",
    "    \n",
    "    for idx_in_store in indices[0]:\n",
    "        if idx_in_store == -1: continue # Placeholder when FAISS finds nothing\n",
    "        \n",
    "        if idx_in_store >= len(doc_store): continue # Prevent index out of bounds\n",
    "\n",
    "        doc_content = doc_store[idx_in_store]\n",
    "        \n",
    "        # Simple cleaning\n",
    "        clean_content = doc_content.replace(\"\\n\", \" \").strip()\n",
    "        if not clean_content: continue\n",
    "\n",
    "        # Length check\n",
    "        if current_chars + len(clean_content) > MAX_CTX_CHARS:\n",
    "            remaining = MAX_CTX_CHARS - current_chars\n",
    "            retrieved_texts.append(f\"Abstract: {clean_content[:remaining]}...\")\n",
    "            break\n",
    "        \n",
    "        retrieved_texts.append(f\"Abstract: {clean_content}\")\n",
    "        current_chars += len(clean_content)\n",
    "    \n",
    "    if not retrieved_texts:\n",
    "        return \"\"\n",
    "    \n",
    "    return \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "\n",
    "# Layer 5: MedMCQA Data Loading\n",
    "def load_medmcqa_example(idx: int = 0, file_path: str = MEDMCQA_FILE):\n",
    "    \"\"\"\n",
    "    Read the idx-th sample from the MedMCQA dataset\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        line_idx = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            if line_idx == idx:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # Construct question stem\n",
    "                q = data.get(\"question\") or data.get(\"Question\") or \"\"\n",
    "                \n",
    "                # Construct options\n",
    "                options_lines = []\n",
    "                option_map = {\"opa\": \"A\", \"opb\": \"B\", \"opc\": \"C\", \"opd\": \"D\"}\n",
    "                found = False\n",
    "                for k, lab in option_map.items():\n",
    "                    if k in data:\n",
    "                        found = True\n",
    "                        options_lines.append(f\"{lab}) {data[k]}\")\n",
    "                if not found and \"options\" in data:\n",
    "                    for i, opt in enumerate(data[\"options\"]):\n",
    "                        options_lines.append(f\"{chr(ord('A')+i)}) {opt}\")\n",
    "                \n",
    "                question_text = q\n",
    "                if options_lines:\n",
    "                    question_text += \"\\nOptions:\\n\" + \"\\n\".join(options_lines)\n",
    "                \n",
    "                # Get answer\n",
    "                answer = data.get(\"cop\") or data.get(\"answer\") or data.get(\"label\")\n",
    "                return {\"raw\": data, \"question_text\": question_text, \"answer\": answer}\n",
    "            line_idx += 1\n",
    "    raise IndexError(f\"Index {idx} out of range\")\n",
    "\n",
    "\n",
    "# Layer 6: GPT-2 QA Interface (No RAG vs Vector RAG)\n",
    "def qa_no_rag(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Baseline: No database query, ask GPT-2 directly\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a medical exam solver.\\n\"\n",
    "        \"You will be given one multiple-choice question with options A, B, C, and D.\\n\"\n",
    "        \"Choose the single best option and reply with ONLY one capital letter: A, B, C, or D.\\n\"\n",
    "        \"Do not output anything else.\\n\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"Answer (A, B, C, or D):\"\n",
    "    )\n",
    "    # do_sample=False indicates greedy search (deterministic results)\n",
    "    full_text = gpt2_generate(prompt, max_new_tokens=8, do_sample=False)\n",
    "    \n",
    "    # Extract the last letter\n",
    "    tail = full_text.strip()\n",
    "    for ch in reversed(tail):\n",
    "        if ch in [\"A\", \"B\", \"C\", \"D\"]: return ch\n",
    "    return tail\n",
    "\n",
    "def qa_with_rag_vector(question: str):\n",
    "    \"\"\"\n",
    "    Vector RAG: Query Database -> Concatenate Context -> Ask GPT-2\n",
    "    \"\"\"\n",
    "    # 1. Get context\n",
    "    context = get_pubmed_context(question, top_k=TOP_K_DOCS)\n",
    "    \n",
    "    if not context:\n",
    "        return \"\", qa_no_rag(question)\n",
    "\n",
    "    # 2. Construct Prompt\n",
    "    prompt = (\n",
    "        \"You are a medical exam solver.\\n\"\n",
    "        \"Below are some relevant research abstracts retrieved from PubMed.\\n\"\n",
    "        \"Use this context to help answer the question.\\n\"\n",
    "        \"You will be given one multiple-choice question with options A, B, C, and D.\\n\"\n",
    "        \"Choose the single best option and reply with ONLY one capital letter: A, B, C, or D.\\n\"\n",
    "        \"Do not output anything else.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\n\"\n",
    "        \"Answer (A, B, C, or D):\"\n",
    "    )\n",
    "    \n",
    "    full_text = gpt2_generate(prompt, max_new_tokens=8, do_sample=False)\n",
    "    \n",
    "    tail = full_text.strip()\n",
    "    for ch in reversed(tail):\n",
    "        if ch in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            return context, ch\n",
    "\n",
    "    return context, tail\n",
    "\n",
    "\n",
    "# Layer 7: Comparison Test Function (Single Item)\n",
    "def compare_rag_medmcqa_vector(idx: int = 0, max_print_chars: int = 600, save_dir: str = \"rag_logs_vector\"):\n",
    "    \"\"\"\n",
    "    Run a single test and print a detailed report\n",
    "    \"\"\"\n",
    "    try:\n",
    "        example = load_medmcqa_example(idx)\n",
    "    except IndexError:\n",
    "        print(f\"Index {idx} out of range.\")\n",
    "        return\n",
    "\n",
    "    question = example[\"question_text\"]\n",
    "    gt_raw = example[\"answer\"]\n",
    "\n",
    "    # Unify answer format to A/B/C/D\n",
    "    def _to_letter(x):\n",
    "        if x is None: return None\n",
    "        s = str(x).strip()\n",
    "        if s in [\"A\", \"B\", \"C\", \"D\"]: return s\n",
    "        if s.isdigit() and 1 <= int(s) <= 4: return chr(ord(\"A\") + int(s) - 1)\n",
    "        return s\n",
    "\n",
    "    gt = _to_letter(gt_raw)\n",
    "\n",
    "    # 1) No RAG\n",
    "    ans_no = qa_no_rag(question)\n",
    "    \n",
    "    # 2) Vector RAG\n",
    "    retrieved_ctx, ans_rag = qa_with_rag_vector(question)\n",
    "\n",
    "    correct_no  = (str(ans_no) == str(gt))\n",
    "    correct_rag = (str(ans_rag) == str(gt))\n",
    "\n",
    "    # Print report\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    out_path = os.path.join(save_dir, f\"medmcqa_idx_{idx}.txt\")\n",
    "    \n",
    "    def _short(s): return s if len(s) <= max_print_chars else s[:max_print_chars] + \"...\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"MedMCQA Sample idx = {idx}\")\n",
    "    print(\"Question:\")\n",
    "    print(_short(question))\n",
    "    print(f\"\\nCorrect Answer (GT): {gt}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"[No RAG] Prediction: {ans_no} | Correct? {correct_no}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"[Vector RAG] Prediction: {ans_rag} | Correct? {correct_rag}\")\n",
    "    print(\"\\n[Retrieved Context (Top 2)]:\")\n",
    "    if retrieved_ctx:\n",
    "        print(_short(retrieved_ctx))\n",
    "    else:\n",
    "        print(\"(No relevant documents)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Q: {question}\\nGT: {gt}\\n\\nCTX:\\n{retrieved_ctx}\\n\\nPred_No: {ans_no}\\nPred_RAG: {ans_rag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime # Import datetime module\n",
    "\n",
    "\n",
    "# Layer 8: Batch Evaluation (Automatic Timestamps + Directory Management)\n",
    "def evaluate_medmcqa_acc(start_idx=0, end_idx=100, output_file=None):\n",
    "    \"\"\"\n",
    "    Batch test and save results.\n",
    "    Arguments:\n",
    "        output_file: (Optional) Specify filename. If not provided, automatically generated based on current time.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Automatically create file names with timestamps\n",
    "    if output_file is None:\n",
    "        # Get current time, format: YYYYMMDD_HHMMSS\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"rag_eval_{timestamp}.csv\"\n",
    "    \n",
    "    # 2. Automatically create the results folder (optional, for neatness)\n",
    "    output_dir = \"results\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Output directory created: {output_dir}\")\n",
    "    \n",
    "    # Combined complete path: results/rag_eval_2025xxxx.csv\n",
    "    full_output_path = os.path.join(output_dir, output_file)\n",
    "    \n",
    "    print(f\"Start batch evaluation: Index {start_idx} -> {end_idx}\")\n",
    "    print(f\"Results will be saved to: {full_output_path}\")\n",
    "    \n",
    "    results = [] \n",
    "    total = 0\n",
    "    correct_no = 0\n",
    "    correct_rag = 0\n",
    "    improved = 0\n",
    "    worsened = 0\n",
    "    \n",
    "    for idx in range(start_idx, end_idx):\n",
    "        try:\n",
    "            ex = load_medmcqa_example(idx)\n",
    "        except: \n",
    "            continue\n",
    "        \n",
    "        q = ex[\"question_text\"]\n",
    "        raw_ans = ex[\"answer\"]\n",
    "        \n",
    "        # Parse GT (Ground Truth)\n",
    "        gt = None\n",
    "        if raw_ans and str(raw_ans).strip() in [\"A\",\"B\",\"C\",\"D\"]: \n",
    "            gt = str(raw_ans).strip()\n",
    "        elif raw_ans and str(raw_ans).isdigit(): \n",
    "            gt = chr(ord(\"A\") + int(raw_ans) - 1)\n",
    "        \n",
    "        if not gt: continue \n",
    "\n",
    "        # === Core Prediction ===\n",
    "        pred_no = qa_no_rag(q)\n",
    "        ctx_rag, pred_rag = qa_with_rag_vector(q)\n",
    "        \n",
    "        # === Statistics ===\n",
    "        is_correct_no = (pred_no == gt)\n",
    "        is_correct_rag = (pred_rag == gt)\n",
    "        \n",
    "        if is_correct_no: correct_no += 1\n",
    "        if is_correct_rag: correct_rag += 1\n",
    "        total += 1\n",
    "        \n",
    "        status = \"Same\"\n",
    "        if not is_correct_no and is_correct_rag:\n",
    "            status = \"Improved\"\n",
    "            improved += 1\n",
    "        elif is_correct_no and not is_correct_rag:\n",
    "            status = \"Worsened\"\n",
    "            worsened += 1\n",
    "        \n",
    "        results.append({\n",
    "            \"Index\": idx,\n",
    "            \"Question\": q,\n",
    "            \"Ground_Truth\": gt,\n",
    "            \"Pred_No_RAG\": pred_no,\n",
    "            \"Correct_No_RAG\": is_correct_no,\n",
    "            \"Pred_Vector_RAG\": pred_rag,\n",
    "            \"Correct_Vector_RAG\": is_correct_rag,\n",
    "            \"Status\": status,\n",
    "            \"Retrieved_Context\": ctx_rag\n",
    "        })\n",
    "\n",
    "        print(f\"[{idx}] GT:{gt} | NoRAG:{pred_no} {'O' if is_correct_no else 'X'} | RAG:{pred_rag} {'O' if is_correct_rag else 'X'} | {status}\")\n",
    "\n",
    "    # === Calculate Final Metrics ===\n",
    "    acc_no = correct_no / total if total > 0 else 0\n",
    "    acc_rag = correct_rag / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nFinal Results ({total} questions):\")\n",
    "    print(f\"Pure GPT-2 Accuracy : {acc_no:.4f}\")\n",
    "    print(f\"RAG (Vector) Accuracy: {acc_rag:.4f}\")\n",
    "    \n",
    "    # === Save File ===\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        df[\"Global_Acc_No_RAG\"] = f\"{acc_no:.2%}\"\n",
    "        df[\"Global_Acc_Vector_RAG\"] = f\"{acc_rag:.2%}\"\n",
    "        \n",
    "        # 1. Save detailed CSV\n",
    "        df.to_csv(full_output_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Detailed results saved: {full_output_path}\")\n",
    "        \n",
    "        # 2. Save Summary CSV (Filename automatically adds _summary)\n",
    "        base, ext = os.path.splitext(output_file) # Separate filename and extension\n",
    "        summary_filename = f\"{base}_summary{ext}\"\n",
    "        full_summary_path = os.path.join(output_dir, summary_filename)\n",
    "        \n",
    "        summary_data = [{\n",
    "            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), # Record exact time\n",
    "            \"Range\": f\"{start_idx}-{end_idx}\",\n",
    "            \"Total_Questions\": total,\n",
    "            \"Acc_No_RAG\": acc_no,\n",
    "            \"Acc_Vector_RAG\": acc_rag,\n",
    "            \"Improved_Count\": improved,\n",
    "            \"Worsened_Count\": worsened\n",
    "        }]\n",
    "        pd.DataFrame(summary_data).to_csv(full_summary_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Summary statistics saved: {full_summary_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No results generated, skipping save.\")\n",
    "\n",
    "#\n",
    "# Run Example\n",
    "#\n",
    "\n",
    "# Method 1: No arguments passed, automatically generate filename with timestamp\n",
    "evaluate_medmcqa_acc(0, 4183)\n",
    "\n",
    "# Method 2: If you want to specify a name, you can pass it (will also be saved in results folder)\n",
    "# evaluate_medmcqa_acc(0, 50, output_file=\"my_custom_experiment.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
