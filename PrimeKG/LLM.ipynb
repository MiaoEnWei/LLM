{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb0fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Found local copy...\n",
      "Loading...\n",
      "Found local copy...\n",
      "Loading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['node_index', 'mondo_id', 'mondo_name', 'group_id_bert',\n",
      "       'group_name_bert', 'mondo_definition', 'umls_description',\n",
      "       'orphanet_definition', 'orphanet_prevalence', 'orphanet_epidemiology',\n",
      "       'orphanet_clinical_description', 'orphanet_management_and_treatment',\n",
      "       'mayo_symptoms', 'mayo_causes', 'mayo_risk_factors',\n",
      "       'mayo_complications', 'mayo_prevention', 'mayo_see_doc'],\n",
      "      dtype='object')\n",
      "Index(['node_index', 'description', 'half_life', 'indication',\n",
      "       'mechanism_of_action', 'protein_binding', 'pharmacodynamics', 'state',\n",
      "       'atc_1', 'atc_2', 'atc_3', 'atc_4', 'category', 'group', 'pathway',\n",
      "       'molecular_weight', 'tpsa', 'clogp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from tdc.resource import PrimeKG\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "data = PrimeKG(path='./data')\n",
    "G = data.to_nx()\n",
    "\n",
    "disease_feature = data.get_features('disease') \n",
    "drug_feature    = data.get_features('drug')\n",
    "\n",
    "print(disease_feature.columns)\n",
    "print(drug_feature.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb76c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['node_id'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PHYHIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KIF15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPANK1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PNMA1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZRSR2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  node_id\n",
       "0  PHYHIP\n",
       "1   KIF15\n",
       "2  GPANK1\n",
       "3   PNMA1\n",
       "4   ZRSR2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten all nodes and their attributes from G into a DataFrame\n",
    "node_rows = []\n",
    "for n, attr in G.nodes(data=True):\n",
    "    row = {\"node_id\": n}   # Node ID in G\n",
    "    row.update(attr)       # Expand attributes like type, name, etc.\n",
    "    node_rows.append(row)\n",
    "\n",
    "nodes_df = pd.DataFrame(node_rows)\n",
    "print(nodes_df.columns)\n",
    "nodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77048fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/miaoen/.local/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: networkx in /home/miaoen/.local/lib/python3.10/site-packages (3.4.2)\n",
      "Collecting openai\n",
      "  Downloading openai-2.9.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: fuzzywuzzy in /home/miaoen/.local/lib/python3.10/site-packages (0.18.0)\n",
      "Collecting python-levenshtein\n",
      "  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/miaoen/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/miaoen/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/miaoen/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/miaoen/.local/lib/python3.10/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/miaoen/.local/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/miaoen/.local/lib/python3.10/site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in /home/miaoen/.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/miaoen/.local/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/miaoen/.local/lib/python3.10/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/miaoen/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/miaoen/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (2.8)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /home/miaoen/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/miaoen/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/miaoen/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/miaoen/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/miaoen/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Collecting Levenshtein==0.27.3 (from python-levenshtein)\n",
      "  Downloading levenshtein-0.27.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-levenshtein)\n",
      "  Downloading rapidfuzz-3.14.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading openai-2.9.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
      "Downloading levenshtein-0.27.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, jiter, Levenshtein, python-levenshtein, openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [openai]2m4/5\u001b[0m [openai]tein]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Levenshtein-0.27.3 jiter-0.12.0 openai-2.9.0 python-levenshtein-0.27.3 rapidfuzz-3.14.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas networkx openai fuzzywuzzy python-levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK. DEVICE = cuda\n"
     ]
    }
   ],
   "source": [
    "#  Layer 1: 基础导入 & 配置\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "GPT2_PATH     = \"/LLM/gpt2\"  # GPT-2\n",
    "PRIMEKG_PATH  = \"kg.csv\"   # primKG.CSV\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Config OK. DEVICE =\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b72fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Layer 2: model (GPT-2)\n",
    "\n",
    "def load_gpt2(model_path: str = GPT2_PATH):\n",
    "    \"\"\"Load local GPT-2 model and tokenizer\"\"\"\n",
    "    print(f\" Loading GPT-2 from {model_path}...\", flush=True)\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # GPT-2 has no pad token, use eos as a substitute\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(DEVICE)\n",
    "\n",
    "    print(f\"  GPT-2 loaded successfully, running on device: {DEVICE}\")\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_gpt2()\n",
    "\n",
    "\n",
    "def gpt2_rewrite_answer(summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask GPT-2 to polish the English of the summary while \"trying not to change facts\".\n",
    "    ⚠ Risk: GPT-2 might still alter content, so use strictly as an auxiliary tool.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a rewriting assistant.\\n\"\n",
    "        \"You will be given an answer that is already factually correct.\\n\"\n",
    "        \"Rewrite it in fluent English, but DO NOT change or add any medical facts, \"\n",
    "        \"names, or relationships.\\n\\n\"\n",
    "        f\"Original answer:\\n{summary}\\n\\n\"\n",
    "        \"Rewritten answer:\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    if inputs.shape[1] > 900:\n",
    "        inputs = inputs[:, -900:]\n",
    "\n",
    "    attention_mask = torch.ones_like(inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Rewritten answer:\" in full_text:\n",
    "        rewritten = full_text.split(\"Rewritten answer:\", 1)[-1].strip()\n",
    "    else:\n",
    "        rewritten = full_text.strip()\n",
    "\n",
    "    return rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e632279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 正在从 kg.csv 加载知识图谱...\n",
      "列名: ['relation', 'display_relation', 'x_index', 'x_id', 'x_type', 'x_name', 'x_source', 'y_index', 'y_id', 'y_type', 'y_name', 'y_source']\n"
     ]
    }
   ],
   "source": [
    "#  Layer 3: Graph Layer (PrimeKG from CSV)\n",
    "\n",
    "def load_kg(path: str = PRIMEKG_PATH) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Load PrimeKG subgraph from CSV.\n",
    "    Assumes existence of at least: x_name, y_name, relation, display_relation, x_type, y_type.\n",
    "    \"\"\"\n",
    "    print(f\" Loading knowledge graph from {path}...\", flush=True)\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    print(\"Column names:\", list(df.columns))\n",
    "\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df,\n",
    "        source=\"x_name\",\n",
    "        target=\"y_name\",\n",
    "        edge_attr=True   # Put all remaining columns into edge_attr for later use\n",
    "    )\n",
    "    print(f\"  Graph loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "    return G\n",
    "\n",
    "G = load_kg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Layer 4: Retrieval Layer (Entity Matching + Structured Extraction)\n",
    "\n",
    "# Build \"lower -> original node name\" index for case-insensitive matching\n",
    "node_index = {str(n).lower(): n for n in G.nodes()}\n",
    "\n",
    "def resolve_node(name: str):\n",
    "    \"\"\"Map user-input entity name to the node name in the graph (case-insensitive).\"\"\"\n",
    "    if not name:\n",
    "        return None\n",
    "    key = name.strip().lower()\n",
    "    return node_index.get(key)\n",
    "\n",
    "\n",
    "def summarize_entity_from_edges(entity: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract one-hop relationships for a specific entity from the KG and organize them into a summary by relationship type.\n",
    "    Uses pure Python logic to ensure GPT-2 does not determine the facts.\n",
    "    \"\"\"\n",
    "    node = resolve_node(entity)\n",
    "    if node is None:\n",
    "        return None\n",
    "\n",
    "    edges = list(G.edges(node, data=True))\n",
    "    if not edges:\n",
    "        return None\n",
    "\n",
    "    carriers = set()\n",
    "    enzymes = set()\n",
    "    targets = set()\n",
    "    contraindications = set()\n",
    "    others = []\n",
    "\n",
    "    for u, v, attr in edges:\n",
    "        x_name = attr.get(\"x_name\", u)\n",
    "        y_name = attr.get(\"y_name\", v)\n",
    "        x_type = attr.get(\"x_type\", \"\")\n",
    "        y_type = attr.get(\"y_type\", \"\")\n",
    "        rel     = str(attr.get(\"relation\", \"\")).lower()\n",
    "        disp_rel = attr.get(\"display_relation\", attr.get(\"relation\", \"related_to\"))\n",
    "\n",
    "        # Unify neighbor / type\n",
    "        if node == x_name:\n",
    "            neighbor, n_type = y_name, y_type\n",
    "        else:\n",
    "            neighbor, n_type = x_name, x_type\n",
    "\n",
    "        # Classify by relation\n",
    "        if \"carrier\" in rel:\n",
    "            carriers.add(neighbor)\n",
    "        elif \"enzyme\" in rel:\n",
    "            enzymes.add(neighbor)\n",
    "        elif \"target\" in rel:\n",
    "            targets.add(neighbor)\n",
    "        elif \"contraindication\" in rel:\n",
    "            contraindications.add(neighbor)\n",
    "        else:\n",
    "            others.append(f\"{entity} is {disp_rel} {neighbor} ({n_type}).\")\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    if carriers:\n",
    "        parts.append(\n",
    "            f\"According to the knowledge graph, {entity} is carried by: \"\n",
    "            + \", \".join(sorted(carriers)) + \".\"\n",
    "        )\n",
    "    if enzymes:\n",
    "        parts.append(\n",
    "            f\"{entity} is metabolized by the enzymes: \"\n",
    "            + \", \".join(sorted(enzymes)) + \".\"\n",
    "        )\n",
    "    if targets:\n",
    "        parts.append(\n",
    "            f\"{entity} acts on targets such as: \"\n",
    "            + \", \".join(sorted(targets)) + \".\"\n",
    "        )\n",
    "    if contraindications:\n",
    "        parts.append(\n",
    "            f\"{entity} has contraindications including: \"\n",
    "            + \", \".join(sorted(contraindications)) + \".\"\n",
    "        )\n",
    "\n",
    "    # If nothing fits into the categories, fall back to the original sentences\n",
    "    if not parts and others:\n",
    "        parts.append(\" \".join(others[:5]))\n",
    "\n",
    "    if not parts:\n",
    "        return None\n",
    "\n",
    "    return \" \".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Layer 5: QA API Layer\n",
    "\n",
    "def answer_with_kg_gpt2(entity: str, question: str | None = None):\n",
    "    \"\"\"\n",
    "    Unified external interface (GPT-2 version):\n",
    "    1) Retrieve 1-hop neighbors from the graph using the entity to construct Facts\n",
    "    2) Answer the question using GPT-2 + Facts\n",
    "    \"\"\"\n",
    "    context = get_knowledge_context(entity, question=question)\n",
    "    if not context:\n",
    "        print(f\"  Entity not found in graph or has no neighbors: {entity}\")\n",
    "        return\n",
    "\n",
    "    print(\"[Retrieved Graph Facts]:\")\n",
    "    print(context)\n",
    "    print()\n",
    "\n",
    "    if not question:\n",
    "        question = f\"What is known about {entity} from these facts?\"\n",
    "\n",
    "    print(\" GPT-2 is generating answer...\\n\")\n",
    "    answer = generate_answer_gpt2(context, question)\n",
    "    print(\"[Answer]:\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Layer 6: Usage Examples (Ready to use in Notebook)\n",
    "\n",
    "# Example: Ask about Warfarin's metabolism and targets\n",
    "answer_with_kg_gpt2(\n",
    "    \"Warfarin\",\n",
    "    \"Based on these facts, what enzymes metabolize Warfarin and what targets does it act on?\"\n",
    ")\n",
    "\n",
    "# You can also switch entities and questions freely:\n",
    "# answer_with_kg_gpt2(\"UBC\", \"Summarize what types of entities are related to UBC in this graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a795e8a",
   "metadata": {},
   "source": [
    "GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ec502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载成功！运行设备: cuda\n",
      "PrimeKG 加载完毕！共 129262 个节点，4049405 条边。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 1. Configuration\n",
    "MODEL_PATH = '/LLM/gpt2'\n",
    "PRIMEKG_PATH = 'kg.csv'\n",
    "MAX_KNOWLEDGE_EDGES = 10\n",
    "\n",
    "# 2. Load Model (Notebook specific)\n",
    "def load_local_llm():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = GPT2LMHeadModel.from_pretrained(MODEL_PATH).to(device)\n",
    "\n",
    "    print(f\"Model loaded successfully! Running device: {device}\")\n",
    "    return tokenizer, model, device\n",
    "\n",
    "\n",
    "# 3. Load Knowledge Graph (Notebook specific)\n",
    "def load_kg(path):\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df, 'x_name', 'y_name',\n",
    "        edge_attr=['relation','display_relation','x_type','y_type']\n",
    "    )\n",
    "    print(f\"PrimeKG loaded! Total {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "    return G\n",
    "\n",
    "# Actual Loading\n",
    "tokenizer, model, device = load_local_llm()\n",
    "G = load_kg(PRIMEKG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49722d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity name case-insensitive mapping\n",
    "node_index = {str(n).lower(): n for n in G.nodes()}\n",
    "\n",
    "def resolve_node(name):\n",
    "    return node_index.get(name.lower().strip())\n",
    "\n",
    "def get_knowledge_context(entity_name):\n",
    "    node = resolve_node(entity_name)\n",
    "    if node is None:\n",
    "        return None\n",
    "    \n",
    "    edges = list(G.edges(node, data=True))\n",
    "    lines = []\n",
    "    for u, v, attr in edges[:MAX_KNOWLEDGE_EDGES]:\n",
    "        # Determine neighbor and type based on edge direction\n",
    "        if node == attr.get(\"x_name\", u):\n",
    "            neighbor = attr.get(\"y_name\", v)\n",
    "            n_type = attr.get(\"y_type\", \"\")\n",
    "        else:\n",
    "            neighbor = attr.get(\"x_name\", u)\n",
    "            n_type = attr.get(\"x_type\", \"\")\n",
    "        \n",
    "        relation = attr.get(\"display_relation\", attr.get(\"relation\", \"related_to\"))\n",
    "        lines.append(f\"{node} is {relation} {neighbor} ({n_type}).\")\n",
    "    \n",
    "    return \" \".join(lines)\n",
    "\n",
    "def generate_answer(context, question):\n",
    "    text = (\n",
    "        \"You are a medical assistant. Use ONLY the following facts to answer.\\n\"\n",
    "        \"If the question cannot be answered from the facts, reply: 'I don't know based on the given facts.'\\n\"\n",
    "        f\"Facts: {context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"Answer in one short paragraph. Do not invent new drugs or diseases.\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    attention_mask = torch.ones(inputs.shape, device=device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.2,   # Reduce randomness\n",
    "        do_sample=False,   # Change to greedy/deterministic\n",
    "        no_repeat_ngram_size=3,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return ans.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "def ask_once(entity, question=None):\n",
    "    ctx = get_knowledge_context(entity)\n",
    "    \n",
    "    if not ctx:\n",
    "        print(f\"  Entity not found in graph: {entity}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Retrieved Knowledge:\\n{ctx}\\n\")\n",
    "    \n",
    "    if not question:\n",
    "        question = f\"What is {entity}?\"\n",
    "    \n",
    "    print(\" Generating answer...\\n\")\n",
    "    ans = generate_answer(ctx, question)\n",
    "    print(\"Answer:\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a813a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_once(\"Warfarin\", \"What is Warfarin used for?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2bb3ad",
   "metadata": {},
   "source": [
    "LLAMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151f66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK. DEVICE = cuda\n"
     ]
    }
   ],
   "source": [
    "#  Layer 1: Basic Imports & Configuration\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLAMA2_PATH   = \"/LLM/llama2\"  # Local LLaMA2 model directory\n",
    "PRIMEKG_PATH  = \"kg.csv\"   # Your graph edge list file\n",
    "MAX_K_EDGES   = 12         # Number of relations to feed the model (too many will exceed context limit)\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Config OK. DEVICE =\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffa738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 正在从 /LLM/llama2 加载 LLaMA2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351050b6f94a439497e373ae6c5e5749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LLaMA2 加载成功，运行设备: cuda\n"
     ]
    }
   ],
   "source": [
    "#  Layer 2: Model Layer (LLaMA2)\n",
    "def load_llama2(model_path: str = LLAMA2_PATH):\n",
    "    \"\"\"Load local LLaMA2 model and tokenizer\"\"\"\n",
    "    print(f\" Loading LLaMA2 from {model_path}...\", flush=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "    print(f\"  LLaMA2 loaded successfully, running device: {DEVICE}\")\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_llama2()\n",
    "\n",
    "\n",
    "def generate_answer_llama2(context: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate answer using LLaMA2 based on graph facts.\n",
    "    Uses LLaMA2 chat style instruction format by default.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"[INST]<<SYS>>\n",
    "You are a helpful and careful medical assistant.\n",
    "Use ONLY the following facts from a biomedical knowledge graph to answer the question.\n",
    "If the facts are not enough, reply exactly: \"I don't know based on the given facts.\"\n",
    "Do NOT invent new drugs, diseases, or genes.\n",
    "<</SYS>>\n",
    "\n",
    "Facts:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer in English in 2-3 sentences.\n",
    "[/INST]\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=160,\n",
    "            temperature=0.2,\n",
    "            do_sample=False,                # Use deterministic generation first to avoid hallucination\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Simple truncation after [/INST]\n",
    "    if \"[/INST]\" in full_text:\n",
    "        answer = full_text.split(\"[/INST]\", 1)[-1].strip()\n",
    "    else:\n",
    "        answer = full_text.strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691aaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Layer 3: Knowledge Graph Layer (PrimeKG from CSV)\n",
    "def load_kg(path: str = PRIMEKG_PATH) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Load PrimeKG subgraph from CSV (assuming columns: x_name, y_name, relation, display_relation, x_type, y_type).\n",
    "    \"\"\"\n",
    "    print(f\" Loading knowledge graph from {path}...\", flush=True)\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    print(\"Column names:\", list(df.columns))\n",
    "\n",
    "    # Conservative approach: Load all columns as edge_attr to ensure relation/type information is preserved\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df,\n",
    "        source=\"x_name\",\n",
    "        target=\"y_name\",\n",
    "        edge_attr=True\n",
    "    )\n",
    "    print(f\"  Graph loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "    return G\n",
    "\n",
    "G = load_kg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb44764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Layer 4: Retrieval Layer (Entity Matching + Context Construction)\n",
    "\n",
    "# Build a simple \"lower -> original node name\" index for case-insensitive matching\n",
    "node_index = {str(n).lower(): n for n in G.nodes()}\n",
    "\n",
    "def resolve_node(name: str):\n",
    "    \"\"\"Map user input entity name to node name in the graph (case-insensitive)\"\"\"\n",
    "    if not name:\n",
    "        return None\n",
    "    key = name.strip().lower()\n",
    "    return node_index.get(key)\n",
    "\n",
    "\n",
    "def get_knowledge_context(entity_name: str, max_edges: int = MAX_K_EDGES) -> str | None:\n",
    "    \"\"\"\n",
    "    Retrieve several edges directly connected to the entity from the graph and assemble them into English Facts text.\n",
    "    Will try to include relation name + neighbor type.\n",
    "    \"\"\"\n",
    "    node = resolve_node(entity_name)\n",
    "    if node is None:\n",
    "        return None\n",
    "\n",
    "    edges = list(G.edges(node, data=True))\n",
    "    if not edges:\n",
    "        return None\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    for u, v, attr in edges[:max_edges]:\n",
    "        x_name = attr.get(\"x_name\", u)\n",
    "        y_name = attr.get(\"y_name\", v)\n",
    "        x_type = attr.get(\"x_type\", \"\")\n",
    "        y_type = attr.get(\"y_type\", \"\")\n",
    "        relation = attr.get(\"display_relation\", attr.get(\"relation\", \"related_to\"))\n",
    "\n",
    "        if node == x_name:\n",
    "            neighbor = y_name\n",
    "            n_type = y_type\n",
    "        elif node == y_name:\n",
    "            neighbor = x_name\n",
    "            n_type = x_type\n",
    "        else:\n",
    "            neighbor = v if node == u else u\n",
    "            n_type = x_type or y_type\n",
    "\n",
    "        if not n_type:\n",
    "            n_type = \"Entity\"\n",
    "\n",
    "        line = f\"{node} is {relation} {neighbor} ({n_type}).\"\n",
    "        lines.append(line)\n",
    "\n",
    "    return \" \".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Layer 5: QA API Layer\n",
    "\n",
    "def answer_with_kg(entity: str, question: str | None = None):\n",
    "    \"\"\"\n",
    "    Unified external interface:\n",
    "    1) Retrieve 1-hop neighbors from the graph using the entity to construct Facts\n",
    "    2) Answer the question using LLaMA2 + Facts\n",
    "    \"\"\"\n",
    "    context = get_knowledge_context(entity)\n",
    "    if not context:\n",
    "        print(f\"  Entity not found in graph or has no neighbors: {entity}\")\n",
    "        return\n",
    "\n",
    "    print(\"[Retrieved Graph Facts]:\")\n",
    "    print(context)\n",
    "    print()\n",
    "\n",
    "    if not question:\n",
    "        question = f\"What is the biomedical role of {entity} according to these facts?\"\n",
    "\n",
    "    print(\" LLaMA2 is generating answer...\\n\")\n",
    "    answer = generate_answer_llama2(context, question)\n",
    "    print(\"[Answer]:\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Layer 6: Usage Examples (You can run this block repeatedly in the Notebook)\n",
    "# Example 1: Ask about Warfarin's metabolism / targets\n",
    "answer_with_kg(\"Warfarin\", \"Based on these facts, what enzymes metabolize Warfarin and what targets does it act on?\")\n",
    "\n",
    "# Example 2: Switch to another entity you know exists in kg.csv\n",
    "# answer_with_kg(\"UBC\", \"Summarize what types of entities are related to UBC in this graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f606b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
