{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924332c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#Configuration Section (Please modify according to your actual paths)\n",
    "MODEL_PATH = \"C:\\\\LLM\\\\gpt2\"\n",
    "PRIMEKG_PATH = \"kg.csv\" \n",
    "PUBMEDQA_FILE = \"C:\\\\LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# RAG Parameter Optimization\n",
    "MAX_RAG_ENTITIES = 3    # Keep count low to avoid interference\n",
    "MAX_K_EDGES      = 3    # Take only the top 3 most important edges per entity to reduce Token usage\n",
    "MAX_TOTAL_TOKENS = 950  # GPT-2 limit is approx 1024, reserve some for generation/computation\n",
    "\n",
    "# Blacklist: Filter out meaningless generic words from PrimeKG caused by tokenization or matching\n",
    "ENTITY_BLACKLIST = {\n",
    "    \"stable\", \"group\", \"left\", \"right\", \"study\", \"human\", \"blood\", \n",
    "    \"comp\", \"case\", \"control\", \"severe\", \"disease\", \"level\", \"high\", \"low\",\n",
    "    \"frequent\", \"risk\", \"measure\", \"ratio\", \"rate\", \"oxygen\", \"normal\"\n",
    "}\n",
    "\n",
    "print(f\"DEVICE: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27435a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. model lodading\n",
    "def load_gpt2(model_path: str = MODEL_PATH):\n",
    "    print(f\"Loading GPT-2 from {model_path} ...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(DEVICE)\n",
    "    model.eval()\n",
    "    print(\"  GPT-2 loaded.\")\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_gpt2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26439a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. kg loading and processing\n",
    "def load_kg(path: str = PRIMEKG_PATH) -> nx.Graph:\n",
    "    print(f\" Loading KG from {path} ...\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df, source=\"x_name\", target=\"y_name\", edge_attr=True\n",
    "    )\n",
    "    print(f\"KG loaded: {G.number_of_nodes()} nodes.\")\n",
    "    return G\n",
    "\n",
    "G = load_kg()\n",
    "node_index = {str(n).lower(): n for n in G.nodes()}\n",
    "\n",
    "def get_knowledge_context_single(entity_name: str, max_edges: int = MAX_K_EDGES) -> str | None:\n",
    "    node = node_index.get(entity_name.lower())\n",
    "    if not node: return None\n",
    "\n",
    "    edges = list(G.edges(node, data=True))\n",
    "    if not edges: return None\n",
    "    \n",
    "\n",
    "    selected_edges = edges[:max_edges]\n",
    "    \n",
    "    lines = []\n",
    "    for u, v, attr in selected_edges:\n",
    "        rel = attr.get(\"display_relation\", attr.get(\"relation\", \"related to\"))\n",
    "        # neighbor node\n",
    "        neighbor = v if u == node else u\n",
    "        lines.append(f\"{node} is {rel} {neighbor}.\")\n",
    "        \n",
    "    return \" \".join(lines)\n",
    "\n",
    "def extract_rag_entities(text: str, max_entities: int = MAX_RAG_ENTITIES):\n",
    "\n",
    "    t = text.lower()\n",
    "    ents = []\n",
    "    candidates = []\n",
    "    words = set(t.split())\n",
    "    \n",
    "    for word in words:\n",
    "        if len(word) < 4: continue\n",
    "        if word in ENTITY_BLACKLIST: continue\n",
    "        \n",
    "        # check if in node index\n",
    "        if word in node_index:\n",
    "            ents.append(node_index[word])\n",
    "            if len(ents) >= max_entities:\n",
    "                break\n",
    "                \n",
    "    return ents\n",
    "\n",
    "def get_knowledge_context_multi(text: str):\n",
    "    entities = extract_rag_entities(text)\n",
    "    if not entities:\n",
    "        return [], \"\"\n",
    "    \n",
    "    facts_all = []\n",
    "    for ent in entities:\n",
    "        ctx = get_knowledge_context_single(ent)\n",
    "        if ctx:\n",
    "            facts_all.append(ctx)\n",
    "            \n",
    "    return entities, \" \".join(facts_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ae020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. data loading\n",
    "def load_pubmedqa_example(idx: int, file_path: str = PUBMEDQA_FILE):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    row = df.iloc[idx]\n",
    "    \n",
    "    return {\n",
    "        \"pmid\": str(row.get(\"pubid\")),\n",
    "        \"question\": str(row.get(\"question\") or \"\"),\n",
    "        \"context\": str(row.get(\"long_answer\") or \"\"), # use long_answer as context\n",
    "        \"answer\": str(row.get(\"final_decision\")).lower().strip()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. middle core: scoring function and prompt construction (with truncation fix)\n",
    "def score_label(prompt: str, label: str) -> float:\n",
    "    # Prompt + Label\n",
    "    full_text = prompt + \" \" + label\n",
    "    enc = tokenizer(full_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "\n",
    "    if input_ids.shape[1] > 1024:\n",
    "        input_ids = input_ids[:, -1024:] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss.item()\n",
    "    return -loss\n",
    "\n",
    "def build_prompt(question: str, context: str, facts: str = \"\") -> str:\n",
    "    instruction = (\n",
    "        \"Question Answering task.\\n\"\n",
    "        \"Fact: {facts}\\n\"\n",
    "        \"Abstract: {context}\\n\"\n",
    "        \"Question: {question}\\n\"\n",
    "        \"Answer (yes/no/maybe):\"\n",
    "    )\n",
    "    \n",
    "    # Estimated length (character-level rough estimate, 1 token ≈ 4 chars)\n",
    "    safe_char_limit = 3000 \n",
    "    \n",
    "    current_len = len(question) + len(facts) + 100 # 100 is template buffer\n",
    "    available_for_context = safe_char_limit - current_len\n",
    "    \n",
    "    if available_for_context < 100:\n",
    "        available_for_context = 100 \n",
    "        \n",
    "    context_truncated = context[:available_for_context]\n",
    "    # if no facts, do not show \"Fact: ...\" line\n",
    "    facts_str = facts if facts else \"None\"\n",
    "    \n",
    "    return instruction.format(\n",
    "        facts=facts_str,\n",
    "        context=context_truncated,\n",
    "        question=question\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pubmedqa_acc(start_idx=0, end_idx=50, save_path=\"pubmedqa_results.csv\"):\n",
    "    correct_no = 0    # No RAG\n",
    "    correct_rag = 0   # RAG\n",
    "    rag_triggered = 0 # search found knowledge\n",
    "    improvements = 0  # RAG fixed errors\n",
    "    hurts = 0         # RAG failed errors\n",
    "    \n",
    "    logs = [] # used to store each detailed result\n",
    "\n",
    "    print(f\"Starting Evaluation [{start_idx} to {end_idx}]...\")\n",
    "\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        try:\n",
    "            ex = load_pubmedqa_example(idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {idx}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        q, c, gt = ex[\"question\"], ex[\"context\"], ex[\"answer\"]\n",
    "        if gt not in [\"yes\", \"no\", \"maybe\"]: continue\n",
    "\n",
    "        # 1. Baseline (no RAG)\n",
    "        prompt_no = build_prompt(q, c, facts=\"\")\n",
    "        scores_no = {lab: score_label(prompt_no, lab) for lab in [\"yes\", \"no\", \"maybe\"]}\n",
    "        pred_no = max(scores_no, key=scores_no.get)\n",
    "        \n",
    "\n",
    "        # 2. With RAG (RAG)\n",
    "        ents, facts = get_knowledge_context_multi(q + \" \" + c)\n",
    "        \n",
    "        if ents and facts:\n",
    "            rag_triggered += 1\n",
    "            prompt_rag = build_prompt(q, c, facts=facts)\n",
    "            scores_rag = {lab: score_label(prompt_rag, lab) for lab in [\"yes\", \"no\", \"maybe\"]}\n",
    "            pred_rag = max(scores_rag, key=scores_rag.get)\n",
    "        else:\n",
    "            pred_rag = pred_no\n",
    "            \n",
    "        # 3. Statistics and Comparisons\n",
    "        if pred_no == gt: correct_no += 1\n",
    "        if pred_rag == gt: correct_rag += 1\n",
    "        \n",
    "        status = \"SAME\"\n",
    "        if pred_no != gt and pred_rag == gt:\n",
    "            improvements += 1\n",
    "            status = \"  FIXED\"\n",
    "        elif pred_no == gt and pred_rag != gt:\n",
    "            hurts += 1\n",
    "            status = \"  BROKE\"\n",
    "            \n",
    "        print(f\"[{idx}] GT={gt} | No={pred_no} | RAG={pred_rag} | {status} | Ents={ents}\")\n",
    "    \n",
    "        logs.append({\n",
    "            \"id\": idx,\n",
    "            \"ground_truth\": gt,\n",
    "            \"pred_no_rag\": pred_no,\n",
    "            \"pred_rag\": pred_rag,\n",
    "            \"status\": status,\n",
    "            \"entities\": str(ents),\n",
    "            \"rag_facts\": facts[:200] if facts else \"\"\n",
    "        })\n",
    "\n",
    "\n",
    "    # 4. Accuracy\n",
    "    total = len(logs)\n",
    "    if total == 0:\n",
    "        print(\"No valid samples processed.\")\n",
    "        return\n",
    "\n",
    "    acc_no = correct_no / total\n",
    "    acc_rag = correct_rag / total\n",
    "\n",
    "    summary_lines = [\n",
    "        \"=\"*40,\n",
    "        f\"RESULTS (Total Samples: {total})\",\n",
    "        f\"No RAG Accuracy   : {acc_no:.4f}  ({correct_no}/{total})\",\n",
    "        f\"With RAG Accuracy : {acc_rag:.4f}  ({correct_rag}/{total})\",\n",
    "        \"-\" * 20,\n",
    "        f\"RAG Triggered     : {rag_triggered}\",\n",
    "        f\"Improvements      : {improvements}\",\n",
    "        f\"Hurts             : {hurts}\",\n",
    "        \"=\"*40\n",
    "    ]\n",
    "    \n",
    "    summary_text = \"\\n\".join(summary_lines)\n",
    "\n",
    "    print(\"\\n\" + summary_text)\n",
    "\n",
    "    # save summary to TXT\n",
    "    summary_path = \"summary_stats.txt\"\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary_text)\n",
    "\n",
    "    # save detailed logs to CSV\n",
    "    df = pd.DataFrame(logs)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(f\"\\nDetailed logs saved to: {save_path}\")\n",
    "    print(f\"Summary stats saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_pubmedqa_acc(0, 50, \"my_experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54655244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': '23506394',\n",
       " 'question': 'Malnutrition, a new inducer for arterial calcification in hemodialysis patients?',\n",
       " 'context': 'Malnutrition is prevalent in hemodialysis patients and is associated with arterial calcification and the expressions of BMP2 and MGP in calcified radial arteries. Malnutrition may be a new inducer candidate for arterial calcification in hemodialysis patients.',\n",
       " 'answer': 'yes'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex0 = load_pubmedqa_example(0)\n",
    "ex0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate_pubmedqa_acc(start_idx=0, end_idx=50)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b845652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# 0. Configuration Parameters\n",
    "class Config:\n",
    "    # Note: For Windows paths, it is recommended to add 'r' before the quotes to prevent escape character errors\n",
    "    parquet_path = r\"C:\\LLM\\data\\pubmedqa_hf\\pqa_labeled_splits\\test.parquet\"\n",
    "    \n",
    "    # Model path (Ensure model files exist in this path, or fill in a huggingface hub id like \"gpt2\")\n",
    "    model_path = r\"C:\\LLM\\gpt2\" \n",
    "    \n",
    "    # Runtime parameters\n",
    "    limit = 50              # Recommended to set small for testing (e.g., 10-50), set to 200 or None after successful run\n",
    "    use_chat_template = False\n",
    "    max_ctx_chars = 4000\n",
    "    seed = 2025\n",
    "\n",
    "args = Config()\n",
    "\n",
    "# Set random seed\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "\n",
    "# 1. RAG Retrieval Interface (Mock)\n",
    "def retrieve_knowledge(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Mock retrieval logic. In real scenarios, this should call VectorDB or Search API.\n",
    "    \"\"\"\n",
    "    q_lower = question.lower()\n",
    "    if \"mutation\" in q_lower:\n",
    "        return \"Fact: Specific gene mutations (e.g., BRCA1) are strong predictors for this condition.\"\n",
    "    if \"treatment\" in q_lower:\n",
    "        return \"Fact: Standard treatment involves a combination of chemotherapy and radiation therapy.\"\n",
    "    if \"risk\" in q_lower:\n",
    "        return \"Fact: High BMI and smoking are significant risk factors.\"\n",
    "    return \"\" \n",
    "\n",
    "\n",
    "# 2. Helper Functions Definition\n",
    "def join_context(c, max_chars):\n",
    "    try:\n",
    "        # Handle potential None or dictionary structures\n",
    "        ctxs = (c or {}).get(\"contexts\", [])\n",
    "        return \" \".join(ctxs)[:max_chars] if ctxs else \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def build_prompt(tok, q, ctx, facts=\"\"):\n",
    "    system_msg = \"You are a helpful biomedical QA assistant.\"\n",
    "    \n",
    "    if facts:\n",
    "        #With RAG\n",
    "        user_content = (\n",
    "            f\"Retrieval Knowledge: {facts}\\n\"\n",
    "            f\"Question: {q}\\nContext: {ctx}\\n\"\n",
    "            \"Answer the question in detail.\"\n",
    "        )\n",
    "        text_prompt = (\n",
    "            f\"Retrieval Knowledge: {facts}\\n\"\n",
    "            f\"Question: {q}\\nContext: {ctx}\\n\\n\"\n",
    "            \"Answer the question in detail using the context and retrieved knowledge.\\nAnswer: \"\n",
    "        )\n",
    "    else:\n",
    "        #No RAG\n",
    "        user_content = (\n",
    "            f\"Question: {q}\\nContext: {ctx}\\n\"\n",
    "            \"Answer the question in detail.\"\n",
    "        )\n",
    "        text_prompt = (\n",
    "            f\"Question: {q}\\nContext: {ctx}\\n\\n\"\n",
    "            \"Answer the question in detail using the context above.\\nAnswer: \"\n",
    "        )\n",
    "\n",
    "    # Prioritize using Chat Template (if model supports)\n",
    "    if args.use_chat_template and hasattr(tok, \"apply_chat_template\"):\n",
    "        msgs = [{\"role\": \"system\", \"content\": system_msg}, {\"role\": \"user\", \"content\": user_content}]\n",
    "        try:\n",
    "            return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        except Exception:\n",
    "            return text_prompt # Fallback to simple concatenation\n",
    "    \n",
    "    return text_prompt\n",
    "\n",
    "def clean_output(full_text, prompt_len):\n",
    "    \"\"\"Clean model generated output, remove Prompt part and redundant stop tokens\"\"\"\n",
    "    gen_text = full_text[prompt_len:].strip()\n",
    "    stop_tokens = [\"\\nQuestion:\", \"\\nContext:\", \"Answer:\", \"Reference:\"]\n",
    "    for stop_t in stop_tokens:\n",
    "        if stop_t in gen_text:\n",
    "            gen_text = gen_text.split(stop_t)[0].strip()\n",
    "    return gen_text if gen_text else \"Error: No answer generated.\"\n",
    "\n",
    "\n",
    "# 3. Data Loading\n",
    "print(f\" Reading data: {args.parquet_path}\")\n",
    "\n",
    "if not os.path.exists(args.parquet_path):\n",
    "    print(f\" Warning: File not found {args.parquet_path}.\")\n",
    "    print(\">>> Creating mock data for demonstration...\")\n",
    "    questions = [\"Is mutation X related to cancer?\", \"What is the treatment for flu?\"] * (args.limit // 2 + 1)\n",
    "    ctx_list = [\"Context about mutation X.\", \"Context about flu.\"] * (args.limit // 2 + 1)\n",
    "    refs = [\"Yes, it is related.\", \"Rest and fluids.\"] * (args.limit // 2 + 1)\n",
    "    \n",
    "    # Truncate to limit\n",
    "    questions = questions[:args.limit]\n",
    "    ctx_list = ctx_list[:args.limit]\n",
    "    refs = refs[:args.limit]\n",
    "else:\n",
    "    try:\n",
    "        tbl = pq.read_table(args.parquet_path)\n",
    "        df = tbl.to_pandas().dropna().head(args.limit)\n",
    "        \n",
    "        # Define key variables\n",
    "        questions = df[\"question\"].tolist()\n",
    "        # Adjust column name reading logic based on dataset structure\n",
    "        ctx_list = df[\"context\"].map(lambda c: join_context(c, args.max_ctx_chars)).tolist()\n",
    "        \n",
    "        target_col = \"long_answer\" if \"long_answer\" in df.columns else \"final_decision\"\n",
    "        if target_col not in df.columns:\n",
    "            # Final fallback to prevent missing columns\n",
    "            target_col = df.columns[-1]\n",
    "            \n",
    "        refs = df[target_col].tolist()\n",
    "        print(f\"  Data loading completed, total {len(questions)} samples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to read Parquet: {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "# 4. Model Loading\n",
    "print(f\" Loading model: {args.model_path}\")\n",
    "\n",
    "try:\n",
    "    # Prioritize local loading\n",
    "    tok = AutoTokenizer.from_pretrained(args.model_path, use_fast=True, local_files_only=True)\n",
    "except:\n",
    "    print(\"Local load failed, trying to load tokenizer online...\")\n",
    "    tok = AutoTokenizer.from_pretrained(args.model_path, use_fast=True)\n",
    "\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Automatically select precision\n",
    "if torch.cuda.is_available():\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    print(f\" Using GPU: {torch.cuda.get_device_name(0)} ({dtype})\")\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "    print(\" Using CPU\")\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_path,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"  Model load failed: {e}\")\n",
    "    # If local gpt2 is missing, provide an automatic download fallback here\n",
    "    if args.model_path == r\"C:\\LLM\\gpt2\": \n",
    "        print(\"Attempting to download gpt2 from HuggingFace...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=dtype, device_map=\"auto\")\n",
    "        tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        tok.pad_token = tok.eos_token\n",
    "    else:\n",
    "        exit()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 5. Inference Loop\n",
    "# Create experiment directory\n",
    "time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_tag = os.path.basename(args.model_path.rstrip(\"/\\\\\")) # Compatible with Windows/Linux path separators\n",
    "run_dir = os.path.join(\"eval_out_notebook\", f\"run_{time_str}_{model_tag}\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "preds_no = []\n",
    "preds_rag = []\n",
    "facts_log = []\n",
    "\n",
    "print(f\"\\n Starting Inference (Limit={len(questions)})...\")\n",
    "\n",
    "for q, ctx in tqdm(zip(questions, ctx_list), total=len(questions), desc=\"Inference\"):\n",
    "    \n",
    "    #Generation Parameters\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"pad_token_id\": tok.pad_token_id,\n",
    "        \"eos_token_id\": tok.eos_token_id,\n",
    "        \"do_sample\": False,        # Greedy decoding\n",
    "        \"repetition_penalty\": 1.2\n",
    "    }\n",
    "\n",
    "    #A. No RAG\n",
    "    prompt_no = build_prompt(tok, q, ctx, facts=\"\")\n",
    "    inputs_no = tok(prompt_no, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_no = model.generate(**inputs_no, **gen_kwargs)\n",
    "    \n",
    "    input_len_no = inputs_no[\"input_ids\"].shape[1]\n",
    "    decoded_out_no = tok.decode(out_no[0], skip_special_tokens=True)\n",
    "    decoded_in_no = tok.decode(out_no[0][:input_len_no], skip_special_tokens=True)\n",
    "    \n",
    "    pred_no_clean = clean_output(decoded_out_no, len(decoded_in_no))\n",
    "    preds_no.append(pred_no_clean)\n",
    "    \n",
    "    #B. With RAG\n",
    "    retrieved_facts = retrieve_knowledge(q)\n",
    "    facts_log.append(retrieved_facts)\n",
    "    \n",
    "    prompt_rag = build_prompt(tok, q, ctx, facts=retrieved_facts)\n",
    "    inputs_rag = tok(prompt_rag, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_rag = model.generate(**inputs_rag, **gen_kwargs)\n",
    "        \n",
    "    input_len_rag = inputs_rag[\"input_ids\"].shape[1]\n",
    "    decoded_out_rag = tok.decode(out_rag[0], skip_special_tokens=True)\n",
    "    decoded_in_rag = tok.decode(out_rag[0][:input_len_rag], skip_special_tokens=True)\n",
    "    \n",
    "    pred_rag_clean = clean_output(decoded_out_rag, len(decoded_in_rag))\n",
    "    preds_rag.append(pred_rag_clean)\n",
    "\n",
    "\n",
    "# 6. Metrics Calculation and Saving\n",
    "print(\"\\n Calculating ROUGE scores...\")\n",
    "try:\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    \n",
    "    # Calculate ROUGE\n",
    "    scores_no = rouge.compute(predictions=preds_no, references=refs, use_aggregator=False)[\"rougeLsum\"]\n",
    "    scores_rag = rouge.compute(predictions=preds_rag, references=refs, use_aggregator=False)[\"rougeLsum\"]\n",
    "\n",
    "    avg_no = np.mean(scores_no)\n",
    "    avg_rag = np.mean(scores_rag)\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\" Results Summary (n={len(questions)})\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\" No RAG Avg ROUGE-Lsum : {avg_no:.4f}\")\n",
    "    print(f\" w/ RAG Avg ROUGE-Lsum : {avg_rag:.4f}\")\n",
    "    print(f\" Improvement           : {(avg_rag - avg_no):.4f}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\" ROUGE calculation failed (possibly due to network issues downloading metric): {e}\")\n",
    "    scores_no = [0.0] * len(questions)\n",
    "    scores_rag = [0.0] * len(questions)\n",
    "\n",
    "# Save results\n",
    "df_res = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"retrieved_facts\": facts_log,\n",
    "    \"ref\": refs,\n",
    "    \"pred_no_rag\": preds_no,\n",
    "    \"pred_with_rag\": preds_rag,\n",
    "    \"score_no\": scores_no,\n",
    "    \"score_rag\": scores_rag,\n",
    "    \"diff\": np.array(scores_rag) - np.array(scores_no)\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(run_dir, \"results.csv\")\n",
    "df_res.to_csv(csv_path, index=False)\n",
    "print(f\"Detailed results saved: {csv_path}\")\n",
    "\n",
    "# Visualize Top 3\n",
    "if len(df_res) > 0:\n",
    "    print(\"\\n Top 3 Most Improved Cases by RAG:\")\n",
    "    top_improved = df_res.sort_values(by=\"diff\", ascending=False).head(3)\n",
    "\n",
    "    for idx, row in top_improved.iterrows():\n",
    "        print(f\"\\n Index: {idx} | Diff: +{row['diff']:.4f}\")\n",
    "        print(f\" Q: {row['question']}\")\n",
    "        print(f\" Facts: {row['retrieved_facts']}\")\n",
    "        print(f\" No RAG: {str(row['pred_no_rag'])[:100]}...\")\n",
    "        print(f\" w/ RAG: {str(row['pred_with_rag'])[:100]}...\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7835651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import evaluate\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# 0. Configuration Parameters\n",
    "class Config:\n",
    "    parquet_path = r\"C:\\LLM\\data\\pubmedqa_hf\\pqa_labeled_splits\\test.parquet\"\n",
    "    model_path = r\"C:\\LLM\\gpt2\" \n",
    "    embedding_model = \"all-MiniLM-L6-v2\" \n",
    "    limit = 50\n",
    "    seed = 2025\n",
    "\n",
    "args = Config()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# 1. Vector Retrieval Module\n",
    "class VectorDB:\n",
    "    def __init__(self, model_name):\n",
    "        print(f\" Loading embedding model: {model_name}...\")\n",
    "        # Replaced random numbers in Kaggle code with real AI semantic understanding\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.docs = []\n",
    "    \n",
    "    def create_index(self, documents):\n",
    "        \"\"\"Step 1: Index documents with FAISS\"\"\"\n",
    "        print(\"Building vector index (FAISS)...\")\n",
    "        self.docs = documents\n",
    "        # Generate real semantic vectors\n",
    "        embeddings = self.model.encode(documents, convert_to_numpy=True, show_progress_bar=True)\n",
    "        \n",
    "        # Initialize FAISS\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(embeddings)\n",
    "        print(f\" Index build completed, total {len(documents)} documents.\")\n",
    "\n",
    "    def search(self, query, top_k=1):\n",
    "        \"\"\"Step 2: Retrieve relevant documents\"\"\"\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Return the most relevant document found\n",
    "        results = [self.docs[i] for i in indices[0]]\n",
    "        return results[0] # Taking only Top 1 for simplicity\n",
    "\n",
    "\n",
    "# 2. Data Processing and Helper Functions\n",
    "def join_context(c):\n",
    "    try:\n",
    "        ctxs = (c or {}).get(\"contexts\", [])\n",
    "        return \" \".join(ctxs) if ctxs else \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def clean_output(text):\n",
    "    if \"Answer:\" in text:\n",
    "        return text.split(\"Answer:\")[-1].strip()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# 3. Load Data\n",
    "print(f\" Reading data: {args.parquet_path}\")\n",
    "if not os.path.exists(args.parquet_path):\n",
    "    print(\" File not found\")\n",
    "    exit()\n",
    "\n",
    "tbl = pq.read_table(args.parquet_path)\n",
    "df = tbl.to_pandas().dropna().head(args.limit)\n",
    "\n",
    "# Prepare \"Knowledge Base\": Treat all Contexts in the test set as a huge database\n",
    "# In real scenarios, this could be millions of papers\n",
    "database_contexts = df[\"context\"].map(join_context).tolist()\n",
    "questions = df[\"question\"].tolist()\n",
    "refs = df[\"long_answer\"].tolist() if \"long_answer\" in df.columns else df[df.columns[-1]].tolist()\n",
    "\n",
    "\n",
    "# 4. Initialize FAISS Vector Database\n",
    "vector_db = VectorDB(args.embedding_model)\n",
    "vector_db.create_index(database_contexts)\n",
    "\n",
    "\n",
    "# 5. Load GPT-2\n",
    "print(f\" Loading generation model: {args.model_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path, use_fast=True, local_files_only=True)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_path, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 6. Run Real RAG Pipeline\n",
    "preds = []\n",
    "print(\"\\n Starting Real RAG Inference (Search -> Generate)...\")\n",
    "\n",
    "for q in tqdm(questions, desc=\"RAG Pipeline\"):\n",
    "    \n",
    "    # Step A: Retrieve\n",
    "    # This time we don't provide the correct answer, let the model search in the library!\n",
    "    retrieved_ctx = vector_db.search(q, top_k=1)\n",
    "    \n",
    "    # Step B: Augment\n",
    "    # Use Prompt format from your Kaggle code\n",
    "    # Note truncation length to prevent GPT-2 OOM (1024 token limit)\n",
    "    truncated_ctx = retrieved_ctx[:800] \n",
    "    \n",
    "    prompt = (\n",
    "        f\"Use the following context to answer the query:\\n\\n\"\n",
    "        f\"Context: {truncated_ctx}\\n\\n\"\n",
    "        f\"Query: {q}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # Step C: Generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Protection mechanism: If Prompt is too long, force truncate\n",
    "    if inputs[\"input_ids\"].shape[1] > 1024:\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"][:, -1024:]\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # max_new_tokens controls generation length\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.pad_token_id, do_sample=False)\n",
    "        \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean result, keep only the part after Answer\n",
    "    final_ans = clean_output(response[len(prompt):]) # Take only the newly generated part\n",
    "    preds.append(final_ans)\n",
    "\n",
    "\n",
    "# 7. Evaluation (BERTScore)\n",
    "print(\"\\n Calculating BERTScore...\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "res = bertscore.compute(predictions=preds, references=refs, lang=\"en\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "avg_score = np.mean(res['f1'])\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\" GPT-2 + FAISS Real RAG Results\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\" Average BERTScore F1: {avg_score:.4f}\")\n",
    "print(f\"{'='*40}\\n\")\n",
    "\n",
    "# Save results\n",
    "df_res = pd.DataFrame({\"question\": questions, \"retrieved_context\": [vector_db.search(q) for q in questions], \"pred\": preds, \"ref\": refs})\n",
    "df_res.to_csv(\"results_gpt2_faiss.csv\", index=False)\n",
    "print(\" Results saved to results_gpt2_faiss.csv\")\n",
    "\n",
    "# Show what was retrieved\n",
    "print(\"\\n Retrieval Effect Demonstration (First Entry):\")\n",
    "print(f\"Q: {questions[0]}\")\n",
    "print(f\" Retrieved: {df_res.iloc[0]['retrieved_context'][:100]}...\")\n",
    "print(f\" GPT-2 Answer: {preds[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import evaluate\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# 0. Configuration Parameters\n",
    "class Config:\n",
    "    parquet_path = r\"C:\\LLM\\data\\pubmedqa_hf\\pqa_labeled_splits\\test.parquet\"\n",
    "    model_path = r\"C:\\LLM\\gpt2\" \n",
    "    embedding_model = \"all-MiniLM-L6-v2\"\n",
    "    limit = 50\n",
    "    seed = 2025\n",
    "\n",
    "args = Config()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "# 1. Vector Retrieval Module\n",
    "class VectorDB:\n",
    "    def __init__(self, model_name):\n",
    "        print(f\" Loading embedding model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.docs = []\n",
    "    \n",
    "    def create_index(self, documents):\n",
    "        print(\" Building FAISS index...\")\n",
    "        self.docs = documents\n",
    "        embeddings = self.model.encode(documents, convert_to_numpy=True, show_progress_bar=True)\n",
    "        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    def search(self, query, top_k=1):\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        _, indices = self.index.search(query_embedding, top_k)\n",
    "        return self.docs[indices[0][0]]\n",
    "\n",
    "\n",
    "# 2. Prompt Construction (GPT-2 Specialized)\n",
    "def build_prompt_native(q):\n",
    "    \"\"\"Native mode: Ask directly without providing materials\"\"\"\n",
    "    return f\"Question: {q}\\nAnswer:\"\n",
    "\n",
    "def build_prompt_rag(q, ctx):\n",
    "    \"\"\"RAG mode: Provide retrieved materials + question\"\"\"\n",
    "    # Simplify Prompt slightly, try to reduce GPT-2's repetitive behavior\n",
    "    return (\n",
    "        f\"Context: {ctx[:800]}\\n\\n\" # Truncate to prevent OOM\n",
    "        f\"Question: {q}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "def clean_output(full_text, prompt_len):\n",
    "    gen = full_text[prompt_len:].strip()\n",
    "    # Truncate common stop words\n",
    "    for stop in [\"\\nQuestion:\", \"\\nContext:\", \"Question:\"]:\n",
    "        if stop in gen:\n",
    "            gen = gen.split(stop)[0].strip()\n",
    "    return gen if gen else \"Error\"\n",
    "\n",
    "\n",
    "# 3. Preparation\n",
    "# Load data\n",
    "if not os.path.exists(args.parquet_path):\n",
    "    print(\" Data file not found\")\n",
    "    exit()\n",
    "tbl = pq.read_table(args.parquet_path)\n",
    "df = tbl.to_pandas().dropna().head(args.limit)\n",
    "database_contexts = df[\"context\"].map(lambda c: \" \".join((c or {}).get(\"contexts\", []))).tolist()\n",
    "questions = df[\"question\"].tolist()\n",
    "refs = df[\"long_answer\"].tolist() if \"long_answer\" in df.columns else df[df.columns[-1]].tolist()\n",
    "\n",
    "# Initialize FAISS\n",
    "vector_db = VectorDB(args.embedding_model)\n",
    "vector_db.create_index(database_contexts)\n",
    "\n",
    "# Load GPT-2\n",
    "print(f\" Loading GPT-2: {args.model_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path, use_fast=True, local_files_only=True)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_path, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 4. Comparative Inference Loop\n",
    "print(f\"\\n Starting comparative experiment (Limit={len(questions)})...\")\n",
    "preds_native = []\n",
    "preds_rag = []\n",
    "retrieved_docs = []\n",
    "\n",
    "for q in tqdm(questions, desc=\"Inference\"):\n",
    "    \n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"do_sample\": False # Greedy decoding\n",
    "    }\n",
    "\n",
    "    #A. Native GPT-2\n",
    "    prompt_native = build_prompt_native(q)\n",
    "    inputs_native = tokenizer(prompt_native, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_native = model.generate(**inputs_native, **gen_kwargs)\n",
    "        \n",
    "    res_native = clean_output(tokenizer.decode(out_native[0], skip_special_tokens=True), inputs_native[\"input_ids\"].shape[1])\n",
    "    preds_native.append(res_native)\n",
    "\n",
    "    #B. RAG GPT-2\n",
    "    # 1. Retrieve\n",
    "    ctx = vector_db.search(q)\n",
    "    retrieved_docs.append(ctx)\n",
    "    \n",
    "    # 2. Generate\n",
    "    prompt_rag = build_prompt_rag(q, ctx)\n",
    "    inputs_rag = tokenizer(prompt_rag, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Length protection\n",
    "    if inputs_rag[\"input_ids\"].shape[1] > 1000:\n",
    "        inputs_rag[\"input_ids\"] = inputs_rag[\"input_ids\"][:, -1000:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_rag = model.generate(**inputs_rag, **gen_kwargs)\n",
    "\n",
    "    res_rag = clean_output(tokenizer.decode(out_rag[0], skip_special_tokens=True), inputs_rag[\"input_ids\"].shape[1])\n",
    "    preds_rag.append(res_rag)\n",
    "\n",
    "\n",
    "# 5. BERTScore Showdown\n",
    "print(\"\\n Calculating BERTScore...\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\" Calculating Native scores...\")\n",
    "f1_native = np.array(bertscore.compute(predictions=preds_native, references=refs, lang=\"en\", device=device, batch_size=32)['f1'])\n",
    "\n",
    "print(\" Calculating RAG scores...\")\n",
    "f1_rag = np.array(bertscore.compute(predictions=preds_rag, references=refs, lang=\"en\", device=device, batch_size=32)['f1'])\n",
    "\n",
    "avg_native = f1_native.mean()\n",
    "avg_rag = f1_rag.mean()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\" GPT-2 Native vs. RAG (Real World)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\" Native Score : {avg_native:.4f}\")\n",
    "print(f\" RAG Score    : {avg_rag:.4f}\")\n",
    "print(f\" Net Impact       : {(avg_rag - avg_native):.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "if avg_rag < avg_native:\n",
    "    print(\" Conclusion: RAG caused performance degradation! GPT-2 got confused by retrieved documents.\")\n",
    "else:\n",
    "    print(\" Conclusion: RAG improved performance.\")\n",
    "\n",
    "# Save comparison results\n",
    "df_res = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"pred_native\": preds_native,\n",
    "    \"pred_rag\": preds_rag,\n",
    "    \"score_native\": f1_native,\n",
    "    \"score_rag\": f1_rag,\n",
    "    \"diff\": f1_rag - f1_native\n",
    "})\n",
    "df_res.to_csv(\"results_gpt2_comparison.csv\", index=False)\n",
    "print(\"\\n Detailed comparison saved to: results_gpt2_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67988e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import fuzz\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 0. Global Configuration\n",
    "class Config:\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Path Configuration\n",
    "    GPT2_PATH = r\"C:\\LLM\\gpt2\"       # Path to GPT-2\n",
    "    LLAMA2_PATH = r\"C:\\LLM\\llama2\"   # Path to LLaMA-2\n",
    "    \n",
    "    # Note: If you don't have kg.csv, the code will automatically create a tiny mock graph to ensure it runs.\n",
    "    PRIMEKG_PATH = \"kg.csv\"          \n",
    "    \n",
    "    PUBMEDQA_PATH = r\"C:\\LLM\\data\\pubmedqa_hf\\pqa_labeled_splits\\test.parquet\"\n",
    "    \n",
    "    MAX_CTX_EDGES = 5   # Limit context length (GPT-2 context window is small, too many will cause overflow)\n",
    "    EVAL_LIMIT = 20     # Number of test samples\n",
    "    \n",
    "# Load NLP Tools\n",
    "print(\"Loading Spacy NER model...\")\n",
    "try:\n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    print(\"Spacy model not found, downloading...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# 1. Model Loader\n",
    "def load_model(model_type=\"gpt2\"):\n",
    "    print(f\"Loading model: {model_type}...\")\n",
    "    if model_type.lower() == \"gpt2\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.GPT2_PATH, use_fast=True, local_files_only=True)\n",
    "        if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(Config.GPT2_PATH).to(Config.DEVICE).eval()\n",
    "    elif model_type.lower() == \"llama2\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.LLAMA2_PATH, use_fast=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(Config.LLAMA2_PATH, torch_dtype=torch.float16, device_map=\"auto\").eval()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# 2. PrimeKG Graph Database Management\n",
    "def load_primekg(path=Config.PRIMEKG_PATH):\n",
    "    # If file does not exist, create a mock Knowledge Graph (Mock KG) for demonstration\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"   {path} not found. Creating Mock Knowledge Graph (Mock Mode)...\")\n",
    "        G = nx.Graph()\n",
    "        # Add some medical common sense for testing\n",
    "        G.add_edge(\"warfarin\", \"bleeding\", relation=\"causes\", x_type=\"Drug\", y_type=\"Effect\")\n",
    "        G.add_edge(\"aspirin\", \"fever\", relation=\"treats\", x_type=\"Drug\", y_type=\"Symptom\")\n",
    "        G.add_edge(\"smoking\", \"lung cancer\", relation=\"causes\", x_type=\"Behavior\", y_type=\"Disease\")\n",
    "        G.add_edge(\"diabetes\", \"insulin\", relation=\"treated_by\", x_type=\"Disease\", y_type=\"Drug\")\n",
    "        G.add_edge(\"hypertension\", \"stroke\", relation=\"risk_factor_for\", x_type=\"Disease\", y_type=\"Disease\")\n",
    "        print(\"   Mock graph created.\")\n",
    "        return G\n",
    "    \n",
    "    print(f\"Loading real PrimeKG: {path} ...\")\n",
    "    try:\n",
    "        df = pd.read_csv(path, low_memory=False)\n",
    "        # Assume CSV has x_name, y_name, relation columns\n",
    "        G = nx.from_pandas_edgelist(df, source=\"x_name\", target=\"y_name\", edge_attr=[\"relation\", \"x_type\", \"y_type\"])\n",
    "        print(f\"   PrimeKG Loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "        return G\n",
    "    except Exception as e:\n",
    "        print(f\"   Failed to load CSV: {e}\")\n",
    "        return nx.Graph()\n",
    "\n",
    "# Node matching index (speed up lookup)\n",
    "_node_cache = {}\n",
    "\n",
    "def resolve_node(G, name):\n",
    "    \"\"\"\n",
    "    Find the closest node name in the graph\n",
    "    \"\"\"\n",
    "    name = name.lower().strip()\n",
    "    if not _node_cache:\n",
    "        # Build cache\n",
    "        for n in G.nodes():\n",
    "            _node_cache[str(n).lower()] = n\n",
    "            \n",
    "    # 1. Exact match\n",
    "    if name in _node_cache:\n",
    "        return _node_cache[name]\n",
    "    \n",
    "    # 2. Fuzzy match (Slow for large graphs, recommended only for Mock Mode or small graphs)\n",
    "    if G.number_of_nodes() < 10000:\n",
    "        choices = list(G.nodes())\n",
    "        best_match, score = None, 0\n",
    "        for node in choices:\n",
    "            s = fuzz.ratio(name, str(node).lower())\n",
    "            if s > score:\n",
    "                score = s\n",
    "                best_match = node\n",
    "        \n",
    "        if score > 80:\n",
    "            return best_match\n",
    "            \n",
    "    return None\n",
    "\n",
    "\n",
    "# 3. Core RAG Retrieval Logic (Key Fix)\n",
    "def extract_keywords(question):\n",
    "    \"\"\"\n",
    "    Improvement: Extract key nouns from the sentence, not just named entities\n",
    "    \"\"\"\n",
    "    doc = NER(question)\n",
    "    # Prioritize entities\n",
    "    keywords = [ent.text for ent in doc.ents]\n",
    "    # If no entities, take noun chunks\n",
    "    if not keywords:\n",
    "        keywords = [chunk.text for chunk in doc.noun_chunks]\n",
    "    # If still nothing, take all words except stop words\n",
    "    if not keywords:\n",
    "        keywords = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "        \n",
    "    return keywords\n",
    "\n",
    "def get_knowledge_context(G, question, max_edges=Config.MAX_CTX_EDGES):\n",
    "    \"\"\"\n",
    "    Retrieve relevant triples from the Knowledge Graph\n",
    "    \"\"\"\n",
    "    keywords = extract_keywords(question)\n",
    "    found_facts = []\n",
    "    \n",
    "    for kw in keywords:\n",
    "        node = resolve_node(G, kw)\n",
    "        if node:\n",
    "            # Get neighbor nodes\n",
    "            edges = list(G.edges(node, data=True))\n",
    "            # Limit quantity to prevent Prompt from being too long\n",
    "            for u, v, attr in edges[:3]: # Find at most 3 edges per keyword\n",
    "                rel = attr.get('relation', 'related_to')\n",
    "                target = v if u == node else u\n",
    "                fact = f\"{node} --[{rel}]--> {target}\"\n",
    "                found_facts.append(fact)\n",
    "    \n",
    "    # Deduplicate and truncate\n",
    "    unique_facts = list(set(found_facts))[:max_edges]\n",
    "    \n",
    "    if not unique_facts:\n",
    "        return \"No specific knowledge found in graph.\"\n",
    "    \n",
    "    return \"; \".join(unique_facts)\n",
    "\n",
    "\n",
    "# 4. Generation and Prompt (Optimized for GPT-2)\n",
    "def generate_answer(tokenizer, model, model_type, context, question):\n",
    "    # Strong Few-Shot Prompt to make GPT-2 behave\n",
    "    if model_type == \"gpt2\":\n",
    "        prompt = (\n",
    "            \"Determine the relationship based on medical knowledge.\\n\\n\"\n",
    "            \"Context: Aspirin --[treats]--> fever\\n\"\n",
    "            \"Question: Is aspirin effective for fever?\\n\"\n",
    "            \"Answer: Yes\\n\\n\"\n",
    "            \"Context: Smoking --[causes]--> lung cancer\\n\"\n",
    "            \"Question: Does smoking improve lung capacity?\\n\"\n",
    "            \"Answer: No\\n\\n\"\n",
    "            f\"Context: {context}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "    else:\n",
    "        # Llama 2 format\n",
    "        prompt = f\"[INST] Context: {context}\\nQuestion: {question}\\nAnswer with Yes, No, or Maybe. [/INST]\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(Config.DEVICE)\n",
    "    \n",
    "    # Truncate to prevent overflow (GPT-2 limit 1024)\n",
    "    if inputs.input_ids.shape[1] > 1000:\n",
    "        inputs.input_ids = inputs.input_ids[:, -1000:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids, \n",
    "            max_new_tokens=10, \n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract answer part\n",
    "    # GPT-2 might repeat, we need to capture content after \"Answer:\"\n",
    "    if \"Answer:\" in full_text:\n",
    "        # Take content after the last \"Answer\" label\n",
    "        generated = full_text.split(\"Answer:\")[-1].strip().lower()\n",
    "    else:\n",
    "        generated = full_text.lower()\n",
    "\n",
    "    # Keyword matching\n",
    "    if \"yes\" in generated: return \"yes\"\n",
    "    if \"no\" in generated: return \"no\"\n",
    "    if \"maybe\" in generated: return \"maybe\"\n",
    "    \n",
    "    return \"maybe\" # Fallback strategy\n",
    "\n",
    "\n",
    "# 5. Evaluation Main Loop\n",
    "def evaluate_on_pubmedqa(G, tokenizer, model, model_type, limit=Config.EVAL_LIMIT, use_rag=True):\n",
    "    print(f\"\\n>>> Start Evaluation: {model_type.upper()} | RAG Mode: {use_rag}\")\n",
    "    \n",
    "    # Load data\n",
    "    if not os.path.exists(Config.PUBMEDQA_PATH):\n",
    "        print(f\"   Data file not found: {Config.PUBMEDQA_PATH}\")\n",
    "        return\n",
    "\n",
    "    df = pq.read_table(Config.PUBMEDQA_PATH).to_pandas().head(limit)\n",
    "    questions = df[\"question\"].tolist()\n",
    "    \n",
    "    # Extract ground truth labels (yes/no/maybe)\n",
    "    golds = []\n",
    "    for x in df[\"final_decision\"]:\n",
    "        x = str(x).lower().strip()\n",
    "        if x.startswith(\"y\"): golds.append(\"yes\")\n",
    "        elif x.startswith(\"n\"): golds.append(\"no\")\n",
    "        else: golds.append(\"maybe\")\n",
    "\n",
    "    preds = []\n",
    "    correct_count = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(total=limit)\n",
    "    \n",
    "    for i, q in enumerate(questions):\n",
    "        # 1. Retrieve\n",
    "        if use_rag:\n",
    "            context = get_knowledge_context(G, q)\n",
    "        else:\n",
    "            context = \"N/A\"\n",
    "            \n",
    "        # 2. Generate\n",
    "        pred = generate_answer(tokenizer, model, model_type, context, q)\n",
    "        preds.append(pred)\n",
    "        \n",
    "        # 3. Real-time statistics\n",
    "        if pred == golds[i]:\n",
    "            correct_count += 1\n",
    "            \n",
    "        pbar.set_description(f\"Acc: {correct_count/(i+1):.2%}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Print the first few examples to check performance\n",
    "        if i < 3:\n",
    "            print(f\"\\n[Case {i}]\")\n",
    "            print(f\"Q: {q}\")\n",
    "            print(f\"KG Context: {context}\")\n",
    "            print(f\"Pred: {pred} | Gold: {golds[i]}\")\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    acc = correct_count / limit\n",
    "    print(f\"\\nFinal Accuracy ({model_type}, RAG={use_rag}): {acc:.2%}\")\n",
    "\n",
    "# 6. Program Entry Point\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load KG\n",
    "    G = load_primekg()\n",
    "    \n",
    "    # 2. Load Model\n",
    "    tokenizer, model = load_model(\"gpt2\")\n",
    "    \n",
    "    # 3. Run Comparative Evaluation\n",
    "    # Test Native first (No RAG)\n",
    "    evaluate_on_pubmedqa(G, tokenizer, model, \"gpt2\", use_rag=False)\n",
    "    \n",
    "    # Test KG-RAG next (With Knowledge Graph)\n",
    "    evaluate_on_pubmedqa(G, tokenizer, model, \"gpt2\", use_rag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94db903",
   "metadata": {},
   "source": [
    "bnuild_pubmed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f288b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Prepare \"Paper Repository\" (Knowledge Base)\n",
    "\n",
    "parquet_path = r\"C:\\LLM\\data\\pubmedqa_hf\\pqa_labeled_splits\\test.parquet\"\n",
    "\n",
    "print(\"Loading dataset to build Knowledge Base...\")\n",
    "df = pq.read_table(parquet_path).to_pandas()\n",
    "# Treat the 'context' field of each row as a \"document\"\n",
    "# 'context' in parquet is usually a dictionary or list, needs to be joined into a string\n",
    "documents = []\n",
    "for raw_ctx in df[\"context\"]:\n",
    "    # Depending on your parquet structure, it could be a list or a dict\n",
    "    # Assuming standard format here, join sentences in the list\n",
    "    try:\n",
    "        if hasattr(raw_ctx, 'get'): # If it is a dict\n",
    "            text_list = raw_ctx.get('contexts', [])\n",
    "        else: # If it is a list\n",
    "            text_list = raw_ctx\n",
    "        \n",
    "        full_abstract = \" \".join(text_list)\n",
    "        documents.append(full_abstract)\n",
    "    except:\n",
    "        documents.append(\"\")\n",
    "\n",
    "print(f\"  Extracted {len(documents)} paper abstracts.\")\n",
    "\n",
    "\n",
    "# 2. Build FAISS Index (Text Index)\n",
    "print(\"Converting abstracts to vectors (Embeddings)...\")\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embed_model.encode(documents, convert_to_numpy=True, show_progress_bar=True)\n",
    "# Build index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "print(\"  FAISS index construction completed!\")\n",
    "\n",
    "\n",
    "# 3. Retrieval Function\n",
    "\n",
    "def retrieve_paper(question, top_k=1):\n",
    "    q_emb = embed_model.encode([question], convert_to_numpy=True)\n",
    "    distances, indices = index.search(q_emb, top_k)\n",
    "    \n",
    "    # Return the most relevant abstract found\n",
    "    doc_id = indices[0][0]\n",
    "    return documents[doc_id]\n",
    "\n",
    "\n",
    "# 4. Test it out\n",
    "q = \"Does histologic chorioamnionitis correspond to clinical chorioamnionitis?\"\n",
    "retrieved_text = retrieve_paper(q)\n",
    "\n",
    "print(f\"\\nQuestion: {q}\")\n",
    "print(f\"Retrieved paper abstract:\\n{retrieved_text[:200]}...\") # Only print the first 200 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750705d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Config loaded. Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import logging as hf_logging\n",
    "import evaluate\n",
    "\n",
    "\n",
    "#  Global Configuration (Config)\n",
    "class Config:\n",
    "    #  Your local absolute path (prefix r prevents escape characters)\n",
    "    parquet_path = r\"C:\\LLM\\data\\pubmedqa_hf\\pqa_labeled\\train-00000-of-00001.parquet\"\n",
    "    \n",
    "    # Model name (use \"gpt2\" for testing; recommend switching to Llama-3-8B or Mistral for formal evaluation)\n",
    "    model_name = \"gpt2\" \n",
    "    \n",
    "    # Limit number of test samples (set to None to run the full 1000 entries)\n",
    "    limit = 20  \n",
    "    \n",
    "    batch_size = 4\n",
    "    max_new_tokens = 64\n",
    "    max_ctx_chars = 4000\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"eval_results\"\n",
    "    seed = 42\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Set random seed\n",
    "def set_seed(seed: int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "hf_logging.set_verbosity_error()\n",
    "print(f\" Config loaded. Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26dd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "print(f\" Reading data from: {cfg.parquet_path}\")\n",
    "\n",
    "if not os.path.exists(cfg.parquet_path):\n",
    "    raise FileNotFoundError(f\"  File not found: {cfg.parquet_path}\")\n",
    "\n",
    "# 1. Read Parquet file\n",
    "df = pd.read_parquet(cfg.parquet_path)\n",
    "\n",
    "# 2. Filter out invalid data\n",
    "df = df.dropna(subset=['question', 'long_answer'])\n",
    "if cfg.limit:\n",
    "    df = df.head(cfg.limit)\n",
    "\n",
    "# 3. Define Context extraction logic\n",
    "def extract_context_text(c):\n",
    "    \"\"\"\n",
    "    Specifically handles the PubMedQA context structure.\n",
    "    The Context column is usually a dictionary: {'contexts': ['A...', 'B...'], 'labels': [...]}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Case A: It is a dictionary (common case)\n",
    "        if isinstance(c, dict) and 'contexts' in c:\n",
    "            # 'contexts' is an array or list\n",
    "            return \" \".join(list(c['contexts']))\n",
    "        \n",
    "        # Case B: It is a Numpy array (sometimes pyarrow reads it as an array)\n",
    "        if isinstance(c, np.ndarray): \n",
    "            # Assuming the array contains structs, try to get the 'contexts' field.\n",
    "            # This is complex, and Case A usually covers it.\n",
    "            # Simple handling here: if the array contains only strings, join directly.\n",
    "            if c.dtype.kind in {'U', 'S'}: \n",
    "                return \" \".join(c)\n",
    "        \n",
    "        # Case C: It is already a list\n",
    "        if isinstance(c, list):\n",
    "            return \" \".join(c)\n",
    "\n",
    "        return str(c)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "print(\" Processing contexts...\")\n",
    "\n",
    "# Prepare Context for RAG mode (extracted from dataset)\n",
    "ctx_list_rag = df[\"context\"].apply(extract_context_text).apply(lambda x: x[:cfg.max_ctx_chars]).tolist()\n",
    "\n",
    "# Prepare Context for Base mode (set all to empty)\n",
    "ctx_list_base = [\"\"] * len(df)\n",
    "\n",
    "questions = df[\"question\"].tolist()\n",
    "refs = df[\"long_answer\"].tolist()\n",
    "decisions = df[\"final_decision\"].tolist() if \"final_decision\" in df else []\n",
    "\n",
    "print(f\" Data loaded successfully!\")\n",
    "print(f\"- Total Samples: {len(df)}\")\n",
    "print(f\"- First Question: {questions[0]}\")\n",
    "print(f\"- First Ref Answer: {refs[0][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeab31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(tok, q: str, ctx: str) -> str:\n",
    "    \"\"\"\n",
    "    Automatically switches between Open-Book / Closed-Book templates based on whether ctx is empty.\n",
    "    \"\"\"\n",
    "    has_ctx = ctx and len(ctx.strip()) > 0\n",
    "    \n",
    "    if has_ctx:\n",
    "        # RAG Mode\n",
    "        return (\n",
    "            f\"Question: {q}\\n\"\n",
    "            f\"Context: {ctx}\\n\\n\"\n",
    "            f\"Answer the question using the context. Provide a reasoning then a final Yes/No/Maybe.\\n\"\n",
    "            f\"Answer: \"\n",
    "        )\n",
    "    else:\n",
    "        # Base Mode (No Context)\n",
    "        return (\n",
    "            f\"Question: {q}\\n\\n\"\n",
    "            f\"Answer the question based on your knowledge. Provide a reasoning then a final Yes/No/Maybe.\\n\"\n",
    "            f\"Answer: \"\n",
    "        )\n",
    "\n",
    "def compute_keyword_recall(preds, refs):\n",
    "    \"\"\"Calculates how many keywords from the reference answer are included in the generated answer.\"\"\"\n",
    "    scores = []\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    for p, r in zip(preds, refs):\n",
    "        try:\n",
    "            vectorizer.fit([r])\n",
    "            ref_keywords = set(vectorizer.get_feature_names_out())\n",
    "            if not ref_keywords:\n",
    "                scores.append(0.0); continue\n",
    "            \n",
    "            p_lower = p.lower()\n",
    "            hit = sum(1 for kw in ref_keywords if kw in p_lower)\n",
    "            scores.append(hit / len(ref_keywords))\n",
    "        except:\n",
    "            scores.append(0.0)\n",
    "    return scores\n",
    "\n",
    "def extract_decision(text):\n",
    "    t = text.lower()\n",
    "    if 'yes' in t: return 'yes'\n",
    "    if 'no' in t: return 'no'\n",
    "    if 'maybe' in t: return 'maybe'\n",
    "    return 'unknown'\n",
    "\n",
    "def run_inference(model, tok, qs, ctxs, cfg, label):\n",
    "    print(f\" Running Inference: {label} ...\")\n",
    "    preds = []\n",
    "    \n",
    "    # Ensure pad_token exists\n",
    "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"left\"\n",
    "    \n",
    "    prompts = [build_prompt(tok, q, c) for q, c in zip(qs, ctxs)]\n",
    "    \n",
    "    total = len(prompts)\n",
    "    for i in range(0, total, cfg.batch_size):\n",
    "        batch = prompts[i : i + cfg.batch_size]\n",
    "        inputs = tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=cfg.max_ctx_chars+256).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=cfg.max_new_tokens, \n",
    "                pad_token_id=tok.pad_token_id, \n",
    "                do_sample=False # For reproducible results\n",
    "            )\n",
    "        \n",
    "        # Decode, taking only the generated part\n",
    "        input_len = inputs.input_ids.shape[1]\n",
    "        decoded = tok.batch_decode(out[:, input_len:], skip_special_tokens=True)\n",
    "        preds.extend([d.strip() for d in decoded])\n",
    "        \n",
    "        if (i // cfg.batch_size) % 5 == 0:\n",
    "            print(f\"   Batch {i // cfg.batch_size + 1} done.\")\n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce746a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Model\n",
    "print(f\" Loading Model: {cfg.model_name}...\")\n",
    "try:\n",
    "    tok = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    print(f\" Error loading model: {e}\")\n",
    "    raise e\n",
    "\n",
    "# 2. Execute Two Rounds of Inference\n",
    "# Round 1: Base (No Context)\n",
    "preds_base = run_inference(model, tok, questions, ctx_list_base, cfg, label=\"Base (No Context)\")\n",
    "\n",
    "# Round 2: RAG (With Context)\n",
    "preds_rag = run_inference(model, tok, questions, ctx_list_rag, cfg, label=\"RAG (With Context)\")\n",
    "print(\"\\n Inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa4432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating Metrics (This may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " EVALUATION SUMMARY\n",
      "========================================\n",
      "{\n",
      "  \"ROUGE-L\": {\n",
      "    \"Base\": 0.1033,\n",
      "    \"RAG\": 0.135\n",
      "  },\n",
      "  \"BERTScore-F1\": {\n",
      "    \"Base\": 0.6773,\n",
      "    \"RAG\": 0.6994\n",
      "  },\n",
      "  \"Keyword-Recall\": {\n",
      "    \"Base\": 0.0746,\n",
      "    \"RAG\": 0.1068\n",
      "  }\n",
      "}\n",
      "\n",
      " Detailed results saved to: eval_results\\result_comparison_20251210_165022.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCalculating Metrics (This may take a moment)...\")\n",
    "\n",
    "# 1. ROUGE (N-gram Overlap)\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "r_base = rouge.compute(predictions=preds_base, references=refs)\n",
    "r_rag = rouge.compute(predictions=preds_rag, references=refs)\n",
    "\n",
    "# 2. BERTScore (Semantic Similarity) - Will download a small model on first run\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bs_base = bertscore.compute(predictions=preds_base, references=refs, lang=\"en\", model_type=\"distilbert-base-uncased\")\n",
    "bs_rag = bertscore.compute(predictions=preds_rag, references=refs, lang=\"en\", model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "# 3. Keyword Recall\n",
    "kw_base = compute_keyword_recall(preds_base, refs)\n",
    "kw_rag = compute_keyword_recall(preds_rag, refs)\n",
    "\n",
    "# 4. Summary of Results\n",
    "summary = {\n",
    "    \"ROUGE-L\": {\n",
    "        \"Base\": round(r_base['rougeL'], 4), \n",
    "        \"RAG\": round(r_rag['rougeL'], 4)\n",
    "    },\n",
    "    \"BERTScore-F1\": {\n",
    "        \"Base\": round(np.mean(bs_base['f1']), 4), \n",
    "        \"RAG\": round(np.mean(bs_rag['f1']), 4)\n",
    "    },\n",
    "    \"Keyword-Recall\": {\n",
    "        \"Base\": round(np.mean(kw_base), 4), \n",
    "        \"RAG\": round(np.mean(kw_rag), 4)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" EVALUATION SUMMARY\")\n",
    "print(\"=\"*40)\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# 5. Save detailed results to file\n",
    "time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "save_path = os.path.join(cfg.output_dir, f\"result_comparison_{time_str}.jsonl\")\n",
    "\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(preds_base)):\n",
    "        row = {\n",
    "            \"id\": i,\n",
    "            \"question\": questions[i],\n",
    "            \"reference\": refs[i],\n",
    "            \"base_pred\": preds_base[i],\n",
    "            \"rag_pred\": preds_rag[i],\n",
    "            \"base_kw_recall\": kw_base[i],\n",
    "            \"rag_kw_recall\": kw_rag[i],\n",
    "            \"rag_context_preview\": ctx_list_rag[i][:100] # Save a bit of context for inspection\n",
    "        }\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\n Detailed results saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] DEVICE: cuda\n",
      "[Info] Reading test data: /LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\n",
      "[Info] Loaded samples: 100 | ref_col=long_answer | has_final_decision=True\n",
      "[Info] Loading KB docs: /LLM/PrimeKG/pubmed_documents.pkl\n",
      "[Info] KB docs loaded: 800\n",
      "[Info] Found FAISS index: /LLM/PrimeKG/pubmed_qa.index\n",
      "[Info] Using FAISS retriever.\n",
      "[Info] ctx example head: The etiology of hemodialysis (HD)-induced hypotension and hypertension remains speculative. There is mounting evidence that endothelin-1 (ET-1) may play a vital role in these hemodynamic changes. We e...\n",
      "[Info] Loading generator: gpt2\n",
      "[Info] model.device: cuda:0\n",
      "[Info] Running inference: Base (No Context)\n",
      "  batch 1 done\n",
      "  batch 11 done\n",
      "  batch 21 done\n",
      "[Info] Running inference: RAG (KB Retrieved Docs)\n",
      "  batch 1 done\n",
      "  batch 11 done\n",
      "  batch 21 done\n",
      "\n",
      "============================================================\n",
      "Decision ACC (Yes/No/Maybe)\n",
      "Base ACC: 0.1000\n",
      "RAG  ACC: 0.2800\n",
      "Gain   : +0.1800\n",
      "\n",
      "Confusion Matrix (RAG):\n",
      "Pred     yes  no  maybe  unknown\n",
      "GT                              \n",
      "yes        1  59      2        0\n",
      "no         1  27      2        0\n",
      "maybe      0   8      0        0\n",
      "unknown    0   0      0        0\n",
      "============================================================\n",
      "\n",
      "ROUGE-L (pure python):\n",
      "Base: 0.1260 | RAG: 0.1108 | Gain: -0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/miaoen/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bertscore/cf4907b18f8f741f202232c0f8009a3bd49ff98802c245abcb6ea51a37a8c05b (last modified on Wed Dec 17 16:39:27 2025) since it couldn't be found locally at evaluate-metric--bertscore, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERTScore-F1:\n",
      "Base: 0.7041 | RAG: 0.6744 | Gain: -0.0297\n",
      "\n",
      "============================================================\n",
      "[Done] Saved:\n",
      "  JSONL  : eval_results_pubmedqa_kbRAG_test/kbRAG_test_20251222_225401.jsonl\n",
      "  CSV    : eval_results_pubmedqa_kbRAG_test/kbRAG_test_20251222_225401.csv\n",
      "  SUMMARY: eval_results_pubmedqa_kbRAG_test/kbRAG_test_20251222_225401_summary.json\n",
      "============================================================\n",
      "{\n",
      "  \"meta\": {\n",
      "    \"parquet_path\": \"/LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\",\n",
      "    \"kb_docs_path\": \"/LLM/PrimeKG/pubmed_documents.pkl\",\n",
      "    \"kb_index_path\": \"/LLM/PrimeKG/pubmed_qa.index\",\n",
      "    \"retriever\": \"faiss\",\n",
      "    \"model\": \"gpt2\",\n",
      "    \"n\": 100,\n",
      "    \"ref_col\": \"long_answer\"\n",
      "  },\n",
      "  \"decision_acc\": {\n",
      "    \"base\": 0.1,\n",
      "    \"rag\": 0.28\n",
      "  },\n",
      "  \"rougeL\": {\n",
      "    \"base\": 0.126,\n",
      "    \"rag\": 0.1108\n",
      "  },\n",
      "  \"bertscore_f1\": {\n",
      "    \"base\": 0.7041,\n",
      "    \"rag\": 0.6744\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Full Script: PubMedQA test + KB(pubmed_documents.pkl) RAG\n",
    "# Metrics: Decision ACC + ROUGE-L (pure python) + BERTScore (optional)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# 0) Config\n",
    "\n",
    "class Config:\n",
    "    # ---- Eval data (TEST) ----\n",
    "    parquet_path = r\"/LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\"\n",
    "\n",
    "    # ---- KB docs (your \"kg\") ----\n",
    "    kb_docs_path = r\"/LLM/PrimeKG/pubmed_documents.pkl\"\n",
    "\n",
    "    # (Optional) FAISS index if you have it. If missing -> TF-IDF fallback\n",
    "    kb_index_path = r\"/LLM/PrimeKG/pubmed_qa.index\"\n",
    "    embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # only used if FAISS exists\n",
    "\n",
    "    # ---- Generator model ----\n",
    "    model_name_or_path = \"gpt2\"\n",
    "    batch_size = 4\n",
    "    max_new_tokens = 64\n",
    "\n",
    "    # ---- RAG params ----\n",
    "    top_k_docs = 2\n",
    "    max_ctx_chars = 1200\n",
    "\n",
    "    # ---- TF-IDF fallback params ----\n",
    "    tfidf_max_docs = None \n",
    "    tfidf_max_features = 200000\n",
    "\n",
    "    # ---- Eval params ----\n",
    "    limit = 100  # None -> run all\n",
    "    seed = 42\n",
    "\n",
    "    # ---- Output ----\n",
    "    output_dir = \"eval_results_pubmedqa_kbRAG_test\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "\n",
    "\n",
    "# 1) Utilities\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[Info] DEVICE:\", DEVICE)\n",
    "\n",
    "\n",
    "def extract_decision(text: str) -> str:\n",
    "    \"\"\"Extract final decision label from generated text (robust regex).\"\"\"\n",
    "    t = (text or \"\").lower()\n",
    "    if re.search(r\"\\byes\\b\", t): return \"yes\"\n",
    "    if re.search(r\"\\bno\\b\", t): return \"no\"\n",
    "    if re.search(r\"\\bmaybe\\b\", t): return \"maybe\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    if not y_true:\n",
    "        return None\n",
    "    correct = sum(int(a == b) for a, b in zip(y_true, y_pred))\n",
    "    return correct / len(y_true)\n",
    "\n",
    "\n",
    "def confusion_table(y_true, y_pred, labels=(\"yes\", \"no\", \"maybe\", \"unknown\")):\n",
    "    \"\"\"Return a pandas crosstab confusion table.\"\"\"\n",
    "    return pd.crosstab(\n",
    "        pd.Series(y_true, name=\"GT\"),\n",
    "        pd.Series(y_pred, name=\"Pred\"),\n",
    "        rownames=[\"GT\"], colnames=[\"Pred\"],\n",
    "        dropna=False\n",
    "    ).reindex(index=list(labels), columns=list(labels), fill_value=0)\n",
    "\n",
    "\n",
    "# ---- Pure Python ROUGE-L F1 ----\n",
    "def _lcs_len(a_tokens, b_tokens):\n",
    "    n, m = len(a_tokens), len(b_tokens)\n",
    "    dp = [0] * (m + 1)\n",
    "    for i in range(1, n + 1):\n",
    "        prev = 0\n",
    "        for j in range(1, m + 1):\n",
    "            tmp = dp[j]\n",
    "            if a_tokens[i - 1] == b_tokens[j - 1]:\n",
    "                dp[j] = prev + 1\n",
    "            else:\n",
    "                dp[j] = max(dp[j], dp[j - 1])\n",
    "            prev = tmp\n",
    "    return dp[m]\n",
    "\n",
    "def rouge_l_f1(pred: str, ref: str) -> float:\n",
    "    pred = (pred or \"\").strip()\n",
    "    ref = (ref or \"\").strip()\n",
    "    if not pred or not ref:\n",
    "        return 0.0\n",
    "    pred_tokens = re.findall(r\"\\w+\", pred.lower())\n",
    "    ref_tokens  = re.findall(r\"\\w+\", ref.lower())\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    lcs = _lcs_len(pred_tokens, ref_tokens)\n",
    "    prec = lcs / len(pred_tokens)\n",
    "    rec  = lcs / len(ref_tokens)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "\n",
    "\n",
    "# 2) Load TEST parquet\n",
    "print(f\"[Info] Reading test data: {cfg.parquet_path}\")\n",
    "if not os.path.exists(cfg.parquet_path):\n",
    "    raise FileNotFoundError(f\"Parquet not found: {cfg.parquet_path}\")\n",
    "\n",
    "tbl = pq.read_table(cfg.parquet_path)\n",
    "df = tbl.to_pandas()\n",
    "\n",
    "df = df.dropna(subset=[\"question\"])\n",
    "if cfg.limit:\n",
    "    df = df.head(cfg.limit)\n",
    "\n",
    "questions = df[\"question\"].astype(str).tolist()\n",
    "\n",
    "# Pick reference column (for ROUGE/BERTScore)\n",
    "ref_col = None\n",
    "for c in [\"long_answer\", \"final_decision\", \"answer\"]:\n",
    "    if c in df.columns:\n",
    "        ref_col = c\n",
    "        break\n",
    "if ref_col is None:\n",
    "    ref_col = df.columns[-1]\n",
    "\n",
    "refs = df[ref_col].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# Decision GT for ACC (best choice is final_decision)\n",
    "gt_decisions = None\n",
    "if \"final_decision\" in df.columns:\n",
    "    gt_decisions = df[\"final_decision\"].fillna(\"\").astype(str).str.lower().tolist()\n",
    "\n",
    "print(f\"[Info] Loaded samples: {len(questions)} | ref_col={ref_col} | has_final_decision={gt_decisions is not None}\")\n",
    "\n",
    "\n",
    "\n",
    "# 3) Load KB docs (pubmed_documents.pkl)\n",
    "print(f\"[Info] Loading KB docs: {cfg.kb_docs_path}\")\n",
    "if not os.path.exists(cfg.kb_docs_path):\n",
    "    raise FileNotFoundError(f\"KB docs not found: {cfg.kb_docs_path}\")\n",
    "\n",
    "with open(cfg.kb_docs_path, \"rb\") as f:\n",
    "    kb_docs = pickle.load(f)\n",
    "\n",
    "if cfg.tfidf_max_docs is not None:\n",
    "    kb_docs = kb_docs[:cfg.tfidf_max_docs]\n",
    "\n",
    "kb_docs = [(\"\" if d is None else str(d)) for d in kb_docs]\n",
    "print(f\"[Info] KB docs loaded: {len(kb_docs)}\")\n",
    "\n",
    "\n",
    "\n",
    "# 4) Build Retriever (FAISS preferred else TF-IDF)\n",
    "use_faiss = False\n",
    "faiss_index = None\n",
    "embed_model = None\n",
    "\n",
    "tfidf_vectorizer = None\n",
    "tfidf_X = None\n",
    "\n",
    "def build_tfidf_retriever(docs):\n",
    "    print(\"[Info] Building TF-IDF retriever (fallback)...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        max_features=cfg.tfidf_max_features,\n",
    "        ngram_range=(1, 2),\n",
    "    )\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    print(\"[Info] TF-IDF ready.\")\n",
    "    return vectorizer, X\n",
    "\n",
    "try:\n",
    "    if os.path.exists(cfg.kb_index_path):\n",
    "        import faiss\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "        print(f\"[Info] Found FAISS index: {cfg.kb_index_path}\")\n",
    "        faiss_index = faiss.read_index(cfg.kb_index_path)\n",
    "        embed_model = SentenceTransformer(cfg.embed_model_name)\n",
    "        use_faiss = True\n",
    "        print(\"[Info] Using FAISS retriever.\")\n",
    "    else:\n",
    "        print(\"[Info] FAISS index not found -> fallback TF-IDF.\")\n",
    "except Exception as e:\n",
    "    print(f\"[Warn] FAISS init failed -> fallback TF-IDF. Error: {e}\")\n",
    "\n",
    "if not use_faiss:\n",
    "    tfidf_vectorizer, tfidf_X = build_tfidf_retriever(kb_docs)\n",
    "\n",
    "def retrieve_docs(query: str, top_k: int):\n",
    "    query = (query or \"\").strip()\n",
    "    if not query:\n",
    "        return []\n",
    "\n",
    "    if use_faiss:\n",
    "        import faiss\n",
    "        q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q_emb)\n",
    "        scores, idxs = faiss_index.search(q_emb, top_k)\n",
    "        idxs = idxs[0].tolist()\n",
    "        return [kb_docs[i] for i in idxs if 0 <= i < len(kb_docs)]\n",
    "    else:\n",
    "        qv = tfidf_vectorizer.transform([query])\n",
    "        sims = (tfidf_X @ qv.T).toarray().ravel()\n",
    "        if top_k >= len(sims):\n",
    "            top_idx = np.argsort(-sims)\n",
    "        else:\n",
    "            top_idx = np.argpartition(-sims, top_k)[:top_k]\n",
    "            top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "        return [kb_docs[i] for i in top_idx.tolist()]\n",
    "\n",
    "def build_rag_context(question: str):\n",
    "    docs = retrieve_docs(question, cfg.top_k_docs)\n",
    "    ctx = \"\\n\\n\".join([d.strip() for d in docs if d and d.strip()])\n",
    "    return ctx[:cfg.max_ctx_chars] if ctx else \"\"\n",
    "\n",
    "ctx_list_rag = [build_rag_context(q) for q in questions]\n",
    "ctx_list_base = [\"\"] * len(questions)\n",
    "\n",
    "print(\"[Info] ctx example head:\", (ctx_list_rag[0][:200] + \"...\") if ctx_list_rag and ctx_list_rag[0] else \"<EMPTY>\")\n",
    "\n",
    "\n",
    "\n",
    "# 5) Load Generator model\n",
    "print(f\"[Info] Loading generator: {cfg.model_name_or_path}\")\n",
    "tok = AutoTokenizer.from_pretrained(cfg.model_name_or_path, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "print(\"[Info] model.device:\", model.device)\n",
    "\n",
    "\n",
    "\n",
    "# 6) Prompt + Inference\n",
    "def build_prompt(q: str, ctx: str) -> str:\n",
    "    has_ctx = bool(ctx and ctx.strip())\n",
    "    if has_ctx:\n",
    "        return (\n",
    "            f\"Question: {q}\\n\\n\"\n",
    "            f\"Retrieved Documents:\\n{ctx}\\n\\n\"\n",
    "            \"Answer the question using ONLY the retrieved documents. \"\n",
    "            \"Provide brief reasoning then a final Yes/No/Maybe.\\n\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            f\"Question: {q}\\n\\n\"\n",
    "            \"Answer the question based on your knowledge. \"\n",
    "            \"Provide brief reasoning then a final Yes/No/Maybe.\\n\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "\n",
    "def run_inference(qs, ctxs, label: str):\n",
    "    print(f\"[Info] Running inference: {label}\")\n",
    "    preds = []\n",
    "    prompts = [build_prompt(q, c) for q, c in zip(qs, ctxs)]\n",
    "\n",
    "    for i in range(0, len(prompts), cfg.batch_size):\n",
    "        batch_prompts = prompts[i:i + cfg.batch_size]\n",
    "        inputs = tok(\n",
    "            batch_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=cfg.max_ctx_chars + 256\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                pad_token_id=tok.pad_token_id,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        decoded = tok.batch_decode(out[:, input_len:], skip_special_tokens=True)\n",
    "        preds.extend([d.strip() for d in decoded])\n",
    "\n",
    "        if (i // cfg.batch_size) % 10 == 0:\n",
    "            print(f\"  batch {i // cfg.batch_size + 1} done\")\n",
    "\n",
    "    return preds\n",
    "\n",
    "preds_base = run_inference(questions, ctx_list_base, \"Base (No Context)\")\n",
    "preds_rag  = run_inference(questions, ctx_list_rag,  \"RAG (KB Retrieved Docs)\")\n",
    "\n",
    "\n",
    "\n",
    "# Decision by logprob (robust)\n",
    "@torch.no_grad()\n",
    "def decision_by_logprob(prompt: str, tok, model, device: str):\n",
    "    \"\"\"\n",
    "    Return best label in {yes,no,maybe} by scoring prompt + candidate.\n",
    "    Uses negative NLL on candidate tokens only.\n",
    "    \"\"\"\n",
    "    cand_map = {\"yes\": \" yes\", \"no\": \" no\", \"maybe\": \" maybe\"}\n",
    "\n",
    "    prompt_ids = tok(prompt, add_special_tokens=False).input_ids\n",
    "    prompt_len = len(prompt_ids)\n",
    "\n",
    "    scores = {}\n",
    "    for lab, suffix in cand_map.items():\n",
    "        ids = tok(prompt + suffix, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "        labels = ids.clone()\n",
    "        labels[:, :prompt_len] = -100  # mask prompt tokens, only score suffix tokens\n",
    "\n",
    "        out = model(input_ids=ids, labels=labels, use_cache=False)\n",
    "        cand_len = ids.shape[1] - prompt_len\n",
    "        # out.loss is mean NLL over scored tokens -> convert to total logprob\n",
    "        logp = -float(out.loss.item()) * float(cand_len)\n",
    "        scores[lab] = logp\n",
    "\n",
    "    best = max(scores, key=scores.get)\n",
    "    return best, scores\n",
    "\n",
    "\n",
    "def extract_decision_last(text: str) -> str:\n",
    "    \"\"\"Extract last occurrence of decision label from text using regex.\"\"\"\n",
    "    matches = re.findall(r\"\\b(yes|no|maybe)\\b\", (text or \"\").lower())\n",
    "    return matches[-1] if matches else \"unknown\"\n",
    "\n",
    "\n",
    "def get_decision(prompt: str, gen_text: str, tok, model, device: str) -> str:\n",
    "    \"\"\"\n",
    "    Get final decision from generated text; if unknown, use logprob scoring.\n",
    "    \"\"\"\n",
    "    d = extract_decision_last(gen_text)\n",
    "    if d != \"unknown\":\n",
    "        return d\n",
    "    best, _ = decision_by_logprob(prompt, tok, model, device)\n",
    "    return best\n",
    "\n",
    "\n",
    "\n",
    "# 7) Metrics: ACC + ROUGE-L + (optional) BERTScore\n",
    "#Decision ACC\n",
    "acc_base = acc_rag = None\n",
    "conf_rag = None\n",
    "prompts_base = [build_prompt(q, c) for q, c in zip(questions, ctx_list_base)]\n",
    "prompts_rag  = [build_prompt(q, c) for q, c in zip(questions, ctx_list_rag)]\n",
    "\n",
    "pred_dec_base = [\n",
    "    get_decision(prompts_base[i], preds_base[i], tok, model, model.device)\n",
    "    for i in range(len(preds_base))\n",
    "]\n",
    "pred_dec_rag = [\n",
    "    get_decision(prompts_rag[i], preds_rag[i], tok, model, model.device)\n",
    "    for i in range(len(preds_rag))\n",
    "]\n",
    "\n",
    "\n",
    "if gt_decisions is not None:\n",
    "    # 只保留 GT ∈ {yes,no,maybe}\n",
    "    valid_idx = [i for i, g in enumerate(gt_decisions) if g in (\"yes\", \"no\", \"maybe\")]\n",
    "    y_true = [gt_decisions[i] for i in valid_idx]\n",
    "    yb = [pred_dec_base[i] for i in valid_idx]\n",
    "    yr = [pred_dec_rag[i] for i in valid_idx]\n",
    "\n",
    "    acc_base = accuracy(y_true, yb)\n",
    "    acc_rag  = accuracy(y_true, yr)\n",
    "    conf_rag = confusion_table(y_true, yr, labels=(\"yes\", \"no\", \"maybe\", \"unknown\"))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Decision ACC (Yes/No/Maybe)\")\n",
    "if acc_base is None:\n",
    "    print(\"[Warn] final_decision not found in parquet -> ACC skipped.\")\n",
    "else:\n",
    "    print(f\"Base ACC: {acc_base:.4f}\")\n",
    "    print(f\"RAG  ACC: {acc_rag:.4f}\")\n",
    "    print(f\"Gain   : {acc_rag - acc_base:+.4f}\")\n",
    "    print(\"\\nConfusion Matrix (RAG):\")\n",
    "    print(conf_rag)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ROUGE-L (pure python)\n",
    "rougeL_base_list = [rouge_l_f1(p, r) for p, r in zip(preds_base, refs)]\n",
    "rougeL_rag_list  = [rouge_l_f1(p, r) for p, r in zip(preds_rag, refs)]\n",
    "rougeL_base = float(np.mean(rougeL_base_list))\n",
    "rougeL_rag  = float(np.mean(rougeL_rag_list))\n",
    "\n",
    "print(\"\\nROUGE-L (pure python):\")\n",
    "print(f\"Base: {rougeL_base:.4f} | RAG: {rougeL_rag:.4f} | Gain: {rougeL_rag - rougeL_base:+.4f}\")\n",
    "\n",
    "# BERTScore (optional; may require downloads/extra deps)\n",
    "bs_base = bs_rag = None\n",
    "try:\n",
    "    import evaluate\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bs_res_base = bertscore.compute(predictions=preds_base, references=refs, lang=\"en\", model_type=\"distilbert-base-uncased\")\n",
    "    bs_res_rag  = bertscore.compute(predictions=preds_rag,  references=refs, lang=\"en\", model_type=\"distilbert-base-uncased\")\n",
    "    bs_base = float(np.mean(bs_res_base[\"f1\"]))\n",
    "    bs_rag  = float(np.mean(bs_res_rag[\"f1\"]))\n",
    "    print(\"\\nBERTScore-F1:\")\n",
    "    print(f\"Base: {bs_base:.4f} | RAG: {bs_rag:.4f} | Gain: {bs_rag - bs_base:+.4f}\")\n",
    "except Exception as e:\n",
    "    print(\"\\n[Warn] BERTScore skipped (dependency/download issue). Error:\", str(e)[:200])\n",
    "\n",
    "\n",
    "\n",
    "# 8) Save results\n",
    "\n",
    "summary = {\n",
    "    \"meta\": {\n",
    "        \"parquet_path\": cfg.parquet_path,\n",
    "        \"kb_docs_path\": cfg.kb_docs_path,\n",
    "        \"kb_index_path\": cfg.kb_index_path if os.path.exists(cfg.kb_index_path) else None,\n",
    "        \"retriever\": \"faiss\" if use_faiss else \"tfidf\",\n",
    "        \"model\": cfg.model_name_or_path,\n",
    "        \"n\": len(questions),\n",
    "        \"ref_col\": ref_col,\n",
    "    },\n",
    "    \"decision_acc\": {\n",
    "        \"base\": None if acc_base is None else round(float(acc_base), 4),\n",
    "        \"rag\":  None if acc_rag  is None else round(float(acc_rag), 4),\n",
    "    },\n",
    "    \"rougeL\": {\n",
    "        \"base\": round(rougeL_base, 4),\n",
    "        \"rag\":  round(rougeL_rag, 4),\n",
    "    },\n",
    "    \"bertscore_f1\": {\n",
    "        \"base\": None if bs_base is None else round(bs_base, 4),\n",
    "        \"rag\":  None if bs_rag  is None else round(bs_rag, 4),\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "jsonl_path = os.path.join(cfg.output_dir, f\"kbRAG_test_{time_str}.jsonl\")\n",
    "csv_path   = os.path.join(cfg.output_dir, f\"kbRAG_test_{time_str}.csv\")\n",
    "summary_path = os.path.join(cfg.output_dir, f\"kbRAG_test_{time_str}_summary.json\")\n",
    "\n",
    "# Save per-sample\n",
    "rows = []\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(questions)):\n",
    "        row = {\n",
    "            \"id\": i,\n",
    "            \"question\": questions[i],\n",
    "            \"reference\": refs[i],\n",
    "            \"gt_decision\": None if gt_decisions is None else gt_decisions[i],\n",
    "            \"base_pred\": preds_base[i],\n",
    "            \"rag_pred\": preds_rag[i],\n",
    "            \"base_decision\": pred_dec_base[i],\n",
    "            \"rag_decision\": pred_dec_rag[i],\n",
    "            \"rougeL_base\": rougeL_base_list[i],\n",
    "            \"rougeL_rag\":  rougeL_rag_list[i],\n",
    "            \"rag_context_preview\": (ctx_list_rag[i][:300] if ctx_list_rag[i] else \"\"),\n",
    "        }\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "        rows.append(row)\n",
    "\n",
    "# Save CSV\n",
    "pd.DataFrame(rows).to_csv(csv_path, index=False)\n",
    "\n",
    "# Save summary JSON\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"[Done] Saved:\")\n",
    "print(\"  JSONL  :\", jsonl_path)\n",
    "print(\"  CSV    :\", csv_path)\n",
    "print(\"  SUMMARY:\", summary_path)\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(summary, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e01c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "LABELS = [\"yes\", \"no\", \"maybe\"]\n",
    "\n",
    "def _score_completion(tok, model, prompt_text: str, completion_text: str, max_len=1024) -> float:\n",
    "    \"\"\"\n",
    "    Compute log-probability score of completion_text given prompt_text.\n",
    "    \"\"\"\n",
    "    full = prompt_text + \" \" + completion_text\n",
    "    enc_full = tok(full, return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
    "    enc_prompt = tok(prompt_text, return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
    "\n",
    "    input_ids = enc_full[\"input_ids\"]\n",
    "    attn = enc_full[\"attention_mask\"]\n",
    "\n",
    "    # labels: mask prompt tokens only score completion tokens\n",
    "    labels = input_ids.clone()\n",
    "    prompt_len = enc_prompt[\"input_ids\"].shape[1]\n",
    "    if prompt_len >= input_ids.shape[1]:\n",
    "        return -1e9\n",
    "    labels[:, :prompt_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attn, labels=labels)\n",
    "        loss = out.loss\n",
    "        if torch.isnan(loss):\n",
    "            return -1e9\n",
    "        return float(-loss.item())\n",
    "\n",
    "def build_decision_prompt(question: str, ctx: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Build prompt for decision scoring.\n",
    "    \"\"\"\n",
    "    if ctx and ctx.strip():\n",
    "        return (\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Retrieved Documents:\\n{ctx}\\n\\n\"\n",
    "            \"Decide the final answer. Respond with ONE WORD only: yes / no / maybe.\\n\"\n",
    "            \"Final answer:\"\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            \"Decide the final answer. Respond with ONE WORD only: yes / no / maybe.\\n\"\n",
    "            \"Final answer:\"\n",
    "        )\n",
    "\n",
    "def predict_decision_by_scoring(tok, model, question: str, ctx: str = \"\"):\n",
    "    prompt = build_decision_prompt(question, ctx)\n",
    "    scores = {lab: _score_completion(tok, model, prompt, lab) for lab in LABELS}\n",
    "    pred = max(scores, key=scores.get)\n",
    "    return pred, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8bd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision prediction by scoring (Base / RAG) \n",
    "pred_dec_base = []\n",
    "pred_dec_rag = []\n",
    "scores_dec_base = []\n",
    "scores_dec_rag = []\n",
    "\n",
    "for q, ctx in zip(questions, ctx_list_rag):\n",
    "    p_b, s_b = predict_decision_by_scoring(tok, model, q, ctx=\"\")\n",
    "    p_r, s_r = predict_decision_by_scoring(tok, model, q, ctx=ctx)\n",
    "    pred_dec_base.append(p_b); scores_dec_base.append(s_b)\n",
    "    pred_dec_rag.append(p_r);  scores_dec_rag.append(s_r)\n",
    "\n",
    "# ACC\n",
    "valid_idx = [i for i,g in enumerate(gt_decisions) if g in (\"yes\",\"no\",\"maybe\")]\n",
    "y_true = [gt_decisions[i] for i in valid_idx]\n",
    "y_base = [pred_dec_base[i] for i in valid_idx]\n",
    "y_rag  = [pred_dec_rag[i]  for i in valid_idx]\n",
    "\n",
    "acc_base = sum(a==b for a,b in zip(y_true,y_base)) / len(y_true)\n",
    "acc_rag  = sum(a==b for a,b in zip(y_true,y_rag )) / len(y_true)\n",
    "\n",
    "print(\"Decision ACC (scoring):\")\n",
    "print(\"Base:\", acc_base)\n",
    "print(\"RAG :\", acc_rag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ff660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check] len(kb_docs) = 800\n",
      "[Check] faiss_index.ntotal = 800\n"
     ]
    }
   ],
   "source": [
    "print(\"[Check] len(kb_docs) =\", len(kb_docs))\n",
    "try:\n",
    "    import faiss\n",
    "    print(\"[Check] faiss_index.ntotal =\", faiss_index.ntotal)\n",
    "    if faiss_index.ntotal != len(kb_docs):\n",
    "        print(\"[FATAL] FAISS index and docs length mismatch! 。\")\n",
    "except Exception as e:\n",
    "    print(\"[Check] faiss not available:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aabdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loaded: eval_results_pubmedqa_kbRAG_test/kbRAG_test_20251222_175710.csv | rows: 20\n",
      "[Info] Columns: ['id', 'question', 'reference', 'gt_decision', 'base_pred', 'rag_pred', 'base_decision', 'rag_decision', 'rougeL_base', 'rougeL_rag', 'rag_context_preview']\n",
      "[Info] BERTScore device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate.loading:Using the latest cached version of the module from /home/miaoen/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bertscore/cf4907b18f8f741f202232c0f8009a3bd49ff98802c245abcb6ea51a37a8c05b (last modified on Wed Dec 17 16:39:27 2025) since it couldn't be found locally at evaluate-metric--bertscore, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Computing BERTScore for BASE ...\n",
      "[Info] Computing BERTScore for RAG ...\n",
      "\n",
      "[Info] BERTScore summary:\n",
      "  base mean: 0.7157986968755722\n",
      "  rag  mean: 0.6796941459178925\n",
      "  gain mean: -0.03610455095767975\n",
      "\n",
      "[Top3] Highest RAG BERTScore:\n",
      " id  bert_f1_rag  bert_gain                                                                                                      question\n",
      "  4     0.763997   0.064787 Body perception: do parents, their children, and their children's physicians perceive body image differently?\n",
      " 11     0.755544   0.009216                     Does laparoscopic surgery decrease the risk of atrial fibrillation after foregut surgery?\n",
      " 14     0.748675   0.035051                                                       Did Chile's traffic law reform push police enforcement?\n",
      "\n",
      "[Bottom3] Lowest RAG BERTScore:\n",
      " id  bert_f1_rag  bert_gain                                                                                                                                                           question\n",
      " 13     0.563964  -0.188528                                                                                        Kell alloimmunization in pregnancy: associated with fetal thrombocytopenia?\n",
      "  8     0.584381  -0.109972                                                                                       Does delivery mode affect women's postpartum quality of life in rural China?\n",
      "  2     0.597281  -0.106894 Screening for gestational diabetes mellitus: are the criteria proposed by the international association of the Diabetes and Pregnancy Study Groups cost-effective?\n",
      "\n",
      "[Top3] Highest BERT Gain (RAG-Base):\n",
      " id  bert_gain  bert_f1_base  bert_f1_rag                                                                                                      question\n",
      "  1   0.094784      0.635924     0.730708                                       Should temperature be monitorized during kidney allograft preservation?\n",
      "  0   0.078883      0.602323     0.681206                              Malnutrition, a new inducer for arterial calcification in hemodialysis patients?\n",
      "  4   0.064787      0.699210     0.763997 Body perception: do parents, their children, and their children's physicians perceive body image differently?\n",
      "\n",
      "[Bottom3] Lowest BERT Gain (RAG-Base):\n",
      " id  bert_gain  bert_f1_base  bert_f1_rag                                                                                                                                                question\n",
      " 13  -0.188528      0.752492     0.563964                                                                             Kell alloimmunization in pregnancy: associated with fetal thrombocytopenia?\n",
      "  9  -0.180490      0.816293     0.635803       Is first-line single-agent mitoxantrone in the treatment of high-risk metastatic breast cancer patients as effective as combination chemotherapy?\n",
      " 16  -0.162230      0.830454     0.668224 Is combined therapy more effective than growth hormone or hyperbaric oxygen alone in the healing of left ischemic and non-ischemic colonic anastomoses?\n",
      "\n",
      "[Saved]\n",
      "  Full (with bertscore): eval_results_pubmedqa_kbRAG_test/kbRAG_test_20251222_175710_with_bertscore.csv\n",
      "  Top3 rag: eval_results_pubmedqa_kbRAG_test/kbRAG_test_20251222_175710_top3_bertscore_rag.csv\n",
      "  Bot3 rag: eval_results_pubmedqa_kbRAG_test/kbRAG_test_20251222_175710_bottom3_bertscore_rag.csv\n",
      "  Top3 gain: eval_results_pubmedqa_kbRAG_test/kbRAG_test_20251222_175710_top3_bertscore_gain.csv\n",
      "  Bot3 gain: eval_results_pubmedqa_kbRAG_test/kbRAG_test_20251222_175710_bottom3_bertscore_gain.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 0) Your input CSV (currently missing bertscore columns)\n",
    "csv_path = r\"eval_results_pubmedqa_kbRAG_test/kbRAG_test_20251222_175710.csv\"\n",
    "assert os.path.exists(csv_path), f\"CSV not found: {csv_path}\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"[Info] Loaded:\", csv_path, \"| rows:\", len(df))\n",
    "print(\"[Info] Columns:\", list(df.columns))\n",
    "\n",
    "# 1) Prepare prediction/reference text\n",
    "# Prevent null values\n",
    "preds_base = df[\"base_pred\"].fillna(\"\").astype(str).tolist()\n",
    "preds_rag  = df[\"rag_pred\"].fillna(\"\").astype(str).tolist()\n",
    "refs       = df[\"reference\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# 2) Load BERTScore (evaluate)\n",
    "# You previously reported rouge needed absl, bertscore might also depend on some packages here\n",
    "# If ModuleNotFoundError: absl occurs again, install it: pip install absl-py\n",
    "import evaluate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[Info] BERTScore device:\", device)\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Note:\n",
    "# - roberta-large is the most accurate but large; distilbert-base-uncased is lighter and faster\n",
    "# - Your previous logs used the default (likely roberta-large), here I'm giving you the distilbert version, stable and saves VRAM\n",
    "model_type = \"distilbert-base-uncased\"\n",
    "\n",
    "print(\"[Info] Computing BERTScore for BASE ...\")\n",
    "res_base = bertscore.compute(\n",
    "    predictions=preds_base,\n",
    "    references=refs,\n",
    "    lang=\"en\",\n",
    "    model_type=model_type,\n",
    "    device=device,\n",
    "    batch_size=16,   # Change to 8 if VRAM is insufficient\n",
    ")\n",
    "f1_base = np.array(res_base[\"f1\"], dtype=float)\n",
    "\n",
    "print(\"[Info] Computing BERTScore for RAG ...\")\n",
    "res_rag = bertscore.compute(\n",
    "    predictions=preds_rag,\n",
    "    references=refs,\n",
    "    lang=\"en\",\n",
    "    model_type=model_type,\n",
    "    device=device,\n",
    "    batch_size=16,\n",
    ")\n",
    "f1_rag = np.array(res_rag[\"f1\"], dtype=float)\n",
    "\n",
    "# Write back to DF\n",
    "df[\"bert_f1_base\"] = f1_base\n",
    "df[\"bert_f1_rag\"]  = f1_rag\n",
    "df[\"bert_gain\"]    = df[\"bert_f1_rag\"] - df[\"bert_f1_base\"]\n",
    "\n",
    "print(\"\\n[Info] BERTScore summary:\")\n",
    "print(\"  base mean:\", float(df[\"bert_f1_base\"].mean()))\n",
    "print(\"  rag  mean:\", float(df[\"bert_f1_rag\"].mean()))\n",
    "print(\"  gain mean:\", float(df[\"bert_gain\"].mean()))\n",
    "\n",
    "# 3) Export Top3 / Bottom3 (By RAG BERTScore) ======\n",
    "top3_rag = df.sort_values(\"bert_f1_rag\", ascending=False).head(3)\n",
    "bot3_rag = df.sort_values(\"bert_f1_rag\", ascending=True).head(3)\n",
    "\n",
    "# Columns you want to save (can add more yourself)\n",
    "keep_cols = [\n",
    "    \"id\",\"question\",\"gt_decision\",\n",
    "    \"bert_f1_base\",\"bert_f1_rag\",\"bert_gain\",\n",
    "    \"rougeL_base\",\"rougeL_rag\",\n",
    "    \"base_pred\",\"rag_pred\",\n",
    "    \"rag_context_preview\"\n",
    "]\n",
    "keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "\n",
    "print(\"\\n[Top3] Highest RAG BERTScore:\")\n",
    "print(top3_rag[[\"id\",\"bert_f1_rag\",\"bert_gain\",\"question\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n[Bottom3] Lowest RAG BERTScore:\")\n",
    "print(bot3_rag[[\"id\",\"bert_f1_rag\",\"bert_gain\",\"question\"]].to_string(index=False))\n",
    "\n",
    "# 4) Optional: Export gain Top3 / Bottom3 (RAG-Base) ======\n",
    "top3_gain = df.sort_values(\"bert_gain\", ascending=False).head(3)\n",
    "bot3_gain = df.sort_values(\"bert_gain\", ascending=True).head(3)\n",
    "\n",
    "print(\"\\n[Top3] Highest BERT Gain (RAG-Base):\")\n",
    "print(top3_gain[[\"id\",\"bert_gain\",\"bert_f1_base\",\"bert_f1_rag\",\"question\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n[Bottom3] Lowest BERT Gain (RAG-Base):\")\n",
    "print(bot3_gain[[\"id\",\"bert_gain\",\"bert_f1_base\",\"bert_f1_rag\",\"question\"]].to_string(index=False))\n",
    "\n",
    "# 5) Save files\n",
    "out_dir = os.path.dirname(csv_path) or \".\"\n",
    "base_name = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "\n",
    "csv_with_bert = os.path.join(out_dir, f\"{base_name}_with_bertscore.csv\")\n",
    "top3_path     = os.path.join(out_dir, f\"{base_name}_top3_bertscore_rag.csv\")\n",
    "bot3_path     = os.path.join(out_dir, f\"{base_name}_bottom3_bertscore_rag.csv\")\n",
    "top3g_path    = os.path.join(out_dir, f\"{base_name}_top3_bertscore_gain.csv\")\n",
    "bot3g_path    = os.path.join(out_dir, f\"{base_name}_bottom3_bertscore_gain.csv\")\n",
    "\n",
    "df.to_csv(csv_with_bert, index=False, encoding=\"utf-8-sig\")\n",
    "top3_rag[keep_cols].to_csv(top3_path, index=False, encoding=\"utf-8-sig\")\n",
    "bot3_rag[keep_cols].to_csv(bot3_path, index=False, encoding=\"utf-8-sig\")\n",
    "top3_gain[keep_cols].to_csv(top3g_path, index=False, encoding=\"utf-8-sig\")\n",
    "bot3_gain[keep_cols].to_csv(bot3g_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[Saved]\")\n",
    "print(\"  Full (with bertscore):\", csv_with_bert)\n",
    "print(\"  Top3 rag:\", top3_path)\n",
    "print(\"  Bot3 rag:\", bot3_path)\n",
    "print(\"  Top3 gain:\", top3g_path)\n",
    "print(\"  Bot3 gain:\", bot3g_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e20766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
