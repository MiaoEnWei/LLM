{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1: Basic Imports & Configuration\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# [New] PEFT Library Import\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "# [Configuration Area] Please modify variables below according to your actual paths\n",
    "# Original GPT-2 Base Path\n",
    "GPT2_BASE_PATH      = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/gpt2\"\n",
    "\n",
    "# [New] Adapter Path trained via Prompt Tuning\n",
    "ADAPTER_PATH        = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/out_gpt2_official_prompt_tuning_e1\"\n",
    "\n",
    "MEDMCQA_FILE        = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/medmcqa/dev.json\"\n",
    "\n",
    "# Knowledge Base File Paths\n",
    "FAISS_INDEX_PATH    = \"pubmed_qa.index\"\n",
    "DOCS_PKL_PATH       = \"pubmed_documents.pkl\"\n",
    "\n",
    "# Embedding Model\n",
    "EMBED_MODEL_NAME    = \"all-MiniLM-L6-v2\" \n",
    "\n",
    "DEVICE              = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# RAG Parameters\n",
    "TOP_K_DOCS          = 2     # Retrieve the top 2 most relevant abstracts\n",
    "MAX_CTX_CHARS       = 2000  # Maximum context characters\n",
    "\n",
    "print(f\"Config OK. DEVICE = {DEVICE}\")\n",
    "\n",
    "\n",
    "# Layer 2: Model Layer (GPT-2 Loading & Fixed Generation Function)\n",
    "def load_peft_model(base_path: str, adapter_path: str):\n",
    "    print(f\"Loading Tokenizer & Base Model from {base_path} ...\")\n",
    "    try:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(base_path)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # 1. Load original base model\n",
    "        base_model = GPT2LMHeadModel.from_pretrained(base_path).to(DEVICE)\n",
    "        \n",
    "        # 2. Load Prompt Tuning Adapter\n",
    "        print(f\"Loading Prompt Tuning Adapter from {adapter_path} ...\")\n",
    "        model = PeftModel.from_pretrained(base_model, adapter_path).to(DEVICE)\n",
    "        \n",
    "        # [Critical] Set model to eval mode, important for Dropout/BN and stable generation\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"PEFT Model loaded successfully.\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {e}\")\n",
    "        return None, None\n",
    "\n",
    "tokenizer, model = load_peft_model(GPT2_BASE_PATH, ADAPTER_PATH)\n",
    "\n",
    "def gpt2_generate(prompt: str, use_adapter: bool = True, max_new_tokens: int = 10, do_sample: bool = False):\n",
    "    \"\"\"\n",
    "    General generation function (Fixed version)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    # Input length truncation protection\n",
    "    if inputs.shape[1] > 900:\n",
    "        inputs = inputs[:, -900:]\n",
    "        \n",
    "    attention_mask = torch.ones_like(inputs)\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        # \"use_cache\": True # Explicitly enabling cache usually speeds up, but causes shape issues in rare versions. Default is usually True\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_adapter:\n",
    "            #Prompt Tuning Mode\n",
    "            # Use model (PeftModel) directly for generation, it automatically appends soft prompts\n",
    "            outputs = model.generate(inputs, **gen_kwargs)\n",
    "        else:\n",
    "            #Base Raw Mode\n",
    "            # [Critical Fix]: Use disable_adapter() context and call base_model.generate directly\n",
    "            # This avoids interference from the PeftModel wrapper while in disabled state\n",
    "            with model.disable_adapter():\n",
    "                outputs = model.base_model.generate(inputs, **gen_kwargs)\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Layer 3: Knowledge Base Layer (Load FAISS & Documents)\n",
    "def load_retrieval_system():\n",
    "    if not os.path.exists(FAISS_INDEX_PATH) or not os.path.exists(DOCS_PKL_PATH):\n",
    "        print(f\"Error: Knowledge base files not found. Please check if {FAISS_INDEX_PATH} and {DOCS_PKL_PATH} exist.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"Loading Embedding Model: {EMBED_MODEL_NAME} ...\")\n",
    "    embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "    \n",
    "    print(f\"Loading FAISS Index ...\")\n",
    "    index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "    \n",
    "    print(f\"Loading Documents ...\")\n",
    "    with open(DOCS_PKL_PATH, \"rb\") as f:\n",
    "        documents = pickle.load(f)\n",
    "        \n",
    "    print(f\"Knowledge Base Loaded! Index size: {index.ntotal}, Docs count: {len(documents)}\")\n",
    "    return embed_model, index, documents\n",
    "\n",
    "embed_model, faiss_index, doc_store = load_retrieval_system()\n",
    "\n",
    "\n",
    "# Layer 4: Vector Retrieval RAG Logic\n",
    "def get_pubmed_context(question_text: str, top_k: int = TOP_K_DOCS) -> str:\n",
    "    if faiss_index is None: return \"\"\n",
    "    \n",
    "    q_emb = embed_model.encode([question_text], convert_to_numpy=True)\n",
    "    distances, indices = faiss_index.search(q_emb, top_k)\n",
    "    \n",
    "    retrieved_texts = []\n",
    "    current_chars = 0\n",
    "    \n",
    "    for idx_in_store in indices[0]:\n",
    "        if idx_in_store == -1 or idx_in_store >= len(doc_store): continue\n",
    "        doc_content = doc_store[idx_in_store].replace(\"\\n\", \" \").strip()\n",
    "        if not doc_content: continue\n",
    "\n",
    "        if current_chars + len(doc_content) > MAX_CTX_CHARS:\n",
    "            remaining = MAX_CTX_CHARS - current_chars\n",
    "            retrieved_texts.append(f\"Abstract: {doc_content[:remaining]}...\")\n",
    "            break\n",
    "        \n",
    "        retrieved_texts.append(f\"Abstract: {doc_content}\")\n",
    "        current_chars += len(doc_content)\n",
    "    \n",
    "    if not retrieved_texts: return \"\"\n",
    "    return \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "\n",
    "# Layer 5: MedMCQA Data Loading\n",
    "def load_medmcqa_example(idx: int = 0, file_path: str = MEDMCQA_FILE):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        line_idx = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            if line_idx == idx:\n",
    "                data = json.loads(line)\n",
    "                q = data.get(\"question\") or data.get(\"Question\") or \"\"\n",
    "                options_lines = []\n",
    "                option_map = {\"opa\": \"A\", \"opb\": \"B\", \"opc\": \"C\", \"opd\": \"D\"}\n",
    "                found = False\n",
    "                for k, lab in option_map.items():\n",
    "                    if k in data:\n",
    "                        found = True\n",
    "                        options_lines.append(f\"{lab}) {data[k]}\")\n",
    "                if not found and \"options\" in data:\n",
    "                    for i, opt in enumerate(data[\"options\"]):\n",
    "                        options_lines.append(f\"{chr(ord('A')+i)}) {opt}\")\n",
    "                \n",
    "                question_text = q\n",
    "                if options_lines:\n",
    "                    question_text += \"\\nOptions:\\n\" + \"\\n\".join(options_lines)\n",
    "                \n",
    "                answer = data.get(\"cop\") or data.get(\"answer\") or data.get(\"label\")\n",
    "                return {\"raw\": data, \"question_text\": question_text, \"answer\": answer}\n",
    "            line_idx += 1\n",
    "    raise IndexError(f\"Index {idx} out of range\")\n",
    "\n",
    "\n",
    "# Layer 6: Comparison QA Interface\n",
    "# 1. Base Model (Raw, No RAG, Adapter Disabled)\n",
    "# 2. Prompt Tuning Model (Adapter Enabled) + RAG\n",
    "\n",
    "def extract_answer(text: str) -> str:\n",
    "    \"\"\"Extract the last valid option from generated text\"\"\"\n",
    "    tail = text.strip()\n",
    "    for ch in reversed(tail):\n",
    "        if ch in [\"A\", \"B\", \"C\", \"D\"]: return ch\n",
    "    return tail\n",
    "\n",
    "def qa_baseline_raw(question: str) -> str:\n",
    "    \"\"\"\n",
    "    [Baseline] Pure Base Model, No Adapter, No RAG\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a medical exam solver.\\n\"\n",
    "        \"Choose the single best option and reply with ONLY one capital letter: A, B, C, or D.\\n\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"Answer (A, B, C, or D):\"\n",
    "    )\n",
    "    # use_adapter=False -> Disable Prompt Tuning weights\n",
    "    full_text = gpt2_generate(prompt, use_adapter=False, max_new_tokens=5, do_sample=False)\n",
    "    return extract_answer(full_text)\n",
    "\n",
    "def qa_pt_rag(question: str):\n",
    "    \"\"\"\n",
    "    [Experimental] Prompt Tuning Model + RAG Context\n",
    "    \"\"\"\n",
    "    # 1. Retrieve\n",
    "    context = get_pubmed_context(question, top_k=TOP_K_DOCS)\n",
    "    \n",
    "    # 2. Construct Prompt (RAG Augmented)\n",
    "    prompt = (\n",
    "        \"You are a medical exam solver.\\n\"\n",
    "        \"Below are relevant research abstracts retrieved from PubMed.\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        \"Using the context above, answer the question below with ONLY one capital letter: A, B, C, or D.\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\n\"\n",
    "        \"Answer (A, B, C, or D):\"\n",
    "    )\n",
    "    \n",
    "    # use_adapter=True -> Enable Prompt Tuning weights\n",
    "    full_text = gpt2_generate(prompt, use_adapter=True, max_new_tokens=5, do_sample=False)\n",
    "    \n",
    "    return context, extract_answer(full_text)\n",
    "\n",
    "\n",
    "# Layer 8: Batch Evaluation (Base vs Prompt Tuning+RAG)\n",
    "def evaluate_pt_rag_comparison(start_idx=0, end_idx=100, output_file=None):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if output_file is None:\n",
    "        output_file = f\"pt_rag_eval_{timestamp}.csv\"\n",
    "    \n",
    "    output_dir = \"results\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    full_output_path = os.path.join(output_dir, output_file)\n",
    "    \n",
    "    print(f\"Start Evaluation: Base Model vs. Prompt Tuning + RAG\")\n",
    "    print(f\"Range: {start_idx} -> {end_idx}\")\n",
    "    print(f\"Results saved to: {full_output_path}\")\n",
    "    \n",
    "    results = [] \n",
    "    total = 0\n",
    "    correct_base = 0\n",
    "    correct_pt_rag = 0\n",
    "    \n",
    "    # Counters: Improved and Worsened cases for PT+RAG compared to Base\n",
    "    improved = 0\n",
    "    worsened = 0\n",
    "    \n",
    "    for idx in range(start_idx, end_idx):\n",
    "        try:\n",
    "            ex = load_medmcqa_example(idx)\n",
    "        except: \n",
    "            continue\n",
    "        \n",
    "        q = ex[\"question_text\"]\n",
    "        raw_ans = ex[\"answer\"]\n",
    "        \n",
    "        # Parse GT\n",
    "        gt = None\n",
    "        if raw_ans and str(raw_ans).strip() in [\"A\",\"B\",\"C\",\"D\"]: \n",
    "            gt = str(raw_ans).strip()\n",
    "        elif raw_ans and str(raw_ans).isdigit(): \n",
    "            gt = chr(ord(\"A\") + int(raw_ans) - 1)\n",
    "        \n",
    "        if not gt: continue \n",
    "\n",
    "        # Core Prediction Comparison\n",
    "        # 1. Base Model (Raw)\n",
    "        pred_base = qa_baseline_raw(q)\n",
    "        \n",
    "        # 2. Prompt Tuning + RAG\n",
    "        ctx_rag, pred_pt_rag = qa_pt_rag(q)\n",
    "        \n",
    "        # Statistics\n",
    "        is_correct_base = (pred_base == gt)\n",
    "        is_correct_pt_rag = (pred_pt_rag == gt)\n",
    "        \n",
    "        if is_correct_base: correct_base += 1\n",
    "        if is_correct_pt_rag: correct_pt_rag += 1\n",
    "        total += 1\n",
    "        \n",
    "        status = \"Same\"\n",
    "        if not is_correct_base and is_correct_pt_rag:\n",
    "            status = \"Improved\"\n",
    "            improved += 1\n",
    "        elif is_correct_base and not is_correct_pt_rag:\n",
    "            status = \"Worsened\"\n",
    "            worsened += 1\n",
    "        \n",
    "        results.append({\n",
    "            \"Index\": idx,\n",
    "            \"Question\": q,\n",
    "            \"Ground_Truth\": gt,\n",
    "            \"Pred_Base_Raw\": pred_base,\n",
    "            \"Correct_Base_Raw\": is_correct_base,\n",
    "            \"Pred_PT_RAG\": pred_pt_rag,\n",
    "            \"Correct_PT_RAG\": is_correct_pt_rag,\n",
    "            \"Status\": status,\n",
    "            \"Retrieved_Context\": ctx_rag\n",
    "        })\n",
    "\n",
    "        print(f\"[{idx}] GT:{gt} | Base:{pred_base} {'O' if is_correct_base else 'X'} | PT+RAG:{pred_pt_rag} {'O' if is_correct_pt_rag else 'X'} | {status}\")\n",
    "\n",
    "    #Calculate Final Metrics\n",
    "    acc_base = correct_base / total if total > 0 else 0\n",
    "    acc_pt_rag = correct_pt_rag / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\n=== Final Results ({total} questions) ===\")\n",
    "    print(f\"Base GPT-2 (Raw) Accuracy   : {acc_base:.4f}\")\n",
    "    print(f\"Prompt Tuning + RAG Accuracy: {acc_pt_rag:.4f}\")\n",
    "    print(f\"Improved Cases: {improved}\")\n",
    "    print(f\"Worsened Cases: {worsened}\")\n",
    "    \n",
    "    # === Save File ===\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        df[\"Global_Acc_Base\"] = f\"{acc_base:.2%}\"\n",
    "        df[\"Global_Acc_PT_RAG\"] = f\"{acc_pt_rag:.2%}\"\n",
    "        \n",
    "        df.to_csv(full_output_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Detailed results saved: {full_output_path}\")\n",
    "        \n",
    "        # Save Summary\n",
    "        base, ext = os.path.splitext(output_file)\n",
    "        summary_filename = f\"{base}_summary{ext}\"\n",
    "        full_summary_path = os.path.join(output_dir, summary_filename)\n",
    "        \n",
    "        summary_data = [{\n",
    "            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"Experiment\": \"Base_vs_PromptTuningRAG\",\n",
    "            \"Range\": f\"{start_idx}-{end_idx}\",\n",
    "            \"Total\": total,\n",
    "            \"Acc_Base\": acc_base,\n",
    "            \"Acc_PT_RAG\": acc_pt_rag,\n",
    "            \"Improved\": improved,\n",
    "            \"Worsened\": worsened\n",
    "        }]\n",
    "        pd.DataFrame(summary_data).to_csv(full_summary_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Summary statistics saved: {full_summary_path}\")\n",
    "    else:\n",
    "        print(\"No results generated, skipping save.\")\n",
    "\n",
    "# Execution Entry Point\n",
    "if __name__ == \"__main__\":\n",
    "    # Run comparison test for 0 to 100 items\n",
    "    evaluate_pt_rag_comparison(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fb28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
