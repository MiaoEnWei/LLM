{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e79179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Reading data: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\n",
      "[Info] Data loaded successfully. Count: 100\n",
      "[Info] Loading model: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/llama2\n",
      "[Info] Enabling 4-bit quantization for speed optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7d9cef752c481c8981199f7526852c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info] Starting comparison experiment (Limit=100)...\n",
      " - No RAG: Closed Book (No context provided)\n",
      " - With RAG: Open Book (Context provided)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/100 [00:00<?, ?it/s]/home/miaoen/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/miaoen/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Inference: 100%|██████████| 100/100 [24:46<00:00, 14.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info] Calculating ROUGE scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/miaoen/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/6e5315f72865c2eaa764c8361360bb938740b9c120a2cf3a7ad218aa0ce452ed (last modified on Sat Nov 15 19:23:49 2025) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] ROUGE calculation failed: No module named 'absl'\n",
      "[Info] Detailed results saved to: eval_out_notebook/run_20251222_164019_fixed_comparison/results_fixed.csv\n",
      "\n",
      "[Info] Top 3 Most Improved Cases:\n",
      "\n",
      "[Case 99] Score Diff: +0.0000\n",
      "Question: Does the treatment of amblyopia normalise subfoveal choroidal thickness in amblyopic children?\n",
      "No RAG Answer: No, the treatment of amblyopia does not necessarily normalize subfoveal choroidal thickness in ambly...\n",
      "With RAG Answer: According to the retrieved documents, the answer is no. The study found that amblyopia treatment did...\n",
      "--------------------------------------------------\n",
      "\n",
      "[Case 0] Score Diff: +0.0000\n",
      "Question: Malnutrition, a new inducer for arterial calcification in hemodialysis patients?\n",
      "No RAG Answer: Malnutrition has been identified as a potential inducer of arterial calcification in hemodialysis pa...\n",
      "With RAG Answer: According to the retrieved documents, there is evidence to suggest that malnutrition may be a new in...\n",
      "--------------------------------------------------\n",
      "\n",
      "[Case 1] Score Diff: +0.0000\n",
      "Question: Should temperature be monitorized during kidney allograft preservation?\n",
      "No RAG Answer: Yes, temperature should be monitored during kidney allograft preservation. The ideal temperature ran...\n",
      "With RAG Answer: Yes, temperature should be monitored during kidney allograft preservation. According to the retrieve...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    parquet_path = r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\"\n",
    "    model_path = r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/llama2\" \n",
    "    \n",
    "    # Parameters\n",
    "    limit = 100\n",
    "    use_chat_template = True \n",
    "    max_ctx_chars = 4000\n",
    "    seed = 2025\n",
    "\n",
    "args = Config()\n",
    "\n",
    "# seed\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "\n",
    "# 1.build Prompt\n",
    "\n",
    "def join_context(c, max_chars):\n",
    "    try:\n",
    "        ctxs = (c or {}).get(\"contexts\", [])\n",
    "        return \" \".join(ctxs)[:max_chars] if ctxs else \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def build_prompt(tok, q, ctx, use_rag=False):\n",
    "    \"\"\"\n",
    "    核心修改：通过 use_rag 参数控制是否提供上下文\n",
    "    \"\"\"\n",
    "    # System Prompt\n",
    "    system_msg = (\n",
    "        \"You are a biomedical expert. Answer the question directly and concisely. \"\n",
    "        \"Do NOT start with 'Thank you'. Just provide the medical answer.\"\n",
    "    )\n",
    "    \n",
    "    if use_rag:\n",
    "        # With RAG\n",
    "        user_content = (\n",
    "            f\"Retrieved Documents:\\n{ctx}\\n\\n\"\n",
    "            f\"Question: {q}\\n\\n\"\n",
    "            \"Answer the question based strictly on the retrieved documents above.\"\n",
    "        )\n",
    "    else:\n",
    "        # No RAG\n",
    "        user_content = (\n",
    "            f\"Question: {q}\\n\\n\"\n",
    "            \"Answer the question to the best of your knowledge. If you don't know, make an educated guess.\"\n",
    "        )\n",
    "\n",
    "    # Chat Template\n",
    "    if args.use_chat_template and hasattr(tok, \"apply_chat_template\"):\n",
    "        msgs = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "        try:\n",
    "            return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Fallback\n",
    "    return f\"[INST] <<SYS>>\\n{system_msg}\\n<</SYS>>\\n\\n{user_content} [/INST]\"\n",
    "\n",
    "def clean_output(full_text, prompt_len):\n",
    "    \"\"\"clean output\"\"\"\n",
    "    gen_text = full_text[prompt_len:].strip()\n",
    "    gen_text = gen_text.replace(\"[/INST]\", \"\").strip()\n",
    "    return gen_text if gen_text else \"Error\"\n",
    "\n",
    "\n",
    "# 2. load data\n",
    "\n",
    "print(f\"[Info] Reading data: {args.parquet_path}\")\n",
    "\n",
    "if not os.path.exists(args.parquet_path):\n",
    "    print(\"[Warning] Data file not found. Using mock data for demonstration.\")\n",
    "    questions = [\"Does study X show that drug A is better than B?\", \"What is the primary outcome of this specific trial?\"] * 25\n",
    "    ctx_list = [\"Study X shows Drug A reduced symptoms by 50% compared to B.\", \"The primary outcome was reduction in mortality.\"] * 25\n",
    "    refs = [\"Yes, drug A is better.\", \"Reduction in mortality.\"] * 25\n",
    "    questions, ctx_list, refs = questions[:args.limit], ctx_list[:args.limit], refs[:args.limit]\n",
    "else:\n",
    "    try:\n",
    "        tbl = pq.read_table(args.parquet_path)\n",
    "        df = tbl.to_pandas().dropna().head(args.limit)\n",
    "        \n",
    "        questions = df[\"question\"].tolist()\n",
    "        ctx_list = df[\"context\"].map(lambda c: join_context(c, args.max_ctx_chars)).tolist()\n",
    "        \n",
    "        target_col = \"long_answer\" if \"long_answer\" in df.columns else \"final_decision\"\n",
    "        if target_col not in df.columns: target_col = df.columns[-1]\n",
    "        refs = df[target_col].tolist()\n",
    "        \n",
    "        print(f\"[Info] Data loaded successfully. Count: {len(questions)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to read parquet file: {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "# 3. 4-bit\n",
    "\n",
    "print(f\"[Info] Loading model: {args.model_path}\")\n",
    "\n",
    "try:\n",
    "    tok = AutoTokenizer.from_pretrained(args.model_path, use_fast=True, local_files_only=True)\n",
    "except:\n",
    "    print(\"[Info] Local tokenizer not found, downloading...\")\n",
    "    tok = AutoTokenizer.from_pretrained(args.model_path, use_fast=True)\n",
    "\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "# 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"[Info] Enabling 4-bit quantization for speed optimization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"[Error] Model loading failed: {e}\")\n",
    "    print(\"Please ensure 'bitsandbytes' is installed: pip install bitsandbytes accelerate\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "\n",
    "time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = os.path.join(\"eval_out_notebook\", f\"run_{time_str}_fixed_comparison\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "preds_no = []\n",
    "preds_rag = []\n",
    "\n",
    "print(f\"\\n[Info] Starting comparison experiment (Limit={len(questions)})...\")\n",
    "print(\" - No RAG: Closed Book (No context provided)\")\n",
    "print(\" - With RAG: Open Book (Context provided)\")\n",
    "\n",
    "for q, ctx in tqdm(zip(questions, ctx_list), total=len(questions), desc=\"Inference\"):\n",
    "    \n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"pad_token_id\": tok.pad_token_id,\n",
    "        \"eos_token_id\": tok.eos_token_id,\n",
    "        \"do_sample\": False,\n",
    "        \"repetition_penalty\": 1.1\n",
    "    }\n",
    "\n",
    "    #A. No RAG\n",
    "    prompt_no = build_prompt(tok, q, ctx, use_rag=False) \n",
    "    inputs_no = tok(prompt_no, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_no = model.generate(**inputs_no, **gen_kwargs)\n",
    "        \n",
    "    input_len_no = inputs_no[\"input_ids\"].shape[1]\n",
    "    decoded_no = tok.decode(out_no[0], skip_special_tokens=True)\n",
    "    preds_no.append(clean_output(decoded_no, len(tok.decode(out_no[0][:input_len_no], skip_special_tokens=True))))\n",
    "    \n",
    "    #B. With RAG\n",
    "    prompt_rag = build_prompt(tok, q, ctx, use_rag=True) \n",
    "    inputs_rag = tok(prompt_rag, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_rag = model.generate(**inputs_rag, **gen_kwargs)\n",
    "        \n",
    "    input_len_rag = inputs_rag[\"input_ids\"].shape[1]\n",
    "    decoded_rag = tok.decode(out_rag[0], skip_special_tokens=True)\n",
    "    preds_rag.append(clean_output(decoded_rag, len(tok.decode(out_rag[0][:input_len_rag], skip_special_tokens=True))))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n[Info] Calculating ROUGE scores...\")\n",
    "try:\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    scores_no = rouge.compute(predictions=preds_no, references=refs, use_aggregator=False)[\"rougeLsum\"]\n",
    "    scores_rag = rouge.compute(predictions=preds_rag, references=refs, use_aggregator=False)[\"rougeLsum\"]\n",
    "\n",
    "    avg_no = np.mean(scores_no)\n",
    "    avg_rag = np.mean(scores_rag)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"No RAG (Closed Book): {avg_no:.4f}\")\n",
    "    print(f\"With RAG (Open Book): {avg_rag:.4f}\")\n",
    "    print(f\"Net Gain            : +{(avg_rag - avg_no):.4f}\")\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"[Error] ROUGE calculation failed: {e}\")\n",
    "    scores_no, scores_rag = [0]*len(questions), [0]*len(questions)\n",
    "\n",
    "# save to csv\n",
    "df_res = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"ref\": refs,\n",
    "    \"pred_no_rag\": preds_no,\n",
    "    \"pred_with_rag\": preds_rag,\n",
    "    \"score_no\": scores_no,\n",
    "    \"score_rag\": scores_rag,\n",
    "    \"diff\": np.array(scores_rag) - np.array(scores_no)\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(run_dir, \"results_fixed.csv\")\n",
    "df_res.to_csv(csv_path, index=False)\n",
    "print(f\"[Info] Detailed results saved to: {csv_path}\")\n",
    "\n",
    "# print sample\n",
    "if len(df_res) > 0:\n",
    "    print(\"\\n[Info] Top 3 Most Improved Cases:\")\n",
    "    top_improved = df_res.sort_values(by=\"diff\", ascending=False).head(3)\n",
    "    for idx, row in top_improved.iterrows():\n",
    "        print(f\"\\n[Case {idx}] Score Diff: +{row['diff']:.4f}\")\n",
    "        print(f\"Question: {row['question']}\")\n",
    "        print(f\"No RAG Answer: {str(row['pred_no_rag'])[:100]}...\")\n",
    "        print(f\"With RAG Answer: {str(row['pred_with_rag'])[:100]}...\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332ef01",
   "metadata": {},
   "source": [
    "LLAMA2 BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Reading file: C:\\LLM\\PrimeKG\\eval_out_notebook\\run_20251209_203948_gpt2\\results.csv\n",
      "[Success] Data loaded successfully. Total samples: 50\n",
      "\n",
      "[Info] Loading BERTScore model (First time run will download roberta-large, ~1.4GB)...\n",
      "[Info] Compute device: cuda\n",
      "\n",
      "[Status] Calculating BERTScore for No RAG (1/2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Status] Calculating BERTScore for With RAG (2/2)...\n",
      "\n",
      "========\n",
      "BERTScore Evaluation Results (F1 Score)\n",
      "========\n",
      "   (1.0 = Perfect semantic match, 0.0 = Completely irrelevant)\n",
      "--------------------------------------------------\n",
      "No RAG (Closed Book) : 0.81608\n",
      "With RAG (Open Book) : 0.81667\n",
      "Net Semantic Gain    : +0.00059\n",
      "========\n",
      "\n",
      "Top 3 Cases with Highest Semantic Improvement:\n",
      "\n",
      "[Index: 47] Semantic Score Gain: +0.0259\n",
      "Question: Human papillomavirus and pterygium. Is the virus a risk factor?\n",
      "Reference Answer: The low presence of HPV DNA in pterygia does not support the hypothesis that HPV is involved in the development of pterygia in Denmark.\n",
      "No RAG Prediction: Human Papiloma Virus (HPV) is an infectious disease of human origin, which has been identified as be...\n",
      "With RAG Prediction: The human Papilioid Virus (HPV) is an infectious viral disease that causes severe, chronic inflammat...\n",
      "------------------------------------------------------------\n",
      "[Index: 11] Semantic Score Gain: +0.0096\n",
      "Question: Does laparoscopic surgery decrease the risk of atrial fibrillation after foregut surgery?\n",
      "Reference Answer: Laparoscopic surgery is associated with lower risk of atrial fibrillation in foregut surgery. Development of atrial fibrillation is associated with increased length of intensive care stay. We recommend a prospective trial to confirm our findings.\n",
      "No RAG Prediction: The study was conducted by Drs. Tzur and Karpeles, who are both members-initiators for a new researc...\n",
      "With RAG Prediction: The study was conducted by a team from University College London, UK (UCL) to examine whether or not...\n",
      "------------------------------------------------------------\n",
      "[Index: 32] Semantic Score Gain: +0.0076\n",
      "Question: Does increased nerve length within the treatment volume improve trigeminal neuralgia radiosurgery?\n",
      "Reference Answer: Increasing the treatment volume to include a longer nerve length for trigeminal neuralgia radiosurgery does not significantly improve pain relief but may increase complications.\n",
      "No RAG Prediction: The answer is yes, but it's not a good idea to use this technique for any reason other than that you...\n",
      "With RAG Prediction: The answer to this is yes, it does not affect your ability or performance at all (or any other aspec...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Info] Results saved to: C:\\LLM\\PrimeKG\\eval_out_notebook\\run_20251209_203948_gpt2\\results_bertscore_evaluated.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "\n",
    "# 1. Configure Path\n",
    "#   Using the absolute path you provided directly\n",
    "csv_path = r\"C:\\LLM\\PrimeKG\\eval_out_notebook\\run_20251210_133244_fixed_comparison\\results_fixed.csv\"\n",
    "\n",
    "\n",
    "# 2. Load Data\n",
    "print(f\"[Info] Reading file: {csv_path}\")\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    print(f\"[Error] File not found. Please check the path: {csv_path}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"[Success] Data loaded successfully. Total samples: {len(df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"[Error] Failed to read CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Extract key columns (prevent null values, convert to string)\n",
    "preds_no = df[\"pred_no_rag\"].fillna(\"\").astype(str).tolist()\n",
    "preds_rag = df[\"pred_with_rag\"].fillna(\"\").astype(str).tolist()\n",
    "refs = df[\"ref\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "\n",
    "# 3. Load BERTScore Model\n",
    "print(\"\\n[Info] Loading BERTScore model (First time run will download roberta-large, ~1.4GB)...\")\n",
    "try:\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "except Exception as e:\n",
    "    print(f\"[Error] Loading failed: {e}\")\n",
    "    print(\"Please check network connection, or run: pip install bert_score\")\n",
    "    exit()\n",
    "\n",
    "# Use GPU acceleration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Info] Compute device: {device}\")\n",
    "\n",
    "\n",
    "# 4. Start Calculation\n",
    "# batch_size=32 fits RTX 3060 (12G), adjust to 16 or 8 if VRAM overflows\n",
    "print(\"\\n[Status] Calculating BERTScore for No RAG (1/2)...\")\n",
    "res_no = bertscore.compute(predictions=preds_no, references=refs, lang=\"en\", device=device, batch_size=32)\n",
    "f1_no = np.array(res_no['f1'])\n",
    "\n",
    "print(\"[Status] Calculating BERTScore for With RAG (2/2)...\")\n",
    "res_rag = bertscore.compute(predictions=preds_rag, references=refs, lang=\"en\", device=device, batch_size=32)\n",
    "f1_rag = np.array(res_rag['f1'])\n",
    "\n",
    "\n",
    "# 5. Results Summary\n",
    "avg_no = f1_no.mean()\n",
    "avg_rag = f1_rag.mean()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BERTScore Evaluation Results (F1 Score)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"   (1.0 = Perfect semantic match, 0.0 = Completely irrelevant)\")\n",
    "print(f\"{'-'*50}\")\n",
    "print(f\"No RAG (Closed Book) : {avg_no:.5f}\")\n",
    "print(f\"With RAG (Open Book) : {avg_rag:.5f}\")\n",
    "print(f\"Net Semantic Gain    : +{(avg_rag - avg_no):.5f}\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "\n",
    "# 6. Case Analysis and Saving\n",
    "# Merge scores back into DataFrame\n",
    "df[\"bert_score_no\"] = f1_no\n",
    "df[\"bert_score_rag\"] = f1_rag\n",
    "df[\"bert_diff\"] = f1_rag - f1_no\n",
    "\n",
    "# Find Top 3 cases with highest semantic improvement\n",
    "top_improved = df.sort_values(by=\"bert_diff\", ascending=False).head(3)\n",
    "\n",
    "print(\"Top 3 Cases with Highest Semantic Improvement:\\n\")\n",
    "for idx, row in top_improved.iterrows():\n",
    "    print(f\"[Index: {idx}] Semantic Score Gain: +{row['bert_diff']:.4f}\")\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Reference Answer: {row['ref']}\")\n",
    "    print(f\"No RAG Prediction: {row['pred_no_rag'][:100]}...\")\n",
    "    print(f\"With RAG Prediction: {row['pred_with_rag'][:100]}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Save new results\n",
    "output_path = csv_path.replace(\".csv\", \"_bertscore_evaluated.csv\")\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\n[Info] Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0108ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    logging as hf_logging\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# Global Configuration Class\n",
    "class Config:\n",
    "    # Local data path\n",
    "    parquet_path = r\"C:\\LLM\\data\\pubmedqa_hf\\pqa_labeled\\train-00000-of-00001.parquet\"\n",
    "    \n",
    "    # Use the permission-free Llama-2-7b-chat mirror/model\n",
    "    model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "    \n",
    "    # Enable Chat Template\n",
    "    use_chat_template = True \n",
    "    \n",
    "    # Number of test samples (None to run the full dataset)\n",
    "    limit = 10 \n",
    "    \n",
    "    # Generation parameters\n",
    "    max_new_tokens = 128\n",
    "    max_ctx_chars = 4000\n",
    "    batch_size = 2\n",
    "    \n",
    "    output_dir = \"eval_results_llama2\"\n",
    "    seed = 42\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# Data Loading\n",
    "print(f\"Reading data from: {cfg.parquet_path}\")\n",
    "if not os.path.exists(cfg.parquet_path):\n",
    "    raise FileNotFoundError(f\"File not found: {cfg.parquet_path}\")\n",
    "\n",
    "df = pd.read_parquet(cfg.parquet_path).dropna(subset=['question', 'long_answer'])\n",
    "if cfg.limit:\n",
    "    df = df.head(cfg.limit)\n",
    "\n",
    "def extract_context_text(c):\n",
    "    try:\n",
    "        if isinstance(c, dict) and 'contexts' in c: return \" \".join(list(c['contexts']))\n",
    "        if isinstance(c, list): return \" \".join(c)\n",
    "        return str(c)\n",
    "    except: return \"\"\n",
    "\n",
    "print(\"Processing contexts...\")\n",
    "ctx_list_rag = df[\"context\"].apply(extract_context_text).apply(lambda x: x[:cfg.max_ctx_chars]).tolist()\n",
    "ctx_list_base = [\"\"] * len(df)\n",
    "questions = df[\"question\"].tolist()\n",
    "refs = df[\"long_answer\"].tolist()\n",
    "print(f\"Loaded {len(df)} samples.\")\n",
    "\n",
    "# Prompt Construction Function (Adapted for Llama 2)\n",
    "def build_prompt(tok, q: str, ctx: str) -> str:\n",
    "    has_ctx = ctx and len(ctx.strip()) > 0\n",
    "    sys_msg = \"You are a helpful biomedical expert. Answer the question precisely based on the provided context if available.\"\n",
    "    \n",
    "    if has_ctx:\n",
    "        user_msg = (\n",
    "            f\"Context:\\n{ctx}\\n\\n\"\n",
    "            f\"Question: {q}\\n\"\n",
    "            f\"Task: Provide a concise medical reasoning (1-2 sentences) followed by a final decision: 'Yes', 'No', or 'Maybe'.\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "    else:\n",
    "        user_msg = (\n",
    "            f\"Question: {q}\\n\"\n",
    "            f\"Task: Provide a concise medical reasoning (1-2 sentences) followed by a final decision: 'Yes', 'No', or 'Maybe'.\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_msg},\n",
    "        {\"role\": \"user\", \"content\": user_msg}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    except:\n",
    "        return f\"[INST] <<SYS>>\\n{sys_msg}\\n<</SYS>>\\n\\n{user_msg} [/INST]\"\n",
    "\n",
    "# Keyword Recall Calculation\n",
    "def compute_keyword_recall(preds, refs):\n",
    "    scores = []\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    for p, r in zip(preds, refs):\n",
    "        try:\n",
    "            vectorizer.fit([r])\n",
    "            ref_keywords = set(vectorizer.get_feature_names_out())\n",
    "            if not ref_keywords: scores.append(0.0); continue\n",
    "            p_lower = p.lower()\n",
    "            hit = sum(1 for kw in ref_keywords if kw in p_lower)\n",
    "            scores.append(hit / len(ref_keywords))\n",
    "        except: scores.append(0.0)\n",
    "    return scores\n",
    "\n",
    "# Inference Function\n",
    "def run_inference(model, tok, qs, ctxs, cfg, label):\n",
    "    print(f\"Inference: {label} ...\")\n",
    "    preds = []\n",
    "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"left\"\n",
    "    \n",
    "    prompts = [build_prompt(tok, q, c) for q, c in zip(qs, ctxs)]\n",
    "    \n",
    "    for i in range(0, len(prompts), cfg.batch_size):\n",
    "        batch = prompts[i : i + cfg.batch_size]\n",
    "        inputs = tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=cfg.max_ctx_chars+512).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                pad_token_id=tok.pad_token_id,\n",
    "                do_sample=False,\n",
    "                temperature=0.0\n",
    "            )\n",
    "        \n",
    "        input_len = inputs.input_ids.shape[1]\n",
    "        decoded = tok.batch_decode(out[:, input_len:], skip_special_tokens=True)\n",
    "        preds.extend([d.strip() for d in decoded])\n",
    "        \n",
    "        if (i // cfg.batch_size) % 2 == 0: print(f\"Batch {i//cfg.batch_size + 1} done.\")\n",
    "            \n",
    "    return preds\n",
    "\n",
    "# Model Loading (4-bit Quantization)\n",
    "print(f\"Loading Model: {cfg.model_name} with 4-bit quantization...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    tok = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
    "    tok.pad_token = tok.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully on GPU!\")\n",
    "except Exception as e:\n",
    "    print(f\"Load Error: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Execute Inference\n",
    "preds_base = run_inference(model, tok, questions, ctx_list_base, cfg, label=\"Base (No Context)\")\n",
    "preds_rag = run_inference(model, tok, questions, ctx_list_rag, cfg, label=\"RAG (With Context)\")\n",
    "\n",
    "# Evaluation and Saving\n",
    "print(\"\\nCalculating Metrics...\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "r_base = rouge.compute(predictions=preds_base, references=refs)\n",
    "r_rag = rouge.compute(predictions=preds_rag, references=refs)\n",
    "\n",
    "print(\"Calculating BERTScore...\")\n",
    "bs_base = bertscore.compute(predictions=preds_base, references=refs, lang=\"en\", model_type=\"distilbert-base-uncased\")\n",
    "bs_rag = bertscore.compute(predictions=preds_rag, references=refs, lang=\"en\", model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "kw_base = compute_keyword_recall(preds_base, refs)\n",
    "kw_rag = compute_keyword_recall(preds_rag, refs)\n",
    "\n",
    "summary = {\n",
    "    \"Model\": cfg.model_name,\n",
    "    \"ROUGE-L\": {\"Base\": r_base['rougeL'], \"RAG\": r_rag['rougeL']},\n",
    "    \"BERTScore-F1\": {\"Base\": np.mean(bs_base['f1']), \"RAG\": np.mean(bs_rag['f1'])},\n",
    "    \"Keyword-Recall\": {\"Base\": np.mean(kw_base), \"RAG\": np.mean(kw_rag)}\n",
    "}\n",
    "\n",
    "print(\"\\nEVALUATION SUMMARY (Llama-2)\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "save_path = os.path.join(cfg.output_dir, f\"llama2_results_{datetime.now().strftime('%H%M%S')}.jsonl\")\n",
    "\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(preds_base)):\n",
    "        row = {\n",
    "            \"id\": i,\n",
    "            \"question\": questions[i],\n",
    "            \"reference\": refs[i],\n",
    "            \"base_pred\": preds_base[i],\n",
    "            \"rag_pred\": preds_rag[i],\n",
    "            \"base_kw\": kw_base[i],\n",
    "            \"rag_kw\": kw_rag[i]\n",
    "        }\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286c898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loaded: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_out_notebook/run_20251209_203948_gpt2/results_bertscore_evaluated.csv | rows: 50\n",
      "[Info] Columns: ['question', 'retrieved_facts', 'ref', 'pred_no_rag', 'pred_with_rag', 'score_no', 'score_rag', 'diff', 'bert_score_no', 'bert_score_rag', 'bert_diff']\n",
      "\n",
      "[Top3] Highest RAG BERTScore (bert_score_rag):\n",
      " bert_score_rag  bert_diff                                                                                              question\n",
      "       0.852124   0.000000 Preoperative locoregional staging of gastric cancer: is there a place for magnetic resonance imaging?\n",
      "       0.839119   0.009647             Does laparoscopic surgery decrease the risk of atrial fibrillation after foregut surgery?\n",
      "       0.839060   0.000000                 Is there any evidence of a \"July effect\" in patients undergoing major cancer surgery?\n",
      "\n",
      "[Bottom3] Lowest RAG BERTScore (bert_score_rag):\n",
      " bert_score_rag  bert_diff                                                                                   question\n",
      "       0.795418        0.0 Cancer of the buccal mucosa: are margins and T-stage accurate predictors of local control?\n",
      "       0.798283        0.0   Juvenile ankylosing spondylitis--is it the same disease as adult ankylosing spondylitis?\n",
      "       0.800442        0.0                       Landolt C and snellen e acuity: differences in strabismus amblyopia?\n",
      "\n",
      "[Top3] Largest Gain (bert_diff):\n",
      " bert_diff  bert_score_no  bert_score_rag                                                                                           question\n",
      "  0.025915       0.798649        0.824564                                    Human papillomavirus and pterygium. Is the virus a risk factor?\n",
      "  0.009647       0.829472        0.839119          Does laparoscopic surgery decrease the risk of atrial fibrillation after foregut surgery?\n",
      "  0.007556       0.806088        0.813644 Does increased nerve length within the treatment volume improve trigeminal neuralgia radiosurgery?\n",
      "\n",
      "[Bottom3] Worst Gain (bert_diff):\n",
      "    bert_diff  bert_score_no  bert_score_rag                                                                                                                                          question\n",
      "-1.670361e-02       0.834256        0.817552 Is first-line single-agent mitoxantrone in the treatment of high-risk metastatic breast cancer patients as effective as combination chemotherapy?\n",
      "-5.302668e-03       0.830958        0.825655                                                              Does early migraine treatment shorten time to headache peak and reduce its severity?\n",
      "-5.960464e-08       0.810807        0.810806                                                Does a history of unintended pregnancy lessen the likelihood of desire for sterilization reversal?\n",
      "\n",
      "[Saved]\n",
      " - top3_rag: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_out_notebook/run_20251209_203948_gpt2/results_bertscore_evaluated_top3_bert_rag.csv\n",
      " - bot3_rag: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_out_notebook/run_20251209_203948_gpt2/results_bertscore_evaluated_bottom3_bert_rag.csv\n",
      " - top3_gain: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_out_notebook/run_20251209_203948_gpt2/results_bertscore_evaluated_top3_bert_gain.csv\n",
      " - bot3_gain: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_out_notebook/run_20251209_203948_gpt2/results_bertscore_evaluated_bottom3_bert_gain.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_out_notebook/run_20251209_203948_gpt2/results_bertscore_evaluated.csv\"\n",
    "\n",
    "assert os.path.exists(csv_path), f\"File not found: {csv_path}\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df.columns = [c.strip() for c in df.columns]  # Strip whitespace from column names to prevent errors\n",
    "print(\"[Info] Loaded:\", csv_path, \"| rows:\", len(df))\n",
    "print(\"[Info] Columns:\", list(df.columns))\n",
    "\n",
    "# Mandatory Column Check\n",
    "need = [\"question\", \"ref\", \"pred_no_rag\", \"pred_with_rag\", \"bert_score_no\", \"bert_score_rag\", \"bert_diff\"]\n",
    "missing = [c for c in need if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns: {missing}\\nAvailable: {list(df.columns)}\")\n",
    "\n",
    "# Convert to Numeric\n",
    "for c in [\"bert_score_no\", \"bert_score_rag\", \"bert_diff\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"bert_score_rag\", \"bert_score_no\", \"bert_diff\"]).reset_index(drop=True)\n",
    "\n",
    "# Top/Bottom Export\n",
    "top3_rag = df.sort_values(\"bert_score_rag\", ascending=False).head(3)\n",
    "bot3_rag = df.sort_values(\"bert_score_rag\", ascending=True).head(3)\n",
    "\n",
    "top3_gain = df.sort_values(\"bert_diff\", ascending=False).head(3)\n",
    "bot3_gain = df.sort_values(\"bert_diff\", ascending=True).head(3)\n",
    "\n",
    "show_cols = [\n",
    "    \"question\", \"ref\", \"pred_no_rag\", \"pred_with_rag\",\n",
    "    \"bert_score_no\", \"bert_score_rag\", \"bert_diff\"\n",
    "]\n",
    "\n",
    "print(\"\\n[Top3] Highest RAG BERTScore (bert_score_rag):\")\n",
    "print(top3_rag[[\"bert_score_rag\",\"bert_diff\",\"question\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n[Bottom3] Lowest RAG BERTScore (bert_score_rag):\")\n",
    "print(bot3_rag[[\"bert_score_rag\",\"bert_diff\",\"question\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n[Top3] Largest Gain (bert_diff):\")\n",
    "print(top3_gain[[\"bert_diff\",\"bert_score_no\",\"bert_score_rag\",\"question\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n[Bottom3] Worst Gain (bert_diff):\")\n",
    "print(bot3_gain[[\"bert_diff\",\"bert_score_no\",\"bert_score_rag\",\"question\"]].to_string(index=False))\n",
    "\n",
    "# Save to the same directory\n",
    "out_dir = os.path.dirname(csv_path) or \".\"\n",
    "base = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "\n",
    "paths = {\n",
    "    \"top3_rag\":  os.path.join(out_dir, f\"{base}_top3_bert_rag.csv\"),\n",
    "    \"bot3_rag\":  os.path.join(out_dir, f\"{base}_bottom3_bert_rag.csv\"),\n",
    "    \"top3_gain\": os.path.join(out_dir, f\"{base}_top3_bert_gain.csv\"),\n",
    "    \"bot3_gain\": os.path.join(out_dir, f\"{base}_bottom3_bert_gain.csv\"),\n",
    "}\n",
    "\n",
    "top3_rag[show_cols].to_csv(paths[\"top3_rag\"], index=False, encoding=\"utf-8-sig\")\n",
    "bot3_rag[show_cols].to_csv(paths[\"bot3_rag\"], index=False, encoding=\"utf-8-sig\")\n",
    "top3_gain[show_cols].to_csv(paths[\"top3_gain\"], index=False, encoding=\"utf-8-sig\")\n",
    "bot3_gain[show_cols].to_csv(paths[\"bot3_gain\"], index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[Saved]\")\n",
    "for k, p in paths.items():\n",
    "    print(f\" - {k}: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a2ee5",
   "metadata": {},
   "source": [
    "llama2_pubmedqa_primekg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fc75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] Loading /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet...\n",
      "[KG] Building from /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/kg.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KG Chunks: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (3,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n",
      "KG Chunks: 1it [00:05,  5.49s/it]/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (3,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n",
      "KG Chunks: 2it [00:11,  5.65s/it]/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (3,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n",
      "KG Chunks: 3it [00:17,  5.95s/it]/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (3,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n",
      "KG Chunks: 5it [00:24,  4.80s/it]\n",
      "Retrieving: 100%|██████████| 100/100 [00:02<00:00, 43.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Loading 4-bit Llama from /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/llama2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16bd4d56a0d4bc0aeff1bc5f067fe76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer base: 100%|██████████| 25/25 [07:12<00:00, 17.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fix] base: repairing unknown=15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fix base: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]\n",
      "Infer rag: 100%|██████████| 25/25 [07:24<00:00, 17.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fix] rag: repairing unknown=33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fix rag: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Distribution Check ===\n",
      "GT dist      : Counter({'yes': 62, 'no': 30, 'maybe': 8})\n",
      "Base dec dist: Counter({'maybe': 60, 'yes': 32, 'no': 8})\n",
      "RAG  dec dist: Counter({'maybe': 55, 'yes': 37, 'no': 8})\n",
      "\n",
      "Summary:\n",
      "Base ACC: 0.2900\n",
      "RAG ACC: 0.2800\n",
      "[Saved] eval_out_closed_book_vs_rag/result_20251231_2250.csv\n",
      "\n",
      "--- sample 0 ---\n",
      "Q : Malnutrition, a new inducer for arterial calcification in hemodialysis patients?\n",
      "GT: yes\n",
      "BASE(dec): yes\n",
      "BASE out : Reasoning:\n",
      "Malnutrition has been increasingly recognized as a significant risk factor for cardiovascular disease in hemodialysis patients. Recent studies have suggested that malnutrition may also contribute to the development of arterial calcification in this population. For example, a study published in the Journal of the American Society of Nephrology found that malnourished hemodialysis patient\n",
      "RAG (dec): yes\n",
      "RAG  out : Reasoning:\n",
      "Arterial calcification is a known complication in hemodialysis patients, and it has been established that malnutrition can be an inducer of this complication. The presence of malnutrition in a parent-child or sibling pair suggests a genetic predisposition to malnutrition, which may contribute to the development of arterial calcification in hemodialysis patients.\n",
      "\n",
      "Final Decision: yes\n",
      "\n",
      "Ma\n",
      "\n",
      "--- sample 1 ---\n",
      "Q : Should temperature be monitorized during kidney allograft preservation?\n",
      "GT: no\n",
      "BASE(dec): yes\n",
      "BASE out : Reasoning: Temperature monitoring during kidney allograft preservation is crucial to ensure proper tissue viability and function. Temperature fluctuations can cause cellular damage, leading to ischemia-reperfusion injury, which can result in graft failure. Studies have shown that maintaining a stable temperature range during preservation can reduce the risk of this injury. For example, a study pub\n",
      "RAG (dec): yes\n",
      "RAG  out : Reasoning:\n",
      "\n",
      "Temperature regulation is a critical aspect of human physiology, and abnormalities in this process can have significant implications for the health of individuals, particularly children. The facts provided in the context highlight several aspects of temperature regulation in the context of parent-child relationships.\n",
      "\n",
      "Firstly, abnormalities in temperature regulation can result in fever\n",
      "\n",
      "--- sample 2 ---\n",
      "Q : Screening for gestational diabetes mellitus: are the criteria proposed by the international association of the Diabetes and Pregnancy Study Groups cost-effective?\n",
      "GT: yes\n",
      "BASE(dec): yes\n",
      "BASE out : Reasoning: The cost-effectiveness of the screening criteria proposed by the International Association of the Diabetes and Pregnancy Study Groups (IADPSG) for gestational diabetes mellitus (GDM) has been evaluated in several studies. These criteria include a 1-hour 50g glucose challenge test (GCT) for women with a history of GDM or other risk factors, and a 3-hour 100g or 2-hour 75g glucose toleran\n",
      "RAG (dec): maybe\n",
      "RAG  out : Reasoning: The cost-effectiveness of screening for gestational diabetes mellitus (GDM) proposed by the International Association of the Diabetes and Pregnancy Study Groups (IADPSG) can be evaluated by considering the benefits and harms of screening, as well as the costs associated with it.\n",
      "\n",
      "Benefits of screening for GDM include:\n",
      "\n",
      "* Early detection and treatment of GDM, which can reduce the risk of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# 1) Config (Only keep RAG evaluation)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    parquet_path: str = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\"\n",
    "    kg_csv_path: str = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/kg.csv\"\n",
    "    model_name_or_path: str = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/llama2\"\n",
    "\n",
    "    # Inference config\n",
    "    batch_size: int = 4\n",
    "    max_new_tokens: int = 256\n",
    "    fix_max_new_tokens: int = 16   # Second pass fix only needs a decision line, keep it short\n",
    "    temperature: float = 0.0       # Default to deterministic output (do_sample=False)\n",
    "\n",
    "    # Retrieval config\n",
    "    top_k_nodes: int = 8\n",
    "    top_facts_per_node: int = 6\n",
    "    max_ctx_chars: int = 1500\n",
    "\n",
    "    # KG Construction (PrimeKG is large, read in streams using chunksize)\n",
    "    kg_chunksize: int = 2_000_000\n",
    "    max_facts_per_node_cache: int = 50\n",
    "    max_node_doc_chars: int = 2000\n",
    "    use_display_relation: bool = True\n",
    "\n",
    "    # Evaluation settings\n",
    "    limit: Optional[int] = 100\n",
    "    seed: int = 42\n",
    "    out_dir: str = \"eval_out_rag_only\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "\n",
    "# 2) Utils & Data Loading\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Set random seed to ensure reproducibility.\"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def ensure_dir(d: str):\n",
    "    \"\"\"Ensure output directory exists.\"\"\"\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def load_pubmedqa(parquet_path: str, limit: Optional[int]) -> pd.DataFrame:\n",
    "    \"\"\"Load PubMedQA parquet and ensure question/final_decision are not empty.\"\"\"\n",
    "    print(f\"[Data] Loading {parquet_path}...\")\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    df = df.dropna(subset=[\"question\", \"final_decision\"])\n",
    "    if limit is not None:\n",
    "        df = df.head(limit)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 3) KG & Retrieval (Use PrimeKG for retrieval context)\n",
    "\n",
    "def make_fact(x, rel, y):\n",
    "    \"\"\"Assemble a triple into a 'fact' sentence.\"\"\"\n",
    "    if not x or not y:\n",
    "        return \"\"\n",
    "    return f\"Fact: {x} {rel} {y}.\"\n",
    "\n",
    "def build_kg_cache(csv_path):\n",
    "    \"\"\"\n",
    "    Stream build from kg.csv:\n",
    "    - node_info: node_id -> node_name\n",
    "    - adj: node_id -> [fact1, fact2, ...] (Cache max max_facts_per_node_cache facts per node)\n",
    "    \"\"\"\n",
    "    print(f\"[KG] Building from {csv_path}...\")\n",
    "    node_info, adj = {}, {}\n",
    "    iterator = pd.read_csv(csv_path, chunksize=cfg.kg_chunksize)\n",
    "\n",
    "    for chunk in tqdm(iterator, desc=\"KG Chunks\"):\n",
    "        chunk = chunk.fillna(\"\")\n",
    "\n",
    "        xs_nm = chunk[\"x_name\"].astype(str).values\n",
    "        ys_nm = chunk[\"y_name\"].astype(str).values\n",
    "        xs_id = chunk[\"x_id\"].astype(str).values\n",
    "        ys_id = chunk[\"y_id\"].astype(str).values\n",
    "        rels  = chunk[\"display_relation\"].astype(str).values if cfg.use_display_relation else chunk[\"relation\"].astype(str).values\n",
    "\n",
    "        for i in range(len(chunk)):\n",
    "            xid, xnm, yid, ynm, r = xs_id[i], xs_nm[i], ys_id[i], ys_nm[i], rels[i]\n",
    "\n",
    "            # Record node names\n",
    "            if xid not in node_info:\n",
    "                node_info[xid] = xnm\n",
    "            if yid not in node_info:\n",
    "                node_info[yid] = ynm\n",
    "\n",
    "            # Generate fact\n",
    "            f = make_fact(xnm, r, ynm)\n",
    "            if not f:\n",
    "                continue\n",
    "\n",
    "            # Attach fact to node x\n",
    "            if xid not in adj:\n",
    "                adj[xid] = []\n",
    "            if len(adj[xid]) < cfg.max_facts_per_node_cache:\n",
    "                adj[xid].append(f)\n",
    "\n",
    "            # Attach to node y as well (undirected diffusion, facilitates retrieval coverage)\n",
    "            if yid not in adj:\n",
    "                adj[yid] = []\n",
    "            if len(adj[yid]) < cfg.max_facts_per_node_cache:\n",
    "                adj[yid].append(f)\n",
    "\n",
    "    node_ids = list(node_info.keys())\n",
    "    return node_ids, node_info, adj\n",
    "\n",
    "def retrieve_kg(questions, vectorizer, X, node_ids, adj):\n",
    "    \"\"\"\n",
    "    TF-IDF Retrieval: Map question -> top_k_nodes, aggregate facts from these nodes as context.\n",
    "    \"\"\"\n",
    "    ctx_list = []\n",
    "    Q = vectorizer.transform(questions)\n",
    "\n",
    "    for i in tqdm(range(Q.shape[0]), desc=\"Retrieving\"):\n",
    "        scores = (Q[i] @ X.T).toarray().ravel()\n",
    "        top_idx = np.argsort(-scores)[:cfg.top_k_nodes]\n",
    "\n",
    "        facts = []\n",
    "        for idx in top_idx:\n",
    "            nid = node_ids[idx]\n",
    "            facts.extend(adj.get(nid, [])[:cfg.top_facts_per_node])\n",
    "\n",
    "        ctx = \"\\n\".join(facts)\n",
    "        ctx_list.append(ctx[:cfg.max_ctx_chars])\n",
    "\n",
    "    return ctx_list\n",
    "\n",
    "\n",
    "# 4) Metrics & Parsing (Only RAG evaluation)\n",
    "\n",
    "DEC_LABELS = (\"yes\", \"no\", \"maybe\")\n",
    "\n",
    "def norm_dec(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract decision from model output:\n",
    "    1) Prioritize finding 'Final Decision: yes/no/maybe'\n",
    "    2) If not found, fallback to matching any yes/no/maybe word\n",
    "    \"\"\"\n",
    "    s = (text or \"\").lower()\n",
    "    m = re.findall(r\"final decision\\s*:\\s*(yes|no|maybe)\", s)\n",
    "    if m:\n",
    "        return m[-1]\n",
    "    m2 = re.findall(r\"\\b(yes|no|maybe)\\b\", s)\n",
    "    return m2[-1] if m2 else \"unknown\"\n",
    "\n",
    "def calc_acc(gts, preds):\n",
    "    \"\"\"Decision Accuracy: Only count samples where gt belongs to yes/no/maybe.\"\"\"\n",
    "    correct, total = 0, 0\n",
    "    for g, p in zip(gts, preds):\n",
    "        gt = str(g).lower().strip()\n",
    "        pred = norm_dec(p)\n",
    "        if gt not in DEC_LABELS:\n",
    "            continue\n",
    "        total += 1\n",
    "        if gt == pred:\n",
    "            correct += 1\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "# 5) Prompts (Only keep RAG prompt + fix prompt)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a medical expert.\\n\"\n",
    "    \"Task: answer PubMed-style yes/no/maybe questions.\\n\"\n",
    "    \"Output format MUST be exactly:\\n\"\n",
    "    \"Reasoning: <one paragraph>\\n\"\n",
    "    \"Final Decision: yes|no|maybe\\n\"\n",
    "    \"If evidence is insufficient, choose maybe.\\n\"\n",
    ")\n",
    "\n",
    "FIX_SYSTEM_PROMPT = (\n",
    "    \"You are a strict formatter.\\n\"\n",
    "    \"Return ONLY one line in exactly this format:\\n\"\n",
    "    \"Final Decision: yes|no|maybe\\n\"\n",
    ")\n",
    "\n",
    "def format_prompt(tokenizer, question, context):\n",
    "    \"\"\"\n",
    "    RAG prompt: Must answer using Context and strictly follow output format.\n",
    "    \"\"\"\n",
    "    user_content = (\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer using the Context.\\n\"\n",
    "        \"Follow the required output format.\"\n",
    "    )\n",
    "\n",
    "    # If tokenizer supports apply_chat_template (Different Llama2 tokenizers might have/not have it)\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        msgs = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_content}]\n",
    "        return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Fallback: Pure text concatenation\n",
    "    return f\"{SYSTEM_PROMPT}\\nUser: {user_content}\\nAssistant:\"\n",
    "\n",
    "def format_fix_prompt(tokenizer, question, answer_text):\n",
    "    \"\"\"\n",
    "    Fix prompt: When decision cannot be extracted (unknown) from model output, generate again requesting only the decision line.\n",
    "    \"\"\"\n",
    "    user_content = (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Model Answer:\\n{answer_text}\\n\\n\"\n",
    "        \"Extract the decision.\"\n",
    "    )\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        msgs = [{\"role\": \"system\", \"content\": FIX_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_content}]\n",
    "        return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    return f\"{FIX_SYSTEM_PROMPT}\\nUser: {user_content}\\nAssistant:\"\n",
    "\n",
    "\n",
    "# 6) Inference (Keep your original left-padding slicing fix logic)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_batch(model, tokenizer, prompts: List[str], max_new_tokens: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Key points:\n",
    "    - With left padding, generate appends new tokens after the unified length of the batch\n",
    "    - So slicing must use inputs.input_ids.shape[1] (do not use attention_mask.sum)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    gen_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,  # You fixed this to deterministic output\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    cut = inputs.input_ids.shape[1]\n",
    "    outs = []\n",
    "    for j in range(gen_ids.size(0)):\n",
    "        new_ids = gen_ids[j, cut:]\n",
    "        outs.append(tokenizer.decode(new_ids, skip_special_tokens=True).strip())\n",
    "    return outs\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_eval_with_fix(model, tokenizer, questions, contexts):\n",
    "    \"\"\"\n",
    "    Run RAG only:\n",
    "    1) first pass generates complete answer\n",
    "    2) second pass: perform \"format fix\" for unknown outputs, forcing completion of Final Decision\n",
    "    \"\"\"\n",
    "    # 1) first pass\n",
    "    outputs = []\n",
    "    for i in tqdm(range(0, len(questions), cfg.batch_size), desc=\"Infer RAG\"):\n",
    "        batch_q = questions[i:i+cfg.batch_size]\n",
    "        batch_c = contexts[i:i+cfg.batch_size]\n",
    "        prompts = [format_prompt(tokenizer, q, c) for q, c in zip(batch_q, batch_c)]\n",
    "        outputs.extend(generate_batch(model, tokenizer, prompts, cfg.max_new_tokens))\n",
    "\n",
    "    # 2) second pass fix for unknown\n",
    "    unknown_idx = [i for i, t in enumerate(outputs) if norm_dec(t) == \"unknown\"]\n",
    "    if unknown_idx:\n",
    "        print(f\"[Fix] RAG: repairing unknown={len(unknown_idx)}/{len(outputs)}\")\n",
    "        for s in tqdm(range(0, len(unknown_idx), cfg.batch_size), desc=\"Fix RAG\"):\n",
    "            idxs = unknown_idx[s:s+cfg.batch_size]\n",
    "            fix_prompts = [format_fix_prompt(tokenizer, questions[k], outputs[k]) for k in idxs]\n",
    "            fixed = generate_batch(model, tokenizer, fix_prompts, cfg.fix_max_new_tokens)\n",
    "\n",
    "            # Write the fixed decision back to the end of the original output to ensure norm_dec can extract it\n",
    "            for k, fx in zip(idxs, fixed):\n",
    "                dec = norm_dec(fx)\n",
    "                if dec in DEC_LABELS:\n",
    "                    outputs[k] = outputs[k].rstrip() + \"\\nFinal Decision: \" + dec\n",
    "                else:\n",
    "                    outputs[k] = outputs[k].rstrip() + \"\\nFinal Decision: maybe\"\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# 7) Main (Only keep RAG path)\n",
    "\n",
    "def main():\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    # Read PubMedQA\n",
    "    df = load_pubmedqa(cfg.parquet_path, cfg.limit)\n",
    "    questions = df[\"question\"].astype(str).tolist()\n",
    "    gts = df[\"final_decision\"].astype(str).tolist()\n",
    "\n",
    "    # --- Build KG Cache (node -> facts) ---\n",
    "    node_ids, node_info, adj = build_kg_cache(cfg.kg_csv_path)\n",
    "\n",
    "    # --- Build a \"node document\" for each node, used for TF-IDF ---\n",
    "    # Note: Here we concatenate node_name + several facts into a document (truncated to max_node_doc_chars)\n",
    "    node_docs = [\n",
    "        f\"{node_info[nid]} {' '.join(adj.get(nid, []))}\"[:cfg.max_node_doc_chars]\n",
    "        for nid in node_ids\n",
    "    ]\n",
    "\n",
    "    # --- TF-IDF Modeling and Retrieval ---\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\", max_features=50000)\n",
    "    X = tfidf.fit_transform(node_docs)\n",
    "    kg_contexts = retrieve_kg(questions, tfidf, X, node_ids, adj)\n",
    "\n",
    "    # --- Load 4-bit Model ---\n",
    "    print(f\"[Model] Loading 4-bit Llama from {cfg.model_name_or_path}...\")\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name_or_path, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    # Single card: device_map={\"\":0}; Multi-card can be changed to \"auto\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name_or_path,\n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map={\"\": 0}\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # --- Run RAG Inference Only (with automatic unknown fix) ---\n",
    "    preds_rag = run_eval_with_fix(model, tokenizer, questions, kg_contexts)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    acc_rag = calc_acc(gts, preds_rag)\n",
    "\n",
    "    print(\"\\n=== Distribution Check ===\")\n",
    "    print(\"GT dist     :\", Counter([str(x).lower().strip() for x in gts]))\n",
    "    print(\"RAG dec dist:\", Counter([norm_dec(p) for p in preds_rag]))\n",
    "\n",
    "    print(f\"\\nSummary:\\nRAG ACC: {acc_rag:.4f}\")\n",
    "\n",
    "    # --- Save Results ---\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    out_csv = f\"{cfg.out_dir}/rag_only_{timestamp}.csv\"\n",
    "    res_df = pd.DataFrame({\n",
    "        \"question\": questions,\n",
    "        \"gt\": gts,\n",
    "        \"pred_rag\": preds_rag,\n",
    "        \"dec_rag\": [norm_dec(p) for p in preds_rag],\n",
    "        \"ctx_rag\": kg_contexts,\n",
    "    })\n",
    "    res_df.to_csv(out_csv, index=False)\n",
    "    print(\"[Saved]\", out_csv)\n",
    "\n",
    "    # --- Quick Sample Check ---\n",
    "    for k in range(min(3, len(questions))):\n",
    "        print(\"\\n--- sample\", k, \"---\")\n",
    "        print(\"Q :\", questions[k])\n",
    "        print(\"GT:\", gts[k])\n",
    "        print(\"RAG(dec):\", norm_dec(preds_rag[k]))\n",
    "        print(\"RAG out :\", preds_rag[k][:400])\n",
    "        print(\"CTX     :\", (kg_contexts[k] or \"\")[:300])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea02e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38fe7a1d",
   "metadata": {},
   "source": [
    "llama2_pumbmedqakg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c27aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] DEVICE: cuda\n",
      "[Info] Reading test data: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\n",
      "[Info] Loaded samples: 100 | ref_col=long_answer | has_final_decision=True\n",
      "[Info] Loading KB docs: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/pubmed_documents.pkl\n",
      "[Info] KB docs loaded: 800\n",
      "[Info] Found FAISS index: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/pubmed_qa.index\n",
      "[Info] Using FAISS retriever.\n",
      "[Info] Retrieving contexts for all questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 195.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loading generator: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/llama2 with 4-bit Quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efae2c7276c4198a7b7615f1599228c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Model loaded.\n",
      "[Info] Running inference: Base (No Context)\n",
      "  batch 1 done\n",
      "  batch 6 done\n",
      "  batch 11 done\n",
      "  batch 16 done\n",
      "  batch 21 done\n",
      "  batch 26 done\n",
      "  batch 31 done\n",
      "  batch 36 done\n",
      "  batch 41 done\n",
      "  batch 46 done\n",
      "[Info] Running inference: RAG (KB Retrieved Docs)\n",
      "  batch 1 done\n",
      "  batch 6 done\n",
      "  batch 11 done\n",
      "  batch 16 done\n",
      "  batch 21 done\n",
      "  batch 26 done\n",
      "  batch 31 done\n",
      "  batch 36 done\n",
      "  batch 41 done\n",
      "  batch 46 done\n",
      "\n",
      " =\n",
      "Decision ACC (Yes/No/Maybe)\n",
      "Base ACC: 0.4800\n",
      "RAG  ACC: 0.4700\n",
      "Gain   : -0.0100\n",
      " =\n",
      "\n",
      "ROUGE-L:\n",
      "Base: 0.1064 | RAG: 0.1123\n",
      "\n",
      "BERTScore-F1:\n",
      "Base: 0.7582 | RAG: 0.7567\n",
      "\n",
      " =\n",
      "[Done] Saved to eval_results_pubmedqa_kbRAG_test_llama2/llama2_results_20251231_191704.jsonl\n",
      "Summary: {\n",
      "  \"model\": \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/llama2\",\n",
      "  \"decision_acc\": {\n",
      "    \"base\": 0.48,\n",
      "    \"rag\": 0.47\n",
      "  },\n",
      "  \"rougeL\": {\n",
      "    \"base\": 0.10636585174230512,\n",
      "    \"rag\": 0.11232739917531231\n",
      "  },\n",
      "  \"bertscore\": {\n",
      "    \"base\": 0.7582157564163208,\n",
      "    \"rag\": 0.7566591346263886\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# RAG ONLY Evaluation Script for PubMedQA Test Set + KB(pubmed_documents.pkl)\n",
    "# Model: Llama-2-7b-chat (4-bit quantization loading)\n",
    "# Metrics: Decision ACC + ROUGE-L + BERTScore\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig  # Used for 4-bit quantization loading\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# 0) Configuration Area\n",
    "\n",
    "class Config:\n",
    "    # ---- PubMedQA Test Set (HF parquet) ----\n",
    "    parquet_path = r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\"\n",
    "\n",
    "    # ---- KB Documents (Your local \"Knowledge Base\") ----\n",
    "    kb_docs_path = r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/pubmed_documents.pkl\"\n",
    "\n",
    "    # ---- (Optional) FAISS Vector Index ----\n",
    "    kb_index_path = r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/pubmed_qa.index\"\n",
    "    embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "    # ---- Generation Model (Llama-2) ----\n",
    "    model_name_or_path = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/llama2\"\n",
    "\n",
    "    # Llama2 consumes a lot of VRAM, small batch_size (1 or 2) is recommended\n",
    "    batch_size = 2\n",
    "    max_new_tokens = 512\n",
    "\n",
    "    # ---- RAG Retrieval Parameters ----\n",
    "    top_k_docs = 2          # Retrieve top_k KB documents for each question\n",
    "    max_ctx_chars = 1200    # Max characters for concatenated Context (prevents prompt from being too long)\n",
    "\n",
    "    # ---- TF-IDF Fallback Parameters (When FAISS is missing or fails to init) ----\n",
    "    tfidf_max_docs = None\n",
    "    tfidf_max_features = 200000\n",
    "\n",
    "    # ---- Evaluation Parameters ----\n",
    "    limit = 100   # None means run all; integer means run only first N samples\n",
    "    seed = 42\n",
    "\n",
    "    # ---- Output Directory ----\n",
    "    output_dir = \"eval_results_pubmedqa_kbRAG_only_llama2\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "\n",
    "# 1) Utility Functions\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Set fixed random seed for reproducibility.\"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[Info] DEVICE:\", DEVICE)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"Simple accuracy.\"\"\"\n",
    "    if not y_true:\n",
    "        return None\n",
    "    correct = sum(1 for a, b in zip(y_true, y_pred) if a == b)\n",
    "    return correct / len(y_true)\n",
    "\n",
    "def confusion_table(y_true, y_pred, labels=(\"yes\", \"no\", \"maybe\", \"unknown\")):\n",
    "    \"\"\"Confusion matrix (using pandas crosstab).\"\"\"\n",
    "    return pd.crosstab(\n",
    "        pd.Series(y_true, name=\"GT\"),\n",
    "        pd.Series(y_pred, name=\"Pred\"),\n",
    "        rownames=[\"GT\"], colnames=[\"Pred\"],\n",
    "        dropna=False\n",
    "    ).reindex(index=list(labels), columns=list(labels), fill_value=0)\n",
    "\n",
    "# ---- Pure Python implementation of ROUGE-L (F1) ----\n",
    "def _lcs_len(a_tokens, b_tokens):\n",
    "    \"\"\"Calculate LCS length of two token sequences.\"\"\"\n",
    "    n, m = len(a_tokens), len(b_tokens)\n",
    "    dp = [0] * (m + 1)\n",
    "    for i in range(1, n + 1):\n",
    "        prev = 0\n",
    "        for j in range(1, m + 1):\n",
    "            tmp = dp[j]\n",
    "            if a_tokens[i - 1] == b_tokens[j - 1]:\n",
    "                dp[j] = prev + 1\n",
    "            else:\n",
    "                dp[j] = max(dp[j], dp[j - 1])\n",
    "            prev = tmp\n",
    "    return dp[m]\n",
    "\n",
    "def rouge_l_f1(pred: str, ref: str) -> float:\n",
    "    \"\"\"ROUGE-L F1 (LCS-based Precision/Recall/F1).\"\"\"\n",
    "    pred = (pred or \"\").strip()\n",
    "    ref = (ref or \"\").strip()\n",
    "    if not pred or not ref:\n",
    "        return 0.0\n",
    "    pred_tokens = re.findall(r\"\\w+\", pred.lower())\n",
    "    ref_tokens  = re.findall(r\"\\w+\", ref.lower())\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    lcs = _lcs_len(pred_tokens, ref_tokens)\n",
    "    prec = lcs / len(pred_tokens)\n",
    "    rec  = lcs / len(ref_tokens)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "\n",
    "# 2) Read PubMedQA TEST parquet\n",
    "\n",
    "print(f\"[Info] Reading test data: {cfg.parquet_path}\")\n",
    "if not os.path.exists(cfg.parquet_path):\n",
    "    raise FileNotFoundError(f\"Parquet not found: {cfg.parquet_path}\")\n",
    "\n",
    "tbl = pq.read_table(cfg.parquet_path)\n",
    "df = tbl.to_pandas()\n",
    "\n",
    "# Filter out samples with empty questions\n",
    "df = df.dropna(subset=[\"question\"])\n",
    "\n",
    "# Run only first N samples (Optional)\n",
    "if cfg.limit:\n",
    "    df = df.head(cfg.limit)\n",
    "\n",
    "questions = df[\"question\"].astype(str).tolist()\n",
    "\n",
    "# ref_col: Find a column that can serve as \"Reference Answer Text\"\n",
    "# Note: In PubMedQA, long_answer might be long text; final_decision is yes/no/maybe; answer might also exist\n",
    "ref_col = None\n",
    "for c in [\"long_answer\", \"final_decision\", \"answer\"]:\n",
    "    if c in df.columns:\n",
    "        ref_col = c\n",
    "        break\n",
    "if ref_col is None:\n",
    "    # Fallback to the last column if not found\n",
    "    ref_col = df.columns[-1]\n",
    "\n",
    "refs = df[ref_col].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# If final_decision exists, use it for Decision ACC\n",
    "gt_decisions = None\n",
    "if \"final_decision\" in df.columns:\n",
    "    gt_decisions = df[\"final_decision\"].fillna(\"\").astype(str).str.lower().tolist()\n",
    "\n",
    "print(f\"[Info] Loaded samples: {len(questions)} | ref_col={ref_col} | has_final_decision={gt_decisions is not None}\")\n",
    "\n",
    "\n",
    "# 3) Load KB Documents (pubmed_documents.pkl)\n",
    "\n",
    "print(f\"[Info] Loading KB docs: {cfg.kb_docs_path}\")\n",
    "if not os.path.exists(cfg.kb_docs_path):\n",
    "    raise FileNotFoundError(f\"KB docs not found: {cfg.kb_docs_path}\")\n",
    "\n",
    "with open(cfg.kb_docs_path, \"rb\") as f:\n",
    "    kb_docs = pickle.load(f)\n",
    "\n",
    "# If you want to use only the first N documents of the KB (for debugging/speedup)\n",
    "if cfg.tfidf_max_docs is not None:\n",
    "    kb_docs = kb_docs[:cfg.tfidf_max_docs]\n",
    "\n",
    "# Ensure every doc is a string\n",
    "kb_docs = [(\"\" if d is None else str(d)) for d in kb_docs]\n",
    "print(f\"[Info] KB docs loaded: {len(kb_docs)}\")\n",
    "\n",
    "\n",
    "# 4) Build Retriever (Prioritize FAISS, otherwise TF-IDF)\n",
    "\n",
    "use_faiss = False\n",
    "faiss_index = None\n",
    "embed_model = None\n",
    "tfidf_vectorizer = None\n",
    "tfidf_X = None\n",
    "\n",
    "def build_tfidf_retriever(docs):\n",
    "    \"\"\"Build TF-IDF vectorizer and matrix (fallback retrieval).\"\"\"\n",
    "    print(\"[Info] Building TF-IDF retriever (fallback)...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        max_features=cfg.tfidf_max_features,\n",
    "        ngram_range=(1, 2),\n",
    "    )\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    print(\"[Info] TF-IDF ready.\")\n",
    "    return vectorizer, X\n",
    "\n",
    "# Attempt to initialize FAISS\n",
    "try:\n",
    "    if os.path.exists(cfg.kb_index_path):\n",
    "        import faiss\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        print(f\"[Info] Found FAISS index: {cfg.kb_index_path}\")\n",
    "        faiss_index = faiss.read_index(cfg.kb_index_path)\n",
    "        embed_model = SentenceTransformer(cfg.embed_model_name)\n",
    "        use_faiss = True\n",
    "        print(\"[Info] Using FAISS retriever.\")\n",
    "    else:\n",
    "        print(\"[Info] FAISS index not found -> fallback TF-IDF.\")\n",
    "except Exception as e:\n",
    "    print(f\"[Warn] FAISS init failed -> fallback TF-IDF. Error: {e}\")\n",
    "\n",
    "# If not using FAISS, build TF-IDF\n",
    "if not use_faiss:\n",
    "    tfidf_vectorizer, tfidf_X = build_tfidf_retriever(kb_docs)\n",
    "\n",
    "def retrieve_docs(query: str, top_k: int):\n",
    "    \"\"\"Retrieve top_k documents for a query, return list of doc texts.\"\"\"\n",
    "    query = (query or \"\").strip()\n",
    "    if not query:\n",
    "        return []\n",
    "\n",
    "    if use_faiss:\n",
    "        # FAISS: First generate vector with SentenceTransformer, then search with faiss\n",
    "        import faiss\n",
    "        q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q_emb)\n",
    "        scores, idxs = faiss_index.search(q_emb, top_k)\n",
    "        idxs = idxs[0].tolist()\n",
    "        return [kb_docs[i] for i in idxs if 0 <= i < len(kb_docs)]\n",
    "    else:\n",
    "        # TF-IDF: Calculate similarity and take top_k\n",
    "        qv = tfidf_vectorizer.transform([query])\n",
    "        sims = (tfidf_X @ qv.T).toarray().ravel()\n",
    "\n",
    "        if top_k >= len(sims):\n",
    "            top_idx = np.argsort(-sims)\n",
    "        else:\n",
    "            top_idx = np.argpartition(-sims, top_k)[:top_k]\n",
    "            top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "\n",
    "        return [kb_docs[i] for i in top_idx.tolist()]\n",
    "\n",
    "def build_rag_context(question: str):\n",
    "    \"\"\"Retrieve top_k_docs based on question and concatenate into a Context string.\"\"\"\n",
    "    docs = retrieve_docs(question, cfg.top_k_docs)\n",
    "    ctx = \"\\n\\n\".join([d.strip() for d in docs if d and d.strip()])\n",
    "    # Truncate to prevent prompt from being too long\n",
    "    return ctx[:cfg.max_ctx_chars] if ctx else \"\"\n",
    "\n",
    "print(\"[Info] Retrieving contexts for all questions...\")\n",
    "ctx_list_rag = [build_rag_context(q) for q in tqdm(questions)]\n",
    "\n",
    "\n",
    "# 5) Load Generator Model (Llama-2 4-bit)\n",
    "\n",
    "print(f\"[Info] Loading generator: {cfg.model_name_or_path} with 4-bit Quantization...\")\n",
    "\n",
    "# 4-bit quantization config (NF4 + double quant), significantly saves VRAM\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(cfg.model_name_or_path, use_fast=True)\n",
    "\n",
    "# Llama2 usually doesn't have a pad_token, needs manual setting\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Left padding is generally more suitable for generation tasks\n",
    "tok.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "print(\"[Info] Model loaded.\")\n",
    "\n",
    "\n",
    "# 6) Prompt + Inference (RAG Only)\n",
    "\n",
    "def build_prompt(q: str, ctx: str) -> str:\n",
    "    \"\"\"\n",
    "    Manually construct Llama-2 Chat format prompt:\n",
    "    [INST] <<SYS>> ... <</SYS>> user_msg [/INST]\n",
    "    \"\"\"\n",
    "    sys_msg = (\n",
    "        \"You are a helpful biomedical expert. \"\n",
    "        \"Your task is to answer the question with a brief medical reasoning \"\n",
    "        \"followed by a final decision: 'Yes', 'No', or 'Maybe'.\"\n",
    "    )\n",
    "\n",
    "    # Only keep RAG here: Always provide Context\n",
    "    user_msg = (\n",
    "        f\"Context:\\n{ctx}\\n\\n\"\n",
    "        f\"Question: {q}\\n\\n\"\n",
    "        \"Based on the context above, provide reasoning and then conclude with \"\n",
    "        \"'Answer: Yes', 'Answer: No', or 'Answer: Maybe'.\"\n",
    "    )\n",
    "\n",
    "    return f\"[INST] <<SYS>>\\n{sys_msg}\\n<</SYS>>\\n\\n{user_msg} [/INST]\"\n",
    "\n",
    "def run_inference_rag(qs, ctxs):\n",
    "    \"\"\"Batch Inference (RAG only).\"\"\"\n",
    "    print(\"[Info] Running inference: RAG (KB Retrieved Docs)\")\n",
    "    preds = []\n",
    "    prompts = [build_prompt(q, c) for q, c in zip(qs, ctxs)]\n",
    "\n",
    "    for i in range(0, len(prompts), cfg.batch_size):\n",
    "        batch_prompts = prompts[i:i + cfg.batch_size]\n",
    "\n",
    "        # tokenize + padding/truncation\n",
    "        inputs = tok(\n",
    "            batch_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=4096  # Llama-2 usually supports 4k\n",
    "        ).to(\"cuda\")  # Note: Defaulting to cuda here\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                pad_token_id=tok.pad_token_id,\n",
    "                do_sample=False,     # Greedy decoding to ensure determinism\n",
    "                temperature=0.0\n",
    "            )\n",
    "\n",
    "        # Only take the newly generated part\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        decoded = tok.batch_decode(out[:, input_len:], skip_special_tokens=True)\n",
    "        preds.extend([d.strip() for d in decoded])\n",
    "\n",
    "        if (i // cfg.batch_size) % 5 == 0:\n",
    "            print(f\"  batch {i // cfg.batch_size + 1} done\")\n",
    "\n",
    "    return preds\n",
    "\n",
    "preds_rag = run_inference_rag(questions, ctx_list_rag)\n",
    "\n",
    "\n",
    "# 7) Extract yes/no/maybe decision from output\n",
    "\n",
    "def extract_decision_last(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Prioritize matching the last occurrence of 'Answer: Yes/No/Maybe';\n",
    "    Otherwise degrade to matching the last occurrence of any yes/no/maybe word.\n",
    "    \"\"\"\n",
    "    matches = re.findall(r\"Answer:\\s*(Yes|No|Maybe)\", text, re.IGNORECASE)\n",
    "    if matches:\n",
    "        return matches[-1].lower()\n",
    "\n",
    "    matches = re.findall(r\"\\b(yes|no|maybe)\\b\", (text or \"\").lower())\n",
    "    return matches[-1] if matches else \"unknown\"\n",
    "\n",
    "pred_dec_rag = [extract_decision_last(p) for p in preds_rag]\n",
    "\n",
    "\n",
    "# 8) Metric Calculation\n",
    "\n",
    "acc_rag = None\n",
    "conf_rag = None\n",
    "\n",
    "# Decision ACC: Requires final_decision column\n",
    "if gt_decisions is not None:\n",
    "    valid_idx = [i for i, g in enumerate(gt_decisions) if g in (\"yes\", \"no\", \"maybe\")]\n",
    "    if valid_idx:\n",
    "        y_true = [gt_decisions[i] for i in valid_idx]\n",
    "        yr = [pred_dec_rag[i] for i in valid_idx]\n",
    "        acc_rag  = accuracy(y_true, yr)\n",
    "        conf_rag = confusion_table(y_true, yr, labels=(\"yes\", \"no\", \"maybe\", \"unknown\"))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Decision ACC (Yes/No/Maybe) - RAG ONLY\")\n",
    "if acc_rag is None:\n",
    "    print(\"[Warn] final_decision not found/valid -> ACC skipped.\")\n",
    "else:\n",
    "    print(f\"RAG ACC: {acc_rag:.4f}\")\n",
    "    print(\"\\nConfusion (RAG):\")\n",
    "    print(conf_rag)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ROUGE-L: Compare generated text vs refs\n",
    "rougeL_rag_list = [rouge_l_f1(p, r) for p, r in zip(preds_rag, refs)]\n",
    "rougeL_rag = float(np.mean(rougeL_rag_list))\n",
    "print(\"\\nROUGE-L (RAG):\")\n",
    "print(f\"RAG: {rougeL_rag:.4f}\")\n",
    "\n",
    "# BERTScore: May require downloading extra model/dependencies; skip if failed\n",
    "bs_rag = None\n",
    "try:\n",
    "    import evaluate\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bs_res_rag = bertscore.compute(\n",
    "        predictions=preds_rag,\n",
    "        references=refs,\n",
    "        lang=\"en\",\n",
    "        model_type=\"distilbert-base-uncased\"\n",
    "    )\n",
    "    bs_rag = float(np.mean(bs_res_rag[\"f1\"]))\n",
    "    print(\"\\nBERTScore-F1 (RAG):\")\n",
    "    print(f\"RAG: {bs_rag:.4f}\")\n",
    "except Exception as e:\n",
    "    print(\"\\n[Warn] BERTScore skipped:\", str(e)[:100])\n",
    "\n",
    "\n",
    "# 9) Save Results\n",
    "summary = {\n",
    "    \"model\": cfg.model_name_or_path,\n",
    "    \"decision_acc\": {\"rag\": acc_rag},\n",
    "    \"rougeL\": {\"rag\": rougeL_rag},\n",
    "    \"bertscore\": {\"rag\": bs_rag}\n",
    "}\n",
    "\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "jsonl_path = os.path.join(cfg.output_dir, f\"llama2_rag_only_{time_str}.jsonl\")\n",
    "csv_path   = os.path.join(cfg.output_dir, f\"llama2_rag_only_{time_str}.csv\")\n",
    "\n",
    "rows = []\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(questions)):\n",
    "        row = {\n",
    "            \"id\": i,\n",
    "            \"question\": questions[i],\n",
    "            \"reference\": refs[i],\n",
    "            \"gt_decision\": None if gt_decisions is None else gt_decisions[i],\n",
    "            \"rag_pred\": preds_rag[i],\n",
    "            \"rag_decision\": pred_dec_rag[i],\n",
    "            \"rag_context\": ctx_list_rag[i],  # Save the fully retrieved context\n",
    "        }\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "        rows.append(row)\n",
    "\n",
    "pd.DataFrame(rows).to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"[Done] Saved to {jsonl_path}\")\n",
    "print(f\"Summary: {json.dumps(summary, indent=2, ensure_ascii=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e86463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Metrics\n",
    "\n",
    "# --- Decision ACC ---\n",
    "acc_rag = None\n",
    "conf_rag = None\n",
    "\n",
    "if gt_decisions is not None:\n",
    "    valid_idx = [i for i, g in enumerate(gt_decisions) if g in (\"yes\", \"no\", \"maybe\")]\n",
    "    if valid_idx:\n",
    "        y_true = [gt_decisions[i] for i in valid_idx]\n",
    "        yr = [pred_dec_rag[i] for i in valid_idx]\n",
    "\n",
    "        acc_rag  = accuracy(y_true, yr)\n",
    "        conf_rag = confusion_table(y_true, yr, labels=(\"yes\", \"no\", \"maybe\", \"unknown\"))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Decision ACC (Yes/No/Maybe)\")\n",
    "if acc_rag is None:\n",
    "    print(\"[Warn] final_decision not found/valid -> ACC skipped.\")\n",
    "else:\n",
    "    print(f\"RAG  ACC: {acc_rag:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- ROUGE-L (Per sample list) ---\n",
    "rougeL_rag_list  = [rouge_l_f1(p, r) for p, r in zip(preds_rag, refs)]\n",
    "rougeL_rag  = float(np.mean(rougeL_rag_list))\n",
    "\n",
    "print(\"\\nROUGE-L:\")\n",
    "print(f\"RAG: {rougeL_rag:.4f}\")\n",
    "\n",
    "# --- BERTScore (Per sample list) ---\n",
    "bs_rag = None\n",
    "# Initialize per-item score lists to prevent errors if BERTScore fails\n",
    "bs_list_rag  = [None] * len(questions)\n",
    "\n",
    "try:\n",
    "    import evaluate\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    \n",
    "    # RAG\n",
    "    bs_res_rag  = bertscore.compute(predictions=preds_rag,  references=refs, lang=\"en\", model_type=\"distilbert-base-uncased\")\n",
    "    bs_list_rag = bs_res_rag[\"f1\"]   # Get F1 for each sample\n",
    "    bs_rag  = float(np.mean(bs_list_rag))\n",
    "    \n",
    "    print(\"\\nBERTScore-F1:\")\n",
    "    print(f\"RAG: {bs_rag:.4f}\")\n",
    "except Exception as e:\n",
    "    print(\"\\n[Warn] BERTScore skipped:\", str(e)[:100])\n",
    "\n",
    "\n",
    "\n",
    "# 8) Save results (Include Per-Sample Scores)\n",
    "summary = {\n",
    "    \"model\": cfg.model_name_or_path,\n",
    "    \"decision_acc\": {\"rag\": acc_rag},\n",
    "    \"rougeL\": {\"rag\": rougeL_rag},\n",
    "    \"bertscore\": {\"rag\": bs_rag}\n",
    "}\n",
    "\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "jsonl_path = os.path.join(cfg.output_dir, f\"llama2_results_{time_str}.jsonl\")\n",
    "csv_path   = os.path.join(cfg.output_dir, f\"llama2_results_{time_str}.csv\")\n",
    "\n",
    "rows = []\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(questions)):\n",
    "        row = {\n",
    "            \"id\": i,\n",
    "            \"question\": questions[i],\n",
    "            \"reference\": refs[i],\n",
    "            \"gt_decision\": None if gt_decisions is None else gt_decisions[i],\n",
    "            \n",
    "            # --- Predictions ---\n",
    "            \"rag_pred\": preds_rag[i],\n",
    "            \"rag_decision\": pred_dec_rag[i],\n",
    "            \n",
    "            # --- Scores (New additions) ---\n",
    "            \"score_rougeL_rag\":  rougeL_rag_list[i],\n",
    "            \"score_bertscore_rag\":  bs_list_rag[i],\n",
    "            \n",
    "            #Context\n",
    "            \"rag_context\": ctx_list_rag[i], \n",
    "        }\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "        rows.append(row)\n",
    "\n",
    "pd.DataFrame(rows).to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"[Done] Saved to {jsonl_path}\")\n",
    "print(f\"Summary: {json.dumps(summary, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81c11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
