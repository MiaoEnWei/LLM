{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a100c64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) 环境准备\n",
    "# =========================\n",
    "# 建议环境：\n",
    "# pip install -U torch transformers accelerate faiss-cpu pandas numpy scikit-learn tqdm nbformat\n",
    "\n",
    "import os, json, math, time, pickle, random, re\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "def set_seed(seed: int = 2025):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(2025)\n",
    "print(\"OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a3d01bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_DIR = /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/eval_out/medmcqa_rag_notebook_20251219_131920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 1) 配置区\n",
    "# =========================\n",
    "DEV_FILE = \"./data/medmcqa/dev.json\"         # <-- 改这里\n",
    "KB_DIR   = \"./rag_cache/medmcqa_train_50k\"   # <-- 改这里\n",
    "\n",
    "# 用于 A/B/C/D 选择打分的 LM\n",
    "MODEL_NAME_OR_PATH = \"./gpt2\"               # <-- 需要时改\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 用于检索 embedding 的 encoder（mean pooling）\n",
    "EMBED_MODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "# 默认参数（后面会做网格搜索）\n",
    "DEFAULT_RAG_K = 3\n",
    "DEFAULT_CTX_MAX_CHARS = 500\n",
    "DEFAULT_EVID_MAX_SENTS = 4\n",
    "DEFAULT_SIM_THRESHOLD = None   # None=不做阈值回退（建议先跑通，再用网格找阈值）\n",
    "\n",
    "RUN_DIR = Path(\"./eval_out\") / f\"medmcqa_rag_notebook_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"RUN_DIR =\", RUN_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad397187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded samples: 4183\n",
      "Example question: Which of the following is not true for myelinated nerve fibers:\n",
      "Options: {'A': 'Impulse through myelinated fibers is slower than non-myelinated fibers', 'B': 'Membrane currents are generated at nodes of Ranvier', 'C': 'Saltatory conduction of impulses is seen', 'D': 'Local anesthesia is effective only when the nerve is not covered by myelin sheath'}\n",
      "Gold: A\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) 数据加载（兼容字段）\n",
    "# =========================\n",
    "def _get_first(d: dict, keys: List[str], default=None):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None:\n",
    "            return d[k]\n",
    "    return default\n",
    "\n",
    "def normalize_gold(g):\n",
    "    # 统一成 int 或 A/B/C/D\n",
    "    if isinstance(g, str):\n",
    "        gg = g.strip()\n",
    "        if gg in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "            return gg\n",
    "        if gg.isdigit():\n",
    "            g = int(gg)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if isinstance(g, (int, np.integer)):\n",
    "        g = int(g)\n",
    "\n",
    "        # 自动判断：更像 1/2/3/4 还是 0/1/2/3\n",
    "        if g in (1, 2, 3, 4):\n",
    "            return \"ABCD\"[g - 1]   # 1-based\n",
    "        if g in (0, 1, 2, 3):\n",
    "            return \"ABCD\"[g]       # 0-based\n",
    "\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_medmcqa(path: str) -> List[dict]:\n",
    "    path = Path(path)\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "\n",
    "    # 1) 先尝试标准 JSON（list/dict）\n",
    "    data = None\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if isinstance(data, dict) and \"data\" in data and isinstance(data[\"data\"], list):\n",
    "            data = data[\"data\"]\n",
    "        if isinstance(data, dict) and \"questions\" in data and isinstance(data[\"questions\"], list):\n",
    "            data = data[\"questions\"]\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"Top-level JSON is not a list\")\n",
    "    except Exception:\n",
    "        # 2) 走 JSONL（每行一个 JSON 对象）\n",
    "        data = []\n",
    "        with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for line_no, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    data.append(obj)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    raise ValueError(\n",
    "                        f\"JSONL parse failed at line {line_no}: {e}\\n\"\n",
    "                        f\"Line content (first 200 chars): {line[:200]}\"\n",
    "                    )\n",
    "\n",
    "    # ====== 下面保持你原来的字段兼容逻辑 ======\n",
    "    samples = []\n",
    "    for i, ex in enumerate(data):\n",
    "        q = _get_first(ex, [\"question\", \"ques\", \"query\", \"prompt\"])\n",
    "        if q is None:\n",
    "            continue\n",
    "\n",
    "        opa = _get_first(ex, [\"opa\",\"option_a\",\"A\",\"a\"])\n",
    "        opb = _get_first(ex, [\"opb\",\"option_b\",\"B\",\"b\"])\n",
    "        opc = _get_first(ex, [\"opc\",\"option_c\",\"C\",\"c\"])\n",
    "        opd = _get_first(ex, [\"opd\",\"option_d\",\"D\",\"d\"])\n",
    "\n",
    "        if any(x is None for x in [opa,opb,opc,opd]) and \"options\" in ex:\n",
    "            opts = ex[\"options\"]\n",
    "            if isinstance(opts, dict):\n",
    "                opa = opa or opts.get(\"A\") or opts.get(\"a\")\n",
    "                opb = opb or opts.get(\"B\") or opts.get(\"b\")\n",
    "                opc = opc or opts.get(\"C\") or opts.get(\"c\")\n",
    "                opd = opd or opts.get(\"D\") or opts.get(\"d\")\n",
    "            elif isinstance(opts, list) and len(opts) >= 4:\n",
    "                opa, opb, opc, opd = opa or opts[0], opb or opts[1], opc or opts[2], opd or opts[3]\n",
    "\n",
    "        gold_raw = _get_first(ex, [\"cop\",\"gold\",\"answer\",\"label\",\"correct_option\",\"correct\"])\n",
    "        gold = normalize_gold(gold_raw)\n",
    "\n",
    "        samples.append({\n",
    "            \"id\": ex.get(\"id\", i),\n",
    "            \"question\": str(q).strip(),\n",
    "            \"options\": {\n",
    "                \"A\": \"\" if opa is None else str(opa).strip(),\n",
    "                \"B\": \"\" if opb is None else str(opb).strip(),\n",
    "                \"C\": \"\" if opc is None else str(opc).strip(),\n",
    "                \"D\": \"\" if opd is None else str(opd).strip(),\n",
    "            },\n",
    "            \"gold\": gold,\n",
    "            \"raw\": ex\n",
    "        })\n",
    "    return samples\n",
    "\n",
    "samples = load_medmcqa(DEV_FILE)\n",
    "print(\"Loaded samples:\", len(samples))\n",
    "print(\"Example question:\", samples[0][\"question\"][:120])\n",
    "print(\"Options:\", samples[0][\"options\"])\n",
    "print(\"Gold:\", samples[0][\"gold\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49532b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw cop top: [(1, 1348), (2, 1085), (3, 925), (4, 825)]\n",
      "normalized gold top: [('A', 1348), ('B', 1085), ('C', 925), ('D', 825)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"raw cop top:\", Counter([s[\"raw\"].get(\"cop\") for s in samples]).most_common(10))\n",
    "print(\"normalized gold top:\", Counter([s[\"gold\"] for s in samples]).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8263196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found index: rag_cache/medmcqa_train_50k/index.faiss\n",
      "Found docs : rag_cache/medmcqa_train_50k/docs.jsonl\n",
      "Docs: 50000 | Example: Q: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchym\n",
      "FAISS index ntotal: 50000\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3) KB 加载（自动发现 index & docs）\n",
    "# =========================\n",
    "def find_first_file(d: Path, exts: Tuple[str, ...]) -> Optional[Path]:\n",
    "    for p in sorted(d.rglob(\"*\")):\n",
    "        if p.is_file() and p.suffix.lower() in exts:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def load_docs_any(path: Path) -> List[str]:\n",
    "    if path.suffix.lower() in [\".pkl\", \".pickle\"]:\n",
    "        obj = pickle.loads(path.read_bytes())\n",
    "        if isinstance(obj, list):\n",
    "            return [str(x) for x in obj]\n",
    "        if isinstance(obj, dict):\n",
    "            for k in [\"documents\", \"docs\", \"texts\", \"corpus\"]:\n",
    "                if k in obj and isinstance(obj[k], list):\n",
    "                    return [str(x) for x in obj[k]]\n",
    "            if all(isinstance(k,(int,np.integer,str)) for k in obj.keys()):\n",
    "                items = sorted(obj.items(), key=lambda kv: int(kv[0]) if str(kv[0]).isdigit() else str(kv[0]))\n",
    "                return [str(v) for _, v in items]\n",
    "        raise ValueError(f\"Unknown pkl format: {type(obj)}\")\n",
    "\n",
    "    if path.suffix.lower() == \".jsonl\":\n",
    "        docs = []\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                j = json.loads(line)\n",
    "                txt = j.get(\"text\") or j.get(\"content\") or j.get(\"document\") or j.get(\"doc\") or j.get(\"passage\")\n",
    "                if txt is None:\n",
    "                    txt = json.dumps(j, ensure_ascii=False)\n",
    "                docs.append(str(txt))\n",
    "        return docs\n",
    "\n",
    "    if path.suffix.lower() == \".json\":\n",
    "        j = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(j, list):\n",
    "            if all(isinstance(x, str) for x in j):\n",
    "                return j\n",
    "            docs=[]\n",
    "            for x in j:\n",
    "                if isinstance(x, dict):\n",
    "                    txt = x.get(\"text\") or x.get(\"content\") or x.get(\"document\") or x.get(\"doc\") or x.get(\"passage\")\n",
    "                    docs.append(str(txt) if txt is not None else json.dumps(x, ensure_ascii=False))\n",
    "                else:\n",
    "                    docs.append(str(x))\n",
    "            return docs\n",
    "        if isinstance(j, dict):\n",
    "            for k in [\"documents\",\"docs\",\"texts\",\"corpus\"]:\n",
    "                if k in j and isinstance(j[k], list):\n",
    "                    return [str(x) for x in j[k]]\n",
    "        raise ValueError(\"Unknown json format\")\n",
    "\n",
    "    raise ValueError(\"Unsupported docs file type: \" + str(path))\n",
    "\n",
    "def load_faiss_index_any(path: Path):\n",
    "    import faiss\n",
    "    return faiss.read_index(str(path))\n",
    "\n",
    "KB_DIR_PATH = Path(KB_DIR)\n",
    "assert KB_DIR_PATH.exists(), f\"KB_DIR not found: {KB_DIR_PATH}\"\n",
    "\n",
    "idx_file = find_first_file(KB_DIR_PATH, (\".faiss\",\".index\"))\n",
    "docs_file = find_first_file(KB_DIR_PATH, (\".pkl\",\".pickle\",\".jsonl\",\".json\"))\n",
    "\n",
    "print(\"Found index:\", idx_file)\n",
    "print(\"Found docs :\", docs_file)\n",
    "assert idx_file is not None, \"未找到 FAISS index 文件（.faiss/.index）\"\n",
    "assert docs_file is not None, \"未找到 documents 文件（.pkl/.jsonl/.json）\"\n",
    "\n",
    "docs = load_docs_any(docs_file)\n",
    "print(\"Docs:\", len(docs), \"| Example:\", docs[0][:120].replace(\"\\n\",\" \"))\n",
    "\n",
    "index = load_faiss_index_any(idx_file)\n",
    "print(\"FAISS index ntotal:\", index.ntotal)\n",
    "assert index.ntotal == len(docs), \"index.ntotal 与 docs 数量不一致：请检查 KB 对齐\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b306b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedder ready: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4) Embedding 模型\n",
    "# =========================\n",
    "@dataclass\n",
    "class EmbedderConfig:\n",
    "    model_name: str\n",
    "    device: str = DEVICE\n",
    "    max_length: int = 256\n",
    "    batch_size: int = 32\n",
    "    fp16: bool = True\n",
    "\n",
    "class MeanPoolEmbedder:\n",
    "    def __init__(self, cfg: EmbedderConfig):\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
    "        self.model = AutoModel.from_pretrained(cfg.model_name)\n",
    "        self.model.eval()\n",
    "        self.model.to(cfg.device)\n",
    "        if cfg.fp16 and cfg.device.startswith(\"cuda\"):\n",
    "            self.model.half()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        all_vecs = []\n",
    "        bs = self.cfg.batch_size\n",
    "        for i in range(0, len(texts), bs):\n",
    "            batch = texts[i:i+bs]\n",
    "            tok = self.tokenizer(batch, padding=True, truncation=True,\n",
    "                                 max_length=self.cfg.max_length, return_tensors=\"pt\")\n",
    "            tok = {k: v.to(self.cfg.device) for k,v in tok.items()}\n",
    "            out = self.model(**tok)\n",
    "            last = out.last_hidden_state  # [B,T,H]\n",
    "            mask = tok[\"attention_mask\"].unsqueeze(-1).to(last.dtype)\n",
    "            summed = (last * mask).sum(dim=1)\n",
    "            denom = mask.sum(dim=1).clamp(min=1e-6)\n",
    "            mean = summed / denom\n",
    "            mean = torch.nn.functional.normalize(mean, p=2, dim=1)\n",
    "            all_vecs.append(mean.float().cpu().numpy())\n",
    "        return np.vstack(all_vecs)\n",
    "\n",
    "embedder = MeanPoolEmbedder(EmbedderConfig(model_name=EMBED_MODEL_NAME))\n",
    "print(\"Embedder ready:\", EMBED_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42927bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ids: [27773, 45434, 40482]\n",
      "Top scores: [0.9015, 0.9006, 0.8966]\n",
      "Evidence preview: Which of the following positions serve as most comfoable for a patient facing difficulty in bre A: All positions are same in respect to comfo. Q: Which of the following statements is true regarding ka\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Retrieval & Evidence extraction\n",
    "# =========================\n",
    "_SENT_SPLIT = re.compile(r\"(?<=[\\.!\\?])\\s+|(?<=[。！？])\")\n",
    "\n",
    "def split_sents(text: str) -> List[str]:\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = _SENT_SPLIT.split(text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def retrieve_topk(query: str, k: int = 3) -> Tuple[List[int], List[float]]:\n",
    "    qv = embedder.encode([query]).astype(np.float32)\n",
    "    scores, ids = index.search(qv, k)\n",
    "    return ids[0].tolist(), scores[0].tolist()\n",
    "\n",
    "def extract_evidence(query: str, doc_ids: List[int], max_sents: int = 4) -> str:\n",
    "    sents = []\n",
    "    for did in doc_ids:\n",
    "        for s in split_sents(docs[did]):\n",
    "            sents.append(s)\n",
    "    if not sents:\n",
    "        return \"\"\n",
    "    qv = embedder.encode([query]).astype(np.float32)[0]\n",
    "    sv = embedder.encode(sents).astype(np.float32)\n",
    "    sims = (sv @ qv.reshape(-1,1)).reshape(-1)  # cosine if normalized\n",
    "    top_idx = np.argsort(-sims)[:max_sents]\n",
    "    chosen = [sents[i] for i in top_idx]\n",
    "    return \" \".join(chosen)\n",
    "\n",
    "# sanity check\n",
    "ids, sc = retrieve_topk(samples[0][\"question\"], k=3)\n",
    "print(\"Top ids:\", ids)\n",
    "print(\"Top scores:\", [round(x,4) for x in sc])\n",
    "print(\"Evidence preview:\", extract_evidence(samples[0][\"question\"], ids, max_sents=4)[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98eb6021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice token id: {'A': 32, 'B': 33, 'C': 34, 'D': 35}\n",
      "LM ready: ./gpt2\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6) LM scorer (next-token logits for A/B/C/D)\n",
    "# =========================\n",
    "@dataclass\n",
    "class LMConfig:\n",
    "    model_name_or_path: str\n",
    "    device: str = DEVICE\n",
    "    max_prompt_tokens: int = 512\n",
    "    fp16: bool = True\n",
    "\n",
    "class ChoiceScorer:\n",
    "    def __init__(self, cfg: LMConfig):\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model_name_or_path, use_fast=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(cfg.model_name_or_path)\n",
    "        self.model.eval()\n",
    "        self.model.to(cfg.device)\n",
    "        if cfg.fp16 and cfg.device.startswith(\"cuda\"):\n",
    "            self.model.half()\n",
    "\n",
    "        self.choice_token_id = {}\n",
    "        for ch in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "            ids = self.tokenizer.encode(ch, add_special_tokens=False)\n",
    "            self.choice_token_id[ch] = ids[0]\n",
    "        print(\"Choice token id:\", self.choice_token_id)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def score_choices_next_token(self, prompt: str) -> Dict[str, float]:\n",
    "        tok = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=self.cfg.max_prompt_tokens)\n",
    "        tok = {k: v.to(self.cfg.device) for k,v in tok.items()}\n",
    "        out = self.model(**tok)\n",
    "        logits = out.logits[:, -1, :]  # [1,vocab]\n",
    "        scores = {}\n",
    "        for ch, tid in self.choice_token_id.items():\n",
    "            scores[ch] = float(logits[0, tid].detach().cpu())\n",
    "        return scores\n",
    "\n",
    "scorer = ChoiceScorer(LMConfig(model_name_or_path=MODEL_NAME_OR_PATH))\n",
    "print(\"LM ready:\", MODEL_NAME_OR_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ced3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7) Prompt builders (no triple-quote nesting)\n",
    "# =========================\n",
    "def build_prompt_base(q: str, options: Dict[str,str]) -> str:\n",
    "    return (\n",
    "        \"You are answering a multiple-choice medical question.\\n\"\n",
    "        f\"Question: {q}\\n\"\n",
    "        f\"A) {options['A']}\\n\"\n",
    "        f\"B) {options['B']}\\n\"\n",
    "        f\"C) {options['C']}\\n\"\n",
    "        f\"D) {options['D']}\\n\"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "\n",
    "def build_prompt_rag_question(q: str, options: Dict[str,str], evidence: str) -> str:\n",
    "    ev = evidence.strip()\n",
    "    ev_block = (f\"Evidence: {ev}\\n\" if ev else \"\")\n",
    "    return (\n",
    "        \"You are answering a multiple-choice medical question using evidence.\\n\"\n",
    "        + ev_block +\n",
    "        f\"Question: {q}\\n\"\n",
    "        f\"A) {options['A']}\\n\"\n",
    "        f\"B) {options['B']}\\n\"\n",
    "        f\"C) {options['C']}\\n\"\n",
    "        f\"D) {options['D']}\\n\"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "\n",
    "def build_prompt_rag_option(q: str, options: Dict[str,str], ev_by_opt: Dict[str,str]) -> str:\n",
    "    lines = [\n",
    "        \"You are answering a multiple-choice medical question using per-option evidence.\",\n",
    "        f\"Question: {q}\"\n",
    "    ]\n",
    "    for ch in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "        lines.append(f\"{ch}) {options[ch]}\")\n",
    "        ev = (ev_by_opt.get(ch, \"\") or \"\").strip()\n",
    "        if ev:\n",
    "            lines.append(f\"Evidence for {ch}: {ev}\")\n",
    "    lines.append(\"Answer: \")\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e847d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8) Predict & evaluate\n",
    "# =========================\n",
    "LABELS = [\"A\",\"B\",\"C\",\"D\"]\n",
    "\n",
    "def predict_one(sample: dict, mode: str,\n",
    "                rag_k: int = DEFAULT_RAG_K,\n",
    "                ctx_max_chars: int = DEFAULT_CTX_MAX_CHARS,\n",
    "                evid_max_sents: int = DEFAULT_EVID_MAX_SENTS,\n",
    "                sim_threshold: Optional[float] = DEFAULT_SIM_THRESHOLD) -> dict:\n",
    "    q = sample[\"question\"]\n",
    "    opts = sample[\"options\"]\n",
    "\n",
    "    record = {\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"gold\": sample[\"gold\"],\n",
    "        \"pred\": None,\n",
    "        \"mode\": mode,\n",
    "        \"rag_k\": rag_k,\n",
    "        \"ctx_max_chars\": ctx_max_chars,\n",
    "        \"evid_max_sents\": evid_max_sents,\n",
    "        \"sim_threshold\": sim_threshold,\n",
    "        \"scores\": None,\n",
    "        \"rag_context\": None,\n",
    "        \"question\": q,\n",
    "        \"options\": opts,\n",
    "    }\n",
    "\n",
    "    if mode == \"base\":\n",
    "        prompt = build_prompt_base(q, opts)\n",
    "        scores = scorer.score_choices_next_token(prompt)\n",
    "        pred = max(scores, key=scores.get)\n",
    "        record.update({\"scores\": scores, \"pred\": pred, \"rag_context\": \"\"})\n",
    "        return record\n",
    "\n",
    "    if mode == \"rag_q\":\n",
    "        ids, scores0 = retrieve_topk(q, k=rag_k)\n",
    "        if sim_threshold is not None and scores0 and scores0[0] < sim_threshold:\n",
    "            evidence = \"\"\n",
    "        else:\n",
    "            evidence = extract_evidence(q, ids, max_sents=evid_max_sents)[:ctx_max_chars]\n",
    "        prompt = build_prompt_rag_question(q, opts, evidence)\n",
    "        scores = scorer.score_choices_next_token(prompt)\n",
    "        pred = max(scores, key=scores.get)\n",
    "        record.update({\"scores\": scores, \"pred\": pred, \"rag_context\": evidence})\n",
    "        return record\n",
    "\n",
    "    if mode == \"rag_opt\":\n",
    "        ev_by_opt = {}\n",
    "        per_opt_chars = max(50, ctx_max_chars // 2)\n",
    "        for ch in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "            query = q + \" \" + opts[ch]\n",
    "            ids, scores0 = retrieve_topk(query, k=rag_k)\n",
    "            if sim_threshold is not None and scores0 and scores0[0] < sim_threshold:\n",
    "                ev = \"\"\n",
    "            else:\n",
    "                ev = extract_evidence(query, ids, max_sents=evid_max_sents)[:per_opt_chars]\n",
    "            ev_by_opt[ch] = ev\n",
    "\n",
    "        prompt = build_prompt_rag_option(q, opts, ev_by_opt)\n",
    "        scores = scorer.score_choices_next_token(prompt)\n",
    "        pred = max(scores, key=scores.get)\n",
    "        record.update({\"scores\": scores, \"pred\": pred, \"rag_context\": ev_by_opt})\n",
    "        return record\n",
    "\n",
    "    raise ValueError(\"Unknown mode: \" + mode)\n",
    "\n",
    "def eval_mode(samples: List[dict], mode: str, **kwargs) -> List[dict]:\n",
    "    out = []\n",
    "    for s in tqdm(samples, desc=f\"eval:{mode}\"):\n",
    "        out.append(predict_one(s, mode=mode, **kwargs))\n",
    "    return out\n",
    "\n",
    "def save_jsonl(records: List[dict], path: Path):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "253e111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 9) Metrics\n",
    "# =========================\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def summarize(records: List[dict]) -> dict:\n",
    "    gold = [r[\"gold\"] for r in records]\n",
    "    pred = [r[\"pred\"] for r in records]\n",
    "    keep = [i for i,(g,p) in enumerate(zip(gold,pred)) if g in LABELS and p in LABELS]\n",
    "    gold = [gold[i] for i in keep]\n",
    "    pred = [pred[i] for i in keep]\n",
    "\n",
    "    acc = accuracy_score(gold, pred)\n",
    "    cm = confusion_matrix(gold, pred, labels=LABELS)\n",
    "\n",
    "    prec, rec, f1, sup = precision_recall_fscore_support(gold, pred, labels=LABELS, zero_division=0)\n",
    "    per_class = []\n",
    "    for i, lab in enumerate(LABELS):\n",
    "        per_class.append({\n",
    "            \"class\": lab,\n",
    "            \"precision\": float(prec[i]),\n",
    "            \"recall\": float(rec[i]),\n",
    "            \"f1\": float(f1[i]),\n",
    "            \"support\": int(sup[i]),\n",
    "            \"correct\": int(cm[i,i]),\n",
    "        })\n",
    "\n",
    "    macro = {\"precision\": float(np.mean(prec)), \"recall\": float(np.mean(rec)), \"f1\": float(np.mean(f1))}\n",
    "    weighted = {\n",
    "        \"precision\": float(np.average(prec, weights=sup)),\n",
    "        \"recall\": float(np.average(rec, weights=sup)),\n",
    "        \"f1\": float(np.average(f1, weights=sup)),\n",
    "    }\n",
    "    dist_pred = dict(pd.Series(pred).value_counts().reindex(LABELS, fill_value=0))\n",
    "    dist_gold = dict(pd.Series(gold).value_counts().reindex(LABELS, fill_value=0))\n",
    "\n",
    "    return {\n",
    "        \"evaluated\": len(gold),\n",
    "        \"acc\": float(acc),\n",
    "        \"dist_pred\": {k:int(v) for k,v in dist_pred.items()},\n",
    "        \"dist_gold\": {k:int(v) for k,v in dist_gold.items()},\n",
    "        \"per_class\": per_class,\n",
    "        \"macro\": macro,\n",
    "        \"weighted\": weighted,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "    }\n",
    "\n",
    "def fix_hurt(base_records: List[dict], rag_records: List[dict]) -> dict:\n",
    "    bmap = {r[\"id\"]: r for r in base_records}\n",
    "    rmap = {r[\"id\"]: r for r in rag_records}\n",
    "    ids = sorted(set(bmap.keys()) & set(rmap.keys()), key=lambda x: str(x))\n",
    "\n",
    "    fix = hurt = same_correct = same_wrong = 0\n",
    "    for i in ids:\n",
    "        b = bmap[i]; r = rmap[i]\n",
    "        g = b[\"gold\"]\n",
    "        if g not in LABELS:\n",
    "            continue\n",
    "        bc = (b[\"pred\"] == g)\n",
    "        rc = (r[\"pred\"] == g)\n",
    "        if (not bc) and rc:\n",
    "            fix += 1\n",
    "        elif bc and (not rc):\n",
    "            hurt += 1\n",
    "        elif bc and rc:\n",
    "            same_correct += 1\n",
    "        else:\n",
    "            same_wrong += 1\n",
    "    return {\"fix\": fix, \"hurt\": hurt, \"same_correct\": same_correct, \"same_wrong\": same_wrong, \"overlap\": len(ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2dbd1aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:base:  44%|████▎     | 1823/4183 [00:13<00:16, 139.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 10) Run one comparison\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m base_records \u001b[38;5;241m=\u001b[39m \u001b[43meval_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m ragq_records \u001b[38;5;241m=\u001b[39m eval_mode(samples, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_q\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                          rag_k\u001b[38;5;241m=\u001b[39mDEFAULT_RAG_K,\n\u001b[1;32m      7\u001b[0m                          ctx_max_chars\u001b[38;5;241m=\u001b[39mDEFAULT_CTX_MAX_CHARS,\n\u001b[1;32m      8\u001b[0m                          evid_max_sents\u001b[38;5;241m=\u001b[39mDEFAULT_EVID_MAX_SENTS,\n\u001b[1;32m      9\u001b[0m                          sim_threshold\u001b[38;5;241m=\u001b[39mDEFAULT_SIM_THRESHOLD)\n\u001b[1;32m     10\u001b[0m rago_records \u001b[38;5;241m=\u001b[39m eval_mode(samples, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_opt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m                          rag_k\u001b[38;5;241m=\u001b[39mDEFAULT_RAG_K,\n\u001b[1;32m     12\u001b[0m                          ctx_max_chars\u001b[38;5;241m=\u001b[39mDEFAULT_CTX_MAX_CHARS,\n\u001b[1;32m     13\u001b[0m                          evid_max_sents\u001b[38;5;241m=\u001b[39mDEFAULT_EVID_MAX_SENTS,\n\u001b[1;32m     14\u001b[0m                          sim_threshold\u001b[38;5;241m=\u001b[39mDEFAULT_SIM_THRESHOLD)\n",
      "Cell \u001b[0;32mIn[45], line 71\u001b[0m, in \u001b[0;36meval_mode\u001b[0;34m(samples, mode, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m tqdm(samples, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpredict_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[45], line 31\u001b[0m, in \u001b[0;36mpredict_one\u001b[0;34m(sample, mode, rag_k, ctx_max_chars, evid_max_sents, sim_threshold)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     30\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m build_prompt_base(q, opts)\n\u001b[0;32m---> 31\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_choices_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m     33\u001b[0m     record\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m: scores, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m: pred, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_context\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 33\u001b[0m, in \u001b[0;36mChoiceScorer.score_choices_next_token\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     31\u001b[0m tok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmax_prompt_tokens)\n\u001b[1;32m     32\u001b[0m tok \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m tok\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 33\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m logits \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# [1,vocab]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m scores \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1062\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    910\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    911\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    912\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         output_attentions,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:403\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    393\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]]:\n\u001b[1;32m    402\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 403\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    405\u001b[0m         hidden_states,\n\u001b[1;32m    406\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/normalization.py:202\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2576\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2574\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2575\u001b[0m     )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 10) Run one comparison\n",
    "# =========================\n",
    "base_records = eval_mode(samples, \"base\")\n",
    "ragq_records = eval_mode(samples, \"rag_q\",\n",
    "                         rag_k=DEFAULT_RAG_K,\n",
    "                         ctx_max_chars=DEFAULT_CTX_MAX_CHARS,\n",
    "                         evid_max_sents=DEFAULT_EVID_MAX_SENTS,\n",
    "                         sim_threshold=DEFAULT_SIM_THRESHOLD)\n",
    "rago_records = eval_mode(samples, \"rag_opt\",\n",
    "                         rag_k=DEFAULT_RAG_K,\n",
    "                         ctx_max_chars=DEFAULT_CTX_MAX_CHARS,\n",
    "                         evid_max_sents=DEFAULT_EVID_MAX_SENTS,\n",
    "                         sim_threshold=DEFAULT_SIM_THRESHOLD)\n",
    "\n",
    "base_sum = summarize(base_records)\n",
    "ragq_sum = summarize(ragq_records)\n",
    "rago_sum = summarize(rago_records)\n",
    "\n",
    "print(\"Base ACC :\", base_sum[\"acc\"])\n",
    "print(\"RAG-Q ACC:\", ragq_sum[\"acc\"], \"Fix/Hurt:\", fix_hurt(base_records, ragq_records))\n",
    "print(\"RAG-O ACC:\", rago_sum[\"acc\"], \"Fix/Hurt:\", fix_hurt(base_records, rago_records))\n",
    "\n",
    "save_jsonl(base_records, RUN_DIR / \"base_records.jsonl\")\n",
    "save_jsonl(ragq_records, RUN_DIR / \"ragq_records.jsonl\")\n",
    "save_jsonl(rago_records, RUN_DIR / \"rago_records.jsonl\")\n",
    "\n",
    "(RUN_DIR / \"summary.json\").write_text(json.dumps({\n",
    "    \"config\": {\n",
    "        \"DEV_FILE\": DEV_FILE, \"KB_DIR\": KB_DIR, \"MODEL\": MODEL_NAME_OR_PATH,\n",
    "        \"EMBED_MODEL\": EMBED_MODEL_NAME,\n",
    "        \"DEFAULT_RAG_K\": DEFAULT_RAG_K,\n",
    "        \"DEFAULT_CTX_MAX_CHARS\": DEFAULT_CTX_MAX_CHARS,\n",
    "        \"DEFAULT_EVID_MAX_SENTS\": DEFAULT_EVID_MAX_SENTS,\n",
    "        \"DEFAULT_SIM_THRESHOLD\": DEFAULT_SIM_THRESHOLD,\n",
    "    },\n",
    "    \"base\": base_sum,\n",
    "    \"rag_q\": ragq_sum,\n",
    "    \"rag_opt\": rago_sum,\n",
    "    \"fixhurt_rag_q\": fix_hurt(base_records, ragq_records),\n",
    "    \"fixhurt_rag_opt\": fix_hurt(base_records, rago_records),\n",
    "}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved to:\", RUN_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:rag_q: 100%|██████████| 4183/4183 [05:00<00:00, 13.90it/s]\n",
      "eval:rag_q: 100%|██████████| 4183/4183 [05:02<00:00, 13.83it/s]\n",
      "eval:rag_q: 100%|██████████| 4183/4183 [05:32<00:00, 12.57it/s]\n",
      "eval:rag_q: 100%|██████████| 4183/4183 [05:32<00:00, 12.58it/s]\n",
      "eval:rag_q: 100%|██████████| 4183/4183 [05:50<00:00, 11.94it/s]\n",
      "eval:rag_q: 100%|██████████| 4183/4183 [05:49<00:00, 11.98it/s]\n",
      "eval:rag_opt: 100%|██████████| 4183/4183 [18:22<00:00,  3.79it/s]\n",
      "eval:rag_opt: 100%|██████████| 4183/4183 [18:26<00:00,  3.78it/s]\n",
      "eval:rag_opt: 100%|██████████| 4183/4183 [20:21<00:00,  3.42it/s]\n",
      "eval:rag_opt: 100%|██████████| 4183/4183 [20:24<00:00,  3.42it/s]\n",
      "eval:rag_opt: 100%|██████████| 4183/4183 [21:31<00:00,  3.24it/s]\n",
      "eval:rag_opt: 100%|██████████| 4183/4183 [21:30<00:00,  3.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>rag_k</th>\n",
       "      <th>ctx_max_chars</th>\n",
       "      <th>evid_max_sents</th>\n",
       "      <th>sim_threshold</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>fix</th>\n",
       "      <th>hurt</th>\n",
       "      <th>evaluated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rag_opt</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322496</td>\n",
       "      <td>0.123023</td>\n",
       "      <td>0.158059</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rag_q</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322496</td>\n",
       "      <td>0.122442</td>\n",
       "      <td>0.157613</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rag_q</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322496</td>\n",
       "      <td>0.122442</td>\n",
       "      <td>0.157613</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rag_opt</td>\n",
       "      <td>5</td>\n",
       "      <td>600</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322257</td>\n",
       "      <td>0.122483</td>\n",
       "      <td>0.157582</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rag_q</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322257</td>\n",
       "      <td>0.121903</td>\n",
       "      <td>0.157136</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rag_q</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322257</td>\n",
       "      <td>0.121881</td>\n",
       "      <td>0.157107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rag_q</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322257</td>\n",
       "      <td>0.121881</td>\n",
       "      <td>0.157107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rag_q</td>\n",
       "      <td>5</td>\n",
       "      <td>600</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322257</td>\n",
       "      <td>0.121881</td>\n",
       "      <td>0.157107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rag_opt</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322257</td>\n",
       "      <td>0.121881</td>\n",
       "      <td>0.157107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rag_opt</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322257</td>\n",
       "      <td>0.121881</td>\n",
       "      <td>0.157107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rag_opt</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.322257</td>\n",
       "      <td>0.121881</td>\n",
       "      <td>0.157107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rag_opt</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0.321540</td>\n",
       "      <td>0.121675</td>\n",
       "      <td>0.156843</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mode  rag_k  ctx_max_chars  evid_max_sents sim_threshold       acc  \\\n",
       "0   rag_opt      3            600               4          None  0.322496   \n",
       "1     rag_q      1            300               4          None  0.322496   \n",
       "2     rag_q      1            600               4          None  0.322496   \n",
       "3   rag_opt      5            600               4          None  0.322257   \n",
       "4     rag_q      3            300               4          None  0.322257   \n",
       "5     rag_q      3            600               4          None  0.322257   \n",
       "6     rag_q      5            300               4          None  0.322257   \n",
       "7     rag_q      5            600               4          None  0.322257   \n",
       "8   rag_opt      1            300               4          None  0.322257   \n",
       "9   rag_opt      3            300               4          None  0.322257   \n",
       "10  rag_opt      5            300               4          None  0.322257   \n",
       "11  rag_opt      1            600               4          None  0.321540   \n",
       "\n",
       "    macro_f1  weighted_f1  fix  hurt  evaluated  \n",
       "0   0.123023     0.158059    4     2       4183  \n",
       "1   0.122442     0.157613    2     0       4183  \n",
       "2   0.122442     0.157613    2     0       4183  \n",
       "3   0.122483     0.157582    3     2       4183  \n",
       "4   0.121903     0.157136    2     1       4183  \n",
       "5   0.121881     0.157107    2     1       4183  \n",
       "6   0.121881     0.157107    2     1       4183  \n",
       "7   0.121881     0.157107    2     1       4183  \n",
       "8   0.121881     0.157107    2     1       4183  \n",
       "9   0.121881     0.157107    2     1       4183  \n",
       "10  0.121881     0.157107    2     1       4183  \n",
       "11  0.121675     0.156843    2     4       4183  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# 11) Grid search\n",
    "# =========================\n",
    "def run_grid(modes=(\"rag_q\",\"rag_opt\"),\n",
    "             rag_k_list=(1,3,5),\n",
    "             ctx_list=(300,600),\n",
    "             evid_sents_list=(4,),\n",
    "             sim_threshold_list=(None,)) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    base = base_records  # reuse\n",
    "\n",
    "    for mode in modes:\n",
    "        for rag_k in rag_k_list:\n",
    "            for ctx in ctx_list:\n",
    "                for es in evid_sents_list:\n",
    "                    for th in sim_threshold_list:\n",
    "                        recs = eval_mode(samples, mode,\n",
    "                                         rag_k=rag_k,\n",
    "                                         ctx_max_chars=ctx,\n",
    "                                         evid_max_sents=es,\n",
    "                                         sim_threshold=th)\n",
    "                        summ = summarize(recs)\n",
    "                        fh = fix_hurt(base, recs)\n",
    "                        rows.append({\n",
    "                            \"mode\": mode,\n",
    "                            \"rag_k\": rag_k,\n",
    "                            \"ctx_max_chars\": ctx,\n",
    "                            \"evid_max_sents\": es,\n",
    "                            \"sim_threshold\": th,\n",
    "                            \"acc\": summ[\"acc\"],\n",
    "                            \"macro_f1\": summ[\"macro\"][\"f1\"],\n",
    "                            \"weighted_f1\": summ[\"weighted\"][\"f1\"],\n",
    "                            \"fix\": fh[\"fix\"],\n",
    "                            \"hurt\": fh[\"hurt\"],\n",
    "                            \"evaluated\": summ[\"evaluated\"],\n",
    "                        })\n",
    "                        key = f\"{mode}_k{rag_k}_ctx{ctx}_es{es}_th{th}\"\n",
    "                        (RUN_DIR / f\"{key}.summary.json\").write_text(json.dumps({\n",
    "                            \"summary\": summ,\n",
    "                            \"fixhurt\": fh,\n",
    "                            \"params\": {\"mode\": mode, \"rag_k\": rag_k, \"ctx_max_chars\": ctx, \"evid_max_sents\": es, \"sim_threshold\": th}\n",
    "                        }, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values([\"acc\",\"macro_f1\"], ascending=False).reset_index(drop=True)\n",
    "    df.to_csv(RUN_DIR / \"grid_results.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "grid_df = run_grid(\n",
    "    modes=(\"rag_q\",\"rag_opt\"),\n",
    "    rag_k_list=(1,3,5),\n",
    "    ctx_list=(300,600),\n",
    "    evid_sents_list=(4,),\n",
    "    sim_threshold_list=(None,)\n",
    ")\n",
    "grid_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed9f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG-Q top1 score: min/mean/max = 0.8763331770896912 0.908949613571167 0.9389159679412842\n",
      "RAG-O top1 score: min/mean/max = 0.8507470488548279 0.9064990282058716 0.9410426020622253\n",
      "rag_q quantiles: {'0.05': 0.888480469584465, '0.1': 0.8951842665672303, '0.25': 0.9004756808280945, '0.5': 0.9088247716426849, '0.75': 0.9172671139240265, '0.9': 0.9245349586009979, '0.95': 0.9296152234077454}\n",
      "rag_opt quantiles: {'0.05': 0.8872941255569458, '0.1': 0.8918971240520477, '0.25': 0.8988986760377884, '0.5': 0.9067673981189728, '0.75': 0.9144319444894791, '0.9': 0.9226700484752655, '0.95': 0.9257474839687347}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 12) Inspect retrieval score distribution\n",
    "# =========================\n",
    "def sample_top1_scores(n=200, mode=\"rag_q\"):\n",
    "    scores = []\n",
    "    pick = random.sample(samples, min(n, len(samples)))\n",
    "    for s in pick:\n",
    "        if mode == \"rag_q\":\n",
    "            q = s[\"question\"]\n",
    "        else:\n",
    "            ch = random.choice([\"A\",\"B\",\"C\",\"D\"])\n",
    "            q = s[\"question\"] + \" \" + s[\"options\"][ch]\n",
    "        _, sc = retrieve_topk(q, k=1)\n",
    "        if sc:\n",
    "            scores.append(sc[0])\n",
    "    return np.array(scores, dtype=np.float32)\n",
    "\n",
    "arr_q = sample_top1_scores(n=300, mode=\"rag_q\")\n",
    "arr_o = sample_top1_scores(n=300, mode=\"rag_opt\")\n",
    "\n",
    "print(\"RAG-Q top1 score: min/mean/max =\", float(arr_q.min()), float(arr_q.mean()), float(arr_q.max()))\n",
    "print(\"RAG-O top1 score: min/mean/max =\", float(arr_o.min()), float(arr_o.mean()), float(arr_o.max()))\n",
    "\n",
    "for name, arr in [(\"rag_q\", arr_q), (\"rag_opt\", arr_o)]:\n",
    "    qs = np.quantile(arr, [0.05,0.1,0.25,0.5,0.75,0.9,0.95])\n",
    "    print(name, \"quantiles:\", {str(k): float(v) for k,v in zip([0.05,0.1,0.25,0.5,0.75,0.9,0.95], qs)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a785144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Q: A patient shows one or more of the following: advanced bone loss, grade II and III furcation involvements, tooth mobility, inaccessible areas, systemic/environmental factors represents:\n",
      "Gold: A\n",
      "Top scores: [0.8943, 0.8906, 0.8901]\n",
      "Top doc[0] snippet: Q: Children with apathy, general weakness, loosening of the skin, marasmic features also has X3B Xerophthalmia features. Eye finding will be A: Corneal ulcer with full thickness\n",
      "Evidence: Q: In osteoporosis there is -a) Decrease in absolute amount of bone massb) More common in malec) Radiographs show normal bone densityd) Hormonal replacement therapy A: ad Q: An 18 year old female patient complains of prominent upper front teeth. Extra-oral examination reveals an acute nasolabial angle and lip strain. Q: Children with apathy, general weakness, loosening of the skin, marasmic featur\n",
      "========================================================================================================================\n",
      "Q: Acquired cause of pure red cell aplasia are all except:\n",
      "Gold: C\n",
      "Top scores: [0.9157, 0.9152, 0.9151]\n",
      "Top doc[0] snippet: Q: A patient has long standing severe hemolytic anemia characterized by hypochromic cells. Electrophoresis studies demonstrate a near complete absence of beta chains. Several years later, the patient develops cardiac failure. Intracardiac deposition of which of the following would be most likely to \n",
      "Evidence: Q: A patient has long standing severe hemolytic anemia characterized by hypochromic cells. Q: All of the following statement regarding measles are true except -a) Rash appears first on legb) Koplik spot are seen on retinac) Long term complication may be seen in form of SSPEd) Caused by RNA viruse) I-P is 2-3 days A: abe Which of the following is most likely enzyme in which this young man A: Beta g\n",
      "========================================================================================================================\n",
      "Q: According to Weber-Fechner's law, strength of stimulus perceived is directly propoional to:\n",
      "Gold: A\n",
      "Top scores: [0.909, 0.9063, 0.9055]\n",
      "Top doc[0] snippet: Q: Increase in cytosolic calcium from intracellular storage, during smooth muscle contraction is/are due to: (PGI Dec 2008) A: 1P.-DAG\n",
      "Evidence: Which of the following positions serve as most comfoable for a patient facing difficulty in bre A: All positions are same in respect to comfo. Q: Increase in cytosolic calcium from intracellular storage, during smooth muscle contraction is/are due to: (PGI Dec 2008) A: 1P.-DAG Q: In a village having population of 1000, we found patients with ceain disease. He has been facing such problems on and o\n",
      "========================================================================================================================\n",
      "Q: Setting expansion of grey MTA when mixed with water is?\n",
      "Gold: C\n",
      "Top scores: [0.919, 0.918, 0.9168]\n",
      "Top doc[0] snippet: Q: Acid base based polyacrylic cements have most  important property of A: Bio adhesion\n",
      "Evidence: Q: Acid base based polyacrylic cements have most important property of A: Bio adhesion Q: Piezoelectric crystals are made use of that is safe from radiation also: A: US Q: Increase in cytosolic calcium from intracellular storage, during smooth muscle contraction is/are due to: (PGI Dec 2008) A: 1P.-DAG\n",
      "========================================================================================================================\n",
      "Q: Anthracis causes which pulmonary manifestations-\n",
      "Gold: B\n",
      "Top scores: [0.917, 0.9168, 0.9151]\n",
      "Top doc[0] snippet: Q: All of the following statement regarding measles are true except -a) Rash appears first on legb) Koplik spot are seen on retinac) Long term complication may be seen in form of SSPEd) Caused by RNA viruse) I-P is 2-3 days A: abe\n",
      "Evidence: Q: Which of the following is true regarding anthrax -a) M' Fadyean reaction shows capsule b) Humans are usually resistant to infectionc) Less than 100 spores can cause pulmonary infectiond) Gram stain shows organism with bulging sporese) Sputum microscopy helps in diagnosis A: abce Q: All of the following statement regarding measles are true except -a) Rash appears first on legb) Koplik spot are s\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def show_ragq_examples(n=5, rag_k=3):\n",
    "    for s in random.sample(samples, n):\n",
    "        q = s[\"question\"]\n",
    "        ids, sc = retrieve_topk(q, k=rag_k)\n",
    "        ev = extract_evidence(q, ids, max_sents=4)\n",
    "        print(\"=\"*120)\n",
    "        print(\"Q:\", q)\n",
    "        print(\"Gold:\", s[\"gold\"])\n",
    "        print(\"Top scores:\", [round(x,4) for x in sc])\n",
    "        print(\"Top doc[0] snippet:\", docs[ids[0]][:300].replace(\"\\n\",\" \"))\n",
    "        print(\"Evidence:\", ev[:400])\n",
    "\n",
    "show_ragq_examples(5, rag_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1007b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
