{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e88f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done] Report saved to: eval_report_llama_D.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "LETTERS = (\"A\", \"B\", \"C\", \"D\")\n",
    "\n",
    "EVAL_PATHS = [\n",
    "    r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_results_llama2_paperRAG/llama2_scoring_20251222_172524.jsonl\",\n",
    "]\n",
    "\n",
    "LIMIT = None  # None=å…¨é‡ï¼›ä¾‹å¦‚ 100 å°±åªè¯„æµ‹å‰100æ¡\n",
    "SAVE_REPORT_TXT = True\n",
    "REPORT_PATH = \"eval_report_llama_D.txt\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# IO\n",
    "# =========================\n",
    "def load_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "\n",
    "def pick_first(rec: Dict[str, Any], keys: List[str], default=None):\n",
    "    for k in keys:\n",
    "        if k in rec and rec[k] is not None:\n",
    "            return rec[k]\n",
    "    return default\n",
    "\n",
    "\n",
    "def normalize_choice(x) -> str:\n",
    "    \"\"\"\n",
    "    å°½é‡æŠŠè¾“å‡ºå½’ä¸€åˆ° A/B/C/Dã€‚\n",
    "    å…¼å®¹: '1/2/3/4', 'A.', 'A)', 'Answer: A', '... option B ...' ç­‰ã€‚\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    s = str(x).strip().upper()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "\n",
    "    mp = {\"1\": \"A\", \"2\": \"B\", \"3\": \"C\", \"4\": \"D\"}\n",
    "    if s in mp:\n",
    "        return mp[s]\n",
    "    if s in LETTERS:\n",
    "        return s\n",
    "\n",
    "    # å¸¸è§å‰ç¼€\n",
    "    # A. / A) / A: / (A)\n",
    "    for c in LETTERS:\n",
    "        if s.startswith(c + \".\") or s.startswith(c + \")\") or s.startswith(c + \":\"):\n",
    "            return c\n",
    "        if s.startswith(\"(\" + c + \")\"):\n",
    "            return c\n",
    "\n",
    "    # \"Answer: A\" / \"Final: B\" / \"Option C\"\n",
    "    for c in LETTERS:\n",
    "        if f\"ANSWER: {c}\" in s or f\"FINAL: {c}\" in s or f\"OPTION {c}\" in s:\n",
    "            return c\n",
    "\n",
    "    # å…œåº•ï¼šæ‰¾ç‹¬ç«‹å­—æ¯ï¼ˆé¿å…æŠŠ \"ABC\" è¯¯åˆ¤ï¼‰\n",
    "    # ç”¨æœ€æœ´ç´ æ–¹å¼ï¼šç”¨ç©ºç™½/æ ‡ç‚¹åŒ…å›´\n",
    "    seps = [\" \", \"\\n\", \"\\t\", \",\", \".\", \";\", \":\", \")\", \"(\", \"[\", \"]\", \"{\", \"}\", \"|\"]\n",
    "    for c in LETTERS:\n",
    "        for left in seps:\n",
    "            for right in seps:\n",
    "                if f\"{left}{c}{right}\" in s:\n",
    "                    return c\n",
    "\n",
    "    # æœ€åï¼šå¦‚æœå­—ç¬¦ä¸²ä»¥ A/B/C/D å¼€å¤´ï¼Œç›´æ¥å–\n",
    "    if len(s) >= 1 and s[0] in LETTERS:\n",
    "        return s[0]\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def build_prompt_from_record(rec: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    æ‹¼ä¸€ä¸ªç”¨äºdebugå±•ç¤ºçš„promptï¼ˆä¸æ˜¯ç”¨äºæ¨ç†ï¼‰\n",
    "    \"\"\"\n",
    "    q = pick_first(rec, [\"question\", \"q\", \"query\", \"prompt\", \"input\"], \"\")\n",
    "    ctx = pick_first(rec, [\"rag_context\", \"context\", \"retrieved_facts\", \"rag_context_preview\"], \"\")\n",
    "    out = [f\"Question: {q}\".strip()]\n",
    "    if isinstance(ctx, str) and ctx.strip():\n",
    "        out.append(\"\\nRAG Context:\\n\" + ctx.strip())\n",
    "    return \"\\n\".join(out).strip()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Metrics\n",
    "# =========================\n",
    "def compute_confusion_and_metrics(\n",
    "    preds: List[str],\n",
    "    gold: List[str],\n",
    "    prompts: List[str],\n",
    "    title: str\n",
    ") -> float:\n",
    "    labels = list(LETTERS)\n",
    "    L = len(labels)\n",
    "    idx = {c: i for i, c in enumerate(labels)}\n",
    "    conf = [[0] * L for _ in range(L)]\n",
    "\n",
    "    correct_samples, wrong_samples = [], []\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        p, g = preds[i], gold[i]\n",
    "        if p not in idx or g not in idx:\n",
    "            continue\n",
    "        conf[idx[g]][idx[p]] += 1\n",
    "\n",
    "        if p == g:\n",
    "            if len(correct_samples) < 5:\n",
    "                correct_samples.append((i, p, g, prompts[i]))\n",
    "        else:\n",
    "            if len(wrong_samples) < 5:\n",
    "                wrong_samples.append((i, p, g, prompts[i]))\n",
    "\n",
    "    gold_cnt = {c: sum(conf[idx[c]]) for c in labels}\n",
    "    pred_cnt = {c: sum(conf[r][idx[c]] for r in range(L)) for c in labels}\n",
    "\n",
    "    def safe(a, b):\n",
    "        return a / b if b > 0 else 0.0\n",
    "\n",
    "    by_cls = {}\n",
    "    for c in labels:\n",
    "        i = idx[c]\n",
    "        TP = conf[i][i]\n",
    "        FP = pred_cnt[c] - TP\n",
    "        FN = gold_cnt[c] - TP\n",
    "        prec = safe(TP, TP + FP)\n",
    "        rec = safe(TP, TP + FN)\n",
    "        f1 = safe(2 * prec * rec, prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        by_cls[c] = dict(\n",
    "            precision=prec, recall=rec, f1=f1,\n",
    "            support=gold_cnt[c], correct=TP\n",
    "        )\n",
    "\n",
    "    total = sum(gold_cnt.values())\n",
    "    correct_total = sum(conf[i][i] for i in range(L))\n",
    "    acc = safe(correct_total, total)\n",
    "\n",
    "    macro_p = sum(by_cls[c][\"precision\"] for c in labels) / L\n",
    "    macro_r = sum(by_cls[c][\"recall\"] for c in labels) / L\n",
    "    macro_f = sum(by_cls[c][\"f1\"] for c in labels) / L\n",
    "\n",
    "    weighted_p = safe(sum(by_cls[c][\"precision\"] * by_cls[c][\"support\"] for c in labels), total)\n",
    "    weighted_r = safe(sum(by_cls[c][\"recall\"] * by_cls[c][\"support\"] for c in labels), total)\n",
    "    weighted_f = safe(sum(by_cls[c][\"f1\"] * by_cls[c][\"support\"] for c in labels), total)\n",
    "\n",
    "    micro_p = acc\n",
    "    micro_r = acc\n",
    "    micro_f = acc\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(title)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n=== Distribution ===\")\n",
    "    print(\"Pred count:\", pred_cnt)\n",
    "    print(\"Gold count:\", gold_cnt)\n",
    "\n",
    "    print(\"\\n=== Per-class ===\")\n",
    "    print(\"Class  |  Prec   Recall   F1     Support   Correct/Gold\")\n",
    "    for c in labels:\n",
    "        d = by_cls[c]\n",
    "        print(\n",
    "            f\"  {c}    |  {d['precision']:.4f}  {d['recall']:.4f}  {d['f1']:.4f}\"\n",
    "            f\"    {d['support']:5d}     {d['correct']}/{d['support']}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n=== Averages ===\")\n",
    "    print(f\"Macro-Avg  precision={macro_p:.4f}  recall={macro_r:.4f}  F1={macro_f:.4f}\")\n",
    "    print(f\"Weighted   precision={weighted_p:.4f}  recall={weighted_r:.4f}  F1={weighted_f:.4f}\")\n",
    "    print(f\"Micro-Avg  precision={micro_p:.4f}  recall={micro_r:.4f}  F1={micro_f:.4f}\")\n",
    "    print(f\"ACC        = {acc:.4f}\")\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def fix_hurt(base_preds: List[str], rag_preds: List[str], gold: List[str]) -> Tuple[int, int, int]:\n",
    "    fix = hurt = same = 0\n",
    "    for bp, rp, g in zip(base_preds, rag_preds, gold):\n",
    "        if not (bp and rp and g):\n",
    "            continue\n",
    "        if bp != g and rp == g:\n",
    "            fix += 1\n",
    "        elif bp == g and rp != g:\n",
    "            hurt += 1\n",
    "        else:\n",
    "            same += 1\n",
    "    return fix, hurt, same\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    all_records: List[Dict[str, Any]] = []\n",
    "    for p in EVAL_PATHS:\n",
    "        recs = load_jsonl(p)\n",
    "        print(f\"[Info] Loaded {len(recs)} from {p}\")\n",
    "        all_records.extend(recs)\n",
    "\n",
    "    if LIMIT is not None:\n",
    "        all_records = all_records[:LIMIT]\n",
    "\n",
    "    gold: List[str] = []\n",
    "    base_preds: List[str] = []\n",
    "    rag_preds: List[str] = []\n",
    "    prompts: List[str] = []\n",
    "    skipped = 0\n",
    "\n",
    "    for rec in all_records:\n",
    "        g = normalize_choice(pick_first(rec, [\"ground_truth\", \"gt\", \"answer\", \"label\", \"gold\"], None))\n",
    "        bp = normalize_choice(pick_first(rec, [\"base_prediction\", \"base_pred\", \"pred_no_rag\", \"prediction\", \"pred\"], None))\n",
    "        rp = normalize_choice(pick_first(rec, [\"rag_prediction\", \"rag_pred\", \"pred_with_rag\"], None))\n",
    "\n",
    "        if not g:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # åŒæ—¶è¯„æµ‹ base + ragï¼šä¸¤è¾¹éƒ½å¿…é¡»å­˜åœ¨\n",
    "        if not bp or not rp:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        gold.append(g)\n",
    "        base_preds.append(bp)\n",
    "        rag_preds.append(rp)\n",
    "        prompts.append(build_prompt_from_record(rec))\n",
    "\n",
    "    print(f\"[Info] used={len(gold)} | skipped={skipped}\")\n",
    "\n",
    "    # 1) Base metrics\n",
    "    base_acc = compute_confusion_and_metrics(base_preds, gold, prompts, title=\"BASE (No RAG)\")\n",
    "\n",
    "    # 2) RAG metrics\n",
    "    rag_acc = compute_confusion_and_metrics(rag_preds, gold, prompts, title=\"RAG\")\n",
    "\n",
    "    # 3) Fix / Hurt\n",
    "    fix, hurt, same = fix_hurt(base_preds, rag_preds, gold)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Fix / Hurt\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"RAG Fix  (Baseé”™â†’RAGå¯¹): {fix}\")\n",
    "    print(f\"RAG Hurt (Baseå¯¹â†’RAGé”™): {hurt}\")\n",
    "    print(f\"Same (éƒ½å¯¹/éƒ½é”™):       {same}\")\n",
    "    print(f\"\\nBase ACC: {base_acc:.4f} | RAG ACC: {rag_acc:.4f} | Gain: {rag_acc - base_acc:+.4f}\")\n",
    "\n",
    "    return base_acc, rag_acc\n",
    "\n",
    "\n",
    "# ================\n",
    "# Run (+ optional save report to txt to avoid Jupyter truncation)\n",
    "# ================\n",
    "if SAVE_REPORT_TXT:\n",
    "    with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f, redirect_stdout(f):\n",
    "        main()\n",
    "    print(f\"[Done] Report saved to: {REPORT_PATH}\")\n",
    "else:\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4563fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 9 fields in line 3, saw 10\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 254\u001b[0m\n\u001b[1;32m    252\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;241m=\u001b[39m f\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;241m=\u001b[39m _stdout\n",
      "Cell \u001b[0;32mIn[14], line 246\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m CSV_PATHS:\n\u001b[0;32m--> 246\u001b[0m         \u001b[43mrun_one_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 186\u001b[0m, in \u001b[0;36mrun_one_csv\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_path):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(csv_path)\n\u001b[0;32m--> 186\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LIMIT \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mhead(LIMIT)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 9 fields in line 3, saw 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "EXP_LABEL = \"A. gpt\"\n",
    "LETTERS = (\"A\", \"B\", \"C\", \"D\")\n",
    "\n",
    "CSV_PATHS = [\n",
    "    r\"PrimeKG/results/rag_eval_20251212_175332.csv\",\n",
    "]\n",
    "\n",
    "LIMIT = None  # None=å…¨é‡ï¼›æˆ–å¡«æ•´æ•°åªå–å‰Nè¡Œ\n",
    "\n",
    "SAVE_TXT = True\n",
    "OUT_TXT = \"eval_report_summary_from_csv.txt\"\n",
    "PRINT_TO_SCREEN = False  # notebook ä¸æƒ³æŠ˜å å°± False\n",
    "\n",
    "# å€™é€‰åˆ—åï¼ˆå¼±åŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™/ä¸‹åˆ’çº¿/ç©ºæ ¼/ç ´æŠ˜å·ï¼‰\n",
    "CAND_GT   = [\"ground_truth\", \"gt\", \"label\", \"answer\", \"gold\"]\n",
    "CAND_BASE = [\"base_prediction\", \"base_pred\", \"pred_no_rag\", \"prediction\", \"pred\", \"base\"]\n",
    "CAND_RAG  = [\"rag_prediction\", \"rag_pred\", \"pred_with_rag\", \"pred_rag\", \"rag\"]\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def canon(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower().strip())\n",
    "\n",
    "def pick_col_fuzzy(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    mp = {canon(c): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        key = canon(cand)\n",
    "        if key in mp:\n",
    "            return mp[key]\n",
    "    return None\n",
    "\n",
    "def normalize_choice(x) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    s = str(x).strip().upper()\n",
    "    mp = {\"1\": \"A\", \"2\": \"B\", \"3\": \"C\", \"4\": \"D\"}\n",
    "    if s in mp:\n",
    "        return mp[s]\n",
    "    if s in LETTERS:\n",
    "        return s\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", s)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    if len(s) > 0 and s[0] in LETTERS:\n",
    "        return s[0]\n",
    "    return \"\"\n",
    "\n",
    "# ============================================================\n",
    "# Metrics\n",
    "# ============================================================\n",
    "def compute_report(preds: List[str], gold: List[str]) -> Dict:\n",
    "    labels = list(LETTERS)\n",
    "    L = len(labels)\n",
    "    idx = {c: i for i, c in enumerate(labels)}\n",
    "    conf = [[0] * L for _ in range(L)]  # rows=gold, cols=pred\n",
    "\n",
    "    total = 0\n",
    "    format_errors = 0\n",
    "\n",
    "    for p, g in zip(preds, gold):\n",
    "        if g not in idx:\n",
    "            continue\n",
    "        total += 1\n",
    "        if p not in idx:\n",
    "            format_errors += 1\n",
    "            continue\n",
    "        conf[idx[g]][idx[p]] += 1\n",
    "\n",
    "    gold_cnt = {c: 0 for c in labels}\n",
    "    for g in gold:\n",
    "        if g in idx:\n",
    "            gold_cnt[g] += 1\n",
    "\n",
    "    pred_cnt = {c: sum(conf[r][idx[c]] for r in range(L)) for c in labels}\n",
    "\n",
    "    def safe(a, b):\n",
    "        return a / b if b > 0 else 0.0\n",
    "\n",
    "    by_cls = {}\n",
    "    for c in labels:\n",
    "        i = idx[c]\n",
    "        TP = conf[i][i]\n",
    "        FP = pred_cnt[c] - TP\n",
    "        FN = gold_cnt[c] - TP\n",
    "        prec = safe(TP, TP + FP)\n",
    "        rec  = safe(TP, TP + FN)\n",
    "        f1   = safe(2 * prec * rec, prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        by_cls[c] = dict(precision=prec, recall=rec, f1=f1, support=gold_cnt[c], correct=TP)\n",
    "\n",
    "    correct_total = sum(conf[i][i] for i in range(L))\n",
    "    acc = safe(correct_total, total)\n",
    "\n",
    "    macro_p = sum(by_cls[c][\"precision\"] for c in labels) / L\n",
    "    macro_r = sum(by_cls[c][\"recall\"] for c in labels) / L\n",
    "    macro_f = sum(by_cls[c][\"f1\"] for c in labels) / L\n",
    "\n",
    "    weighted_p = safe(sum(by_cls[c][\"precision\"] * by_cls[c][\"support\"] for c in labels), sum(gold_cnt.values()))\n",
    "    weighted_r = safe(sum(by_cls[c][\"recall\"]    * by_cls[c][\"support\"] for c in labels), sum(gold_cnt.values()))\n",
    "    weighted_f = safe(sum(by_cls[c][\"f1\"]        * by_cls[c][\"support\"] for c in labels), sum(gold_cnt.values()))\n",
    "\n",
    "    micro_p = acc\n",
    "    micro_r = acc\n",
    "    micro_f = acc\n",
    "\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "        \"conf\": conf,                  # âœ… æ–°å¢ï¼šæ··æ·†çŸ©é˜µ\n",
    "        \"pred_cnt\": pred_cnt,\n",
    "        \"gold_cnt\": gold_cnt,\n",
    "        \"by_cls\": by_cls,\n",
    "        \"macro\": (macro_p, macro_r, macro_f),\n",
    "        \"weighted\": (weighted_p, weighted_r, weighted_f),\n",
    "        \"micro\": (micro_p, micro_r, micro_f),\n",
    "        \"acc\": acc,\n",
    "        \"format_errors\": format_errors,\n",
    "        \"total\": total,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_block(title: str, report: Dict):\n",
    "    bar = \"=\" * 60\n",
    "    print(bar)\n",
    "    print(title)\n",
    "    print(bar)\n",
    "\n",
    "    print(\"\\n=== Distribution ===\")\n",
    "    print(\"Pred count:\", report[\"pred_cnt\"])\n",
    "    print(\"Gold count:\", report[\"gold_cnt\"])\n",
    "\n",
    "    print(\"\\n=== Confusion Matrix (rows=Gold, cols=Pred) ===\")\n",
    "    labels = report[\"labels\"]\n",
    "    conf = report[\"conf\"]\n",
    "\n",
    "    header = \"    \" + \" \".join([f\"{c:>5s}\" for c in labels])\n",
    "    print(header)\n",
    "    for i, g in enumerate(labels):\n",
    "        row = \" \".join([f\"{conf[i][j]:5d}\" for j in range(len(labels))])\n",
    "        print(f\"{g} | {row}\")\n",
    "\n",
    "    print(\"\\n=== Per-class ===\")\n",
    "    print(\"Class  |  Prec   Recall   F1     Support   Correct/Gold\")\n",
    "    for c in LETTERS:\n",
    "        d = report[\"by_cls\"][c]\n",
    "        print(f\"  {c}    |  {d['precision']:.4f}  {d['recall']:.4f}  {d['f1']:.4f}     {d['support']:5d}     {d['correct']}/{d['support']}\")\n",
    "\n",
    "    macro_p, macro_r, macro_f = report[\"macro\"]\n",
    "    w_p, w_r, w_f = report[\"weighted\"]\n",
    "    mi_p, mi_r, mi_f = report[\"micro\"]\n",
    "\n",
    "    print(\"\\n=== Averages ===\")\n",
    "    print(f\"Macro-Avg  precision={macro_p:.4f}  recall={macro_r:.4f}  F1={macro_f:.4f}\")\n",
    "    print(f\"Weighted   precision={w_p:.4f}  recall={w_r:.4f}  F1={w_f:.4f}\")\n",
    "    print(f\"Micro-Avg  precision={mi_p:.4f}  recall={mi_r:.4f}  F1={mi_f:.4f}\")\n",
    "    print(f\"ACC        = {report['acc']:.4f}\")\n",
    "    print(f\"format_errors = {report['format_errors']} / total_eval = {report['total']}\\n\")\n",
    "\n",
    "\n",
    "def fix_hurt(base_preds: List[str], rag_preds: List[str], gold: List[str]) -> Tuple[int, int, int]:\n",
    "    fix = hurt = same = 0\n",
    "    for bp, rp, g in zip(base_preds, rag_preds, gold):\n",
    "        if bp != g and rp == g:\n",
    "            fix += 1\n",
    "        elif bp == g and rp != g:\n",
    "            hurt += 1\n",
    "        else:\n",
    "            same += 1\n",
    "    return fix, hurt, same\n",
    "\n",
    "# ============================================================\n",
    "# Main logic\n",
    "# ============================================================\n",
    "def run_one_csv(csv_path: str):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(csv_path)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if LIMIT is not None:\n",
    "        df = df.head(LIMIT)\n",
    "\n",
    "    col_gt   = pick_col_fuzzy(df, CAND_GT)\n",
    "    col_base = pick_col_fuzzy(df, CAND_BASE)\n",
    "    col_rag  = pick_col_fuzzy(df, CAND_RAG)\n",
    "\n",
    "    if col_gt is None or col_base is None:\n",
    "        raise KeyError(\n",
    "            f\"åˆ—ååŒ¹é…å¤±è´¥ï¼ˆè‡³å°‘è¦æœ‰ GT å’Œ BASEï¼‰ã€‚\\n\"\n",
    "            f\"æ£€æµ‹åˆ°åˆ—: {list(df.columns)}\\n\"\n",
    "            f\"åŒ¹é…ç»“æœ: GT={col_gt}, BASE={col_base}, RAG={col_rag}\"\n",
    "        )\n",
    "\n",
    "    # âœ… å…³é”®ä¿®å¤ï¼šå¦‚æœæ‰¾ä¸åˆ° RAG åˆ—ï¼Œå°±é€€åŒ–ä¸ºâ€œåªæœ‰ä¸€åˆ— predâ€\n",
    "    same_pred = False\n",
    "    if col_rag is None:\n",
    "        col_rag = col_base\n",
    "        same_pred = True\n",
    "\n",
    "    gold_all = df[col_gt].apply(normalize_choice).tolist()\n",
    "    base_all = df[col_base].apply(normalize_choice).tolist()\n",
    "    rag_all  = df[col_rag].apply(normalize_choice).tolist()\n",
    "\n",
    "    # ç»Ÿä¸€è¯„æµ‹é›†åˆï¼šåªè¦ gold æœ‰æ•ˆå°±è¿›ï¼›pred æ— æ•ˆç®— format error\n",
    "    gold_eval, base_eval, rag_eval = [], [], []\n",
    "    for g, b, r in zip(gold_all, base_all, rag_all):\n",
    "        if not g:\n",
    "            continue\n",
    "        gold_eval.append(g)\n",
    "        base_eval.append(b)\n",
    "        rag_eval.append(r)\n",
    "\n",
    "    base_report = compute_report(base_eval, gold_eval)\n",
    "    rag_report  = compute_report(rag_eval,  gold_eval)\n",
    "\n",
    "    print(EXP_LABEL)\n",
    "    print(f\"[Info] File: {csv_path}\")\n",
    "    print(f\"[Info] Columns used: GT={col_gt} | BASE={col_base} | RAG={col_rag}\")\n",
    "    print(f\"[Info] Used rows (gold valid): {len(gold_eval)}\")\n",
    "    if same_pred:\n",
    "        print(\"[Warn] è¿™ä¸ª CSV åªæœ‰ä¸€åˆ—é¢„æµ‹ï¼ˆæ²¡æœ‰ rag_pred/base_pred åˆ†å¼€å­˜ï¼‰ã€‚æ‰€ä»¥ BASE å’Œ RAG ä¼šå®Œå…¨ä¸€æ ·ï¼ŒFix/Hurt ä¹Ÿæ²¡æœ‰æ„ä¹‰ã€‚\")\n",
    "    print()\n",
    "\n",
    "    print_block(\"BASE (No RAG)\", base_report)\n",
    "    print_block(\"RAG\", rag_report)\n",
    "\n",
    "    fix, hurt, same = fix_hurt(base_eval, rag_eval, gold_eval)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Fix / Hurt\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"RAG Fix  (Baseé”™â†’RAGå¯¹): {fix}\")\n",
    "    print(f\"RAG Hurt (Baseå¯¹â†’RAGé”™): {hurt}\")\n",
    "    print(f\"Same (éƒ½å¯¹/éƒ½é”™):       {same}\")\n",
    "    print(f\"\\nBase ACC: {base_report['acc']:.4f} | RAG ACC: {rag_report['acc']:.4f} | Gain: {rag_report['acc']-base_report['acc']:+.4f}\")\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    for p in CSV_PATHS:\n",
    "        run_one_csv(p)\n",
    "\n",
    "# å†™ txt é˜²æ­¢ notebook æˆªæ–­\n",
    "if SAVE_TXT:\n",
    "    with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = f\n",
    "        try:\n",
    "            main()\n",
    "        finally:\n",
    "            sys.stdout = _stdout\n",
    "    print(f\"[Done] Saved report to: {OUT_TXT}\")\n",
    "\n",
    "if PRINT_TO_SCREEN:\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd8f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "æ­£åœ¨åˆ†ææ–‡ä»¶: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_results_gpt35_rag/gpt35_rag_eval_20251229_170029.csv\n",
      "âœ… è¯†åˆ«æˆåŠŸ: çœŸå®å€¼='Ground_Truth', é¢„æµ‹å€¼='Pred_Vector_RAG'\n",
      "\n",
      "==================== Counts ====================\n",
      "Pred count: {'A': 766, 'B': 1351, 'C': 1402, 'D': 664}\n",
      "Gold count: {'A': 1348, 'B': 1085, 'C': 925, 'D': 825}\n",
      "\n",
      "==================== Per-class ====================\n",
      "Class  | Precision | Recall    | F1        | Support | Correct/Gold\n",
      "-------------------------------------------------------------------\n",
      "A      | 0.6044    | 0.3435    | 0.4380    | 1348    | 463/1348\n",
      "B      | 0.4271    | 0.5318    | 0.4737    | 1085    | 577/1085\n",
      "C      | 0.3894    | 0.5903    | 0.4693    | 925     | 546/925\n",
      "D      | 0.4398    | 0.3539    | 0.3922    | 825     | 292/825\n",
      "\n",
      "==================== Averages ====================\n",
      "Averages     | Precision | Recall    | F1       \n",
      "-------------------------------------------------------------------\n",
      "Macro-Avg    | 0.4652    | 0.4549    | 0.4433   \n",
      "Weighted     | 0.4784    | 0.4490    | 0.4452   \n",
      "Micro-Avg    | 0.4490    | 0.4490    | 0.4490   \n",
      "\n",
      "ğŸ† ACC = 0.4490\n",
      "\n",
      "==================== Confusion Matrix ====================\n",
      "        Pred A  Pred B  Pred C  Pred D\n",
      "True A     463     370     359     156\n",
      "True B     128     577     258     122\n",
      "True C      81     204     546      94\n",
      "True D      94     200     239     292\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def analyze_results(file_path):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"æ­£åœ¨åˆ†ææ–‡ä»¶: {file_path}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "        return\n",
    "\n",
    "    # 1. åŠ è½½æ–‡ä»¶\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.jsonl'):\n",
    "            data = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        data.append(json.loads(line))\n",
    "            df = pd.DataFrame(data)\n",
    "        else:\n",
    "            print(\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"è¯»å–å¤±è´¥: {e}\")\n",
    "        return\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. ä¿®æ­£ï¼šæ·»åŠ ä½ çš„ CSV æ–‡ä»¶ä¸­å®é™…çš„åˆ—å\n",
    "    # ==========================================\n",
    "    # çœŸå®æ ‡ç­¾çš„å¯èƒ½åˆ—å\n",
    "    gt_cols = [\n",
    "        'Ground_Truth', 'ground_truth',  # ä½ çš„æ–‡ä»¶ç”¨çš„æ˜¯ Ground_Truth\n",
    "        'gt', 'gold', 'answer', 'final_decision', 'gt_decision', 'label'\n",
    "    ]\n",
    "    \n",
    "    # é¢„æµ‹æ ‡ç­¾çš„å¯èƒ½åˆ—å (ä¼˜å…ˆæ‰¾ RAG çš„ï¼Œå¦‚æœæƒ³çœ‹ Base çš„å¯ä»¥è°ƒæ•´é¡ºåº)\n",
    "    pred_cols = [\n",
    "        'Pred_Vector_RAG',  # ä½ çš„ RAG ç»“æœåˆ—\n",
    "        'Pred_No_RAG',      # ä½ çš„ Base ç»“æœåˆ— (å¦‚æœæƒ³è¯„æµ‹è¿™ä¸ªï¼ŒæŠŠè¿™ä¸€è¡Œæåˆ°ä¸Šé¢)\n",
    "        'pred', 'rag_pred', 'base_pred', 'prediction', 'generated_answer'\n",
    "    ]\n",
    "\n",
    "    col_gt = next((c for c in gt_cols if c in df.columns), None)\n",
    "    col_pred = next((c for c in pred_cols if c in df.columns), None)\n",
    "\n",
    "    if not col_gt or not col_pred:\n",
    "        print(f\"æ— æ³•è¯†åˆ«åˆ—åã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "        print(f\"è¯·æ£€æŸ¥ gt_cols å’Œ pred_cols åˆ—è¡¨æ˜¯å¦åŒ…å«ä½ çš„åˆ—åã€‚\")\n",
    "        return\n",
    "\n",
    "    print(f\"è¯†åˆ«æˆåŠŸ: çœŸå®å€¼='{col_gt}', é¢„æµ‹å€¼='{col_pred}'\")\n",
    "\n",
    "    # 3. æ•°æ®æ¸…æ´—\n",
    "    df_clean = df.dropna(subset=[col_gt, col_pred]).copy()\n",
    "    y_true = df_clean[col_gt].astype(str).str.strip().str.upper()\n",
    "    y_pred = df_clean[col_pred].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    valid_labels = sorted([l for l in list(set(y_true.unique()) | set(y_pred.unique())) if len(l) < 10])\n",
    "\n",
    "    # 4.1 æ‰“å° Count\n",
    "    pred_counts = y_pred.value_counts().to_dict()\n",
    "    gold_counts = y_true.value_counts().to_dict()\n",
    "    pred_counts_sorted = {k: pred_counts.get(k, 0) for k in valid_labels}\n",
    "    gold_counts_sorted = {k: gold_counts.get(k, 0) for k in valid_labels}\n",
    "\n",
    "    print(\"\\n\" + \"=\"*20 + \" Counts \" + \"=\"*20)\n",
    "    print(f\"Pred count: {pred_counts_sorted}\")\n",
    "    print(f\"Gold count: {gold_counts_sorted}\")\n",
    "\n",
    "    # 4.2 è®¡ç®—æ··æ·†çŸ©é˜µ & Correct\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=valid_labels)\n",
    "    correct_counts = dict(zip(valid_labels, cm.diagonal()))\n",
    "    report_dict = classification_report(y_true, y_pred, labels=valid_labels, output_dict=True, zero_division=0)\n",
    "\n",
    "    # 4.3 æ‰“å° Per-class è¡¨æ ¼\n",
    "    print(\"\\n\" + \"=\"*20 + \" Per-class \" + \"=\"*20)\n",
    "    header = f\"{'Class':<6} | {'Precision':<9} | {'Recall':<9} | {'F1':<9} | {'Support':<7} | {'Correct/Gold':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for label in valid_labels:\n",
    "        metrics = report_dict[label]\n",
    "        print(f\"{label:<6} | {metrics['precision']:<9.4f} | {metrics['recall']:<9.4f} | {metrics['f1-score']:<9.4f} | {int(metrics['support']):<7} | {correct_counts.get(label,0)}/{int(metrics['support'])}\")\n",
    "\n",
    "    # 4.4 æ‰“å° Averages\n",
    "    print(\"\\n\" + \"=\"*20 + \" Averages \" + \"=\"*20)\n",
    "    print(f\"{'Averages':<12} | {'Precision':<9} | {'Recall':<9} | {'F1':<9}\")\n",
    "    print(\"-\" * len(header)) # åˆ†å‰²çº¿\n",
    "\n",
    "    macro = report_dict['macro avg']\n",
    "    print(f\"{'Macro-Avg':<12} | {macro['precision']:<9.4f} | {macro['recall']:<9.4f} | {macro['f1-score']:<9.4f}\")\n",
    "    \n",
    "    weighted = report_dict['weighted avg']\n",
    "    print(f\"{'Weighted':<12} | {weighted['precision']:<9.4f} | {weighted['recall']:<9.4f} | {weighted['f1-score']:<9.4f}\")\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"{'Micro-Avg':<12} | {acc:<9.4f} | {acc:<9.4f} | {acc:<9.4f}\")\n",
    "    print(f\"\\nğŸ† ACC = {acc:.4f}\")\n",
    "\n",
    "    # 4.5 æ‰“å°æ··æ·†çŸ©é˜µ\n",
    "    print(\"\\n\" + \"=\"*20 + \" Confusion Matrix \" + \"=\"*20)\n",
    "    cm_df = pd.DataFrame(cm, index=[f\"True {x}\" for x in valid_labels], columns=[f\"Pred {x}\" for x in valid_labels])\n",
    "    print(cm_df)\n",
    "\n",
    "# ==========================================\n",
    "# è¿è¡Œ\n",
    "# ==========================================\n",
    "file_path = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_results_gpt35_rag/gpt35_rag_eval_20251229_170029.csv\"\n",
    "analyze_results(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdba5854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "æ­£åœ¨åˆ†ææ–‡ä»¶: /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_results_gpt35_rag/gpt35_rag_eval_20251229_170029.csv\n",
      "âŒ æ— æ³•è¯†åˆ«åˆ—åã€‚å½“å‰åˆ—: ['Index', 'Question', 'Ground_Truth', 'Pred_No_RAG', 'Correct_No_RAG', 'Pred_Vector_RAG', 'Correct_Vector_RAG', 'Status', 'Retrieved_Context', 'Global_Acc_No_RAG', 'Global_Acc_Vector_RAG']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "def analyze_results(file_path):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"æ­£åœ¨åˆ†ææ–‡ä»¶: {file_path}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"âŒ æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "        return\n",
    "\n",
    "    # 1. åŠ è½½æ–‡ä»¶\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.jsonl'):\n",
    "            data = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        data.append(json.loads(line))\n",
    "            df = pd.DataFrame(data)\n",
    "        else:\n",
    "            print(\"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–å¤±è´¥: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. è‡ªåŠ¨è¯†åˆ«åˆ—å\n",
    "    gt_cols = ['gt', 'gold', 'ground_truth', 'answer', 'final_decision', 'gt_decision', 'label']\n",
    "    pred_cols = ['pred', 'rag_pred', 'base_pred', 'prediction', 'generated_answer']\n",
    "\n",
    "    col_gt = next((c for c in gt_cols if c in df.columns), None)\n",
    "    col_pred = next((c for c in pred_cols if c in df.columns), None)\n",
    "\n",
    "    if not col_gt or not col_pred:\n",
    "        print(f\"âŒ æ— æ³•è¯†åˆ«åˆ—åã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "        return\n",
    "\n",
    "    # 3. æ•°æ®æ¸…æ´—\n",
    "    df_clean = df.dropna(subset=[col_gt, col_pred]).copy()\n",
    "    y_true = df_clean[col_gt].astype(str).str.strip().str.upper()\n",
    "    y_pred = df_clean[col_pred].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    # è‡ªåŠ¨ç¡®å®šæœ‰æ•ˆæ ‡ç­¾ (æ’é™¤ nan ç­‰)\n",
    "    valid_labels = sorted([l for l in list(set(y_true.unique()) | set(y_pred.unique())) if len(l) < 10])\n",
    "\n",
    "    # ==========================================\n",
    "    # ğŸ‘‡ è¿™é‡Œæ˜¯ä¿®æ”¹çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œæ¨¡ä»¿å›¾ç‰‡çš„è¾“å‡ºæ ¼å¼\n",
    "    # ==========================================\n",
    "\n",
    "    # 4.1 æ‰“å° Count (Pred count & Gold count)\n",
    "    # ä½¿ç”¨ value_counts() å¹¶è½¬ä¸ºå­—å…¸\n",
    "    pred_counts = y_pred.value_counts().to_dict()\n",
    "    gold_counts = y_true.value_counts().to_dict()\n",
    "    \n",
    "    # æŒ‰æ ‡ç­¾æ’åºè®©æ˜¾ç¤ºæ›´å¥½çœ‹\n",
    "    pred_counts_sorted = {k: pred_counts.get(k, 0) for k in valid_labels}\n",
    "    gold_counts_sorted = {k: gold_counts.get(k, 0) for k in valid_labels}\n",
    "\n",
    "    print(\"\\n\" + \"=\"*20 + \" Counts \" + \"=\"*20)\n",
    "    print(f\"Pred count: {pred_counts_sorted}\")\n",
    "    print(f\"Gold count: {gold_counts_sorted}\")\n",
    "\n",
    "    # 4.2 è®¡ç®—æ··æ·†çŸ©é˜µ (ä¸ºäº†è·å– Correct æ•°é‡)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=valid_labels)\n",
    "    # å¯¹è§’çº¿ä¸Šçš„æ•°å­—å°±æ˜¯å„ç±»åˆ«é¢„æµ‹æ­£ç¡®çš„æ•°é‡\n",
    "    correct_counts = dict(zip(valid_labels, cm.diagonal()))\n",
    "\n",
    "    # 4.3 è·å–è¯¦ç»†æŒ‡æ ‡å­—å…¸\n",
    "    report_dict = classification_report(y_true, y_pred, labels=valid_labels, output_dict=True, zero_division=0)\n",
    "\n",
    "    # 4.4 æ‰“å° Per-class è¡¨æ ¼\n",
    "    print(\"\\n\" + \"=\"*20 + \" Per-class \" + \"=\"*20)\n",
    "    header = f\"{'Class':<6} | {'Precision':<9} | {'Recall':<9} | {'F1':<9} | {'Support':<7} | {'Correct/Gold':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for label in valid_labels:\n",
    "        metrics = report_dict[label]\n",
    "        p = metrics['precision']\n",
    "        r = metrics['recall']\n",
    "        f1 = metrics['f1-score']\n",
    "        sup = int(metrics['support'])\n",
    "        corr = correct_counts.get(label, 0)\n",
    "        \n",
    "        print(f\"{label:<6} | {p:<9.4f} | {r:<9.4f} | {f1:<9.4f} | {sup:<7} | {corr}/{sup}\")\n",
    "\n",
    "    # 4.5 æ‰“å° Averages è¡¨æ ¼\n",
    "    print(\"\\n\" + \"=\"*20 + \" Averages \" + \"=\"*20)\n",
    "    avg_header = f\"{'Averages':<12} | {'Precision':<9} | {'Recall':<9} | {'F1':<9}\"\n",
    "    print(avg_header)\n",
    "    print(\"-\" * len(avg_header))\n",
    "\n",
    "    # Macro Avg\n",
    "    macro = report_dict['macro avg']\n",
    "    print(f\"{'Macro-Avg':<12} | {macro['precision']:<9.4f} | {macro['recall']:<9.4f} | {macro['f1-score']:<9.4f}\")\n",
    "\n",
    "    # Weighted Avg\n",
    "    weighted = report_dict['weighted avg']\n",
    "    print(f\"{'Weighted':<12} | {weighted['precision']:<9.4f} | {weighted['recall']:<9.4f} | {weighted['f1-score']:<9.4f}\")\n",
    "\n",
    "    # Micro Avg (åœ¨å•æ ‡ç­¾å¤šåˆ†ç±»ä¸­ï¼ŒMicro P/R/F1 éƒ½ç­‰äº Accuracy)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"{'Micro-Avg':<12} | {acc:<9.4f} | {acc:<9.4f} | {acc:<9.4f}\")\n",
    "    \n",
    "    print(f\"\\n ACC = {acc:.4f}\")\n",
    "\n",
    "    # 4.6 æ‰“å°æ··æ·†çŸ©é˜µ\n",
    "    print(\"\\n\" + \"=\"*20 + \" Confusion Matrix \" + \"=\"*20)\n",
    "    cm_df = pd.DataFrame(cm, index=[f\"True {x}\" for x in valid_labels], columns=[f\"Pred {x}\" for x in valid_labels])\n",
    "    print(cm_df)\n",
    "\n",
    "# ==========================================\n",
    "# è¿è¡Œç¤ºä¾‹\n",
    "# ==========================================\n",
    "file_path_1 = \"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_results_gpt35_rag/gpt35_rag_eval_20251229_170029.csv\"\n",
    "if os.path.exists(file_path_1):\n",
    "    analyze_results(file_path_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c03ae",
   "metadata": {},
   "source": [
    "Pubmedqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a69d41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"æ‰¾ä¸åˆ° reference åˆ—ã€‚å½“å‰åˆ—: ['i', 'question', 'gold_decision', 'pred_decision', 'gold_text', 'pred_text', 'rag_context']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 423\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Done] Saved csv    : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 423\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 414\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m INPUT_PATHS:\n\u001b[0;32m--> 414\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mrun_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m         tag \u001b[38;5;241m=\u001b[39m stem_tag(p)\n\u001b[1;32m    416\u001b[0m         out_txt, out_json, out_csv \u001b[38;5;241m=\u001b[39m save_outputs(res, OUT_DIR, tag)\n",
      "Cell \u001b[0;32mIn[25], line 266\u001b[0m, in \u001b[0;36mrun_one\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    263\u001b[0m COL_RAG_DEC   \u001b[38;5;241m=\u001b[39m pick_col(df, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_kw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_decision\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COL_REF \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæ‰¾ä¸åˆ° reference åˆ—ã€‚å½“å‰åˆ—: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COL_BASE_TEXT \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæ‰¾ä¸åˆ° base_pred/base_text åˆ—ã€‚å½“å‰åˆ—: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"æ‰¾ä¸åˆ° reference åˆ—ã€‚å½“å‰åˆ—: ['i', 'question', 'gold_decision', 'pred_decision', 'gold_text', 'pred_text', 'rag_context']\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "INPUT_PATHS = [\n",
    "    # ä½ çš„ç»“æœæ–‡ä»¶ï¼ˆjsonl / csv éƒ½è¡Œï¼‰\n",
    "    r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_results_llama2/llama2_results_165240.jsonl\",\n",
    "]\n",
    "\n",
    "# å¦‚æœä½ çš„ç»“æœæ–‡ä»¶é‡Œæ²¡æœ‰ gt_decision/final_decisionï¼Œå°±åœ¨è¿™é‡Œå¡« PubMedQA parquetï¼ˆç”¨äºæŒ‰ id å›å¡« GTï¼‰\n",
    "PUBMEDQA_PARQUET = r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\"\n",
    "PARQUET_ID_IS_ROW_INDEX = True  # PubMedQA è¿™ä¸ª parquet é€šå¸¸ id=è¡Œå·(0..n-1)ï¼ŒåŸºæœ¬éƒ½å¯¹\n",
    "\n",
    "LIMIT = None\n",
    "OUT_DIR = \"reports\"\n",
    "TITLE = \"A. llama2 (KG.csv RAG)\"\n",
    "\n",
    "# BERTScore é…ç½®\n",
    "BERT_LANG = \"en\"\n",
    "BERT_MODEL_TYPE = \"distilbert-base-uncased\"\n",
    "BERT_BATCH_SIZE = 32\n",
    "BERT_RESCALE_WITH_BASELINE = False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def normalize_path(p: str) -> str:\n",
    "    p = p.strip()\n",
    "    # ä½ å†™çš„ //media/... åœ¨ Linux æœ‰æ—¶ä¹Ÿèƒ½ç”¨ï¼Œä½†ä¸å»ºè®®ï¼›è¿™é‡Œç›´æ¥è§„æ•´\n",
    "    if p.startswith(\"//media/\"):\n",
    "        p = \"/\" + p.lstrip(\"/\")\n",
    "    return os.path.normpath(p)\n",
    "\n",
    "def canon(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower().strip())\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    mp = {canon(c): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        key = canon(cand)\n",
    "        if key in mp:\n",
    "            return mp[key]\n",
    "    return None\n",
    "\n",
    "def load_table(path: str) -> pd.DataFrame:\n",
    "    path = normalize_path(path)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    elif ext == \".jsonl\":\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    rows.append(json.loads(line))\n",
    "        df = pd.DataFrame(rows)\n",
    "    elif ext == \".json\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "        df = pd.DataFrame(obj if isinstance(obj, list) else obj.get(\"data\", []))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "    if LIMIT is not None:\n",
    "        df = df.head(LIMIT)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Decision parsing\n",
    "# =========================\n",
    "DEC_SET = (\"yes\", \"no\", \"maybe\")\n",
    "\n",
    "def norm_decision(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    s = str(x).strip().lower()\n",
    "    if s in DEC_SET:\n",
    "        return s\n",
    "    m = re.search(r\"\\b(yes|no|maybe)\\b\", s)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_decision_from_text(text: str) -> str:\n",
    "    s = (text or \"\").lower()\n",
    "    matches = re.findall(r\"\\b(yes|no|maybe)\\b\", s)\n",
    "    return matches[-1] if matches else \"unknown\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Decision ACC + confusion\n",
    "# =========================\n",
    "def compute_acc_and_confusion_decision(gt_raw: List[str], pred_raw: List[str]) -> Dict[str, Any]:\n",
    "    total = 0\n",
    "    support = 0\n",
    "    format_errors = 0\n",
    "    correct = 0\n",
    "\n",
    "    confusion = {\n",
    "        \"yes->yes\": 0, \"yes->no\": 0, \"yes->maybe\": 0,\n",
    "        \"no->yes\": 0,  \"no->no\": 0,  \"no->maybe\": 0,\n",
    "        \"maybe->yes\": 0, \"maybe->no\": 0, \"maybe->maybe\": 0\n",
    "    }\n",
    "\n",
    "    for g0, p0 in zip(gt_raw, pred_raw):\n",
    "        g = norm_decision(g0)\n",
    "        if not g:\n",
    "            continue\n",
    "        total += 1\n",
    "\n",
    "        p = norm_decision(p0)\n",
    "        if not p:\n",
    "            format_errors += 1\n",
    "            continue\n",
    "\n",
    "        support += 1\n",
    "        confusion[f\"{g}->{p}\"] += 1\n",
    "        if g == p:\n",
    "            correct += 1\n",
    "\n",
    "    acc = (correct / total) if total else 0.0  # format_errors ä¹Ÿç®—é”™\n",
    "    return {\n",
    "        \"acc\": float(acc),\n",
    "        \"support\": int(support),\n",
    "        \"format_errors\": int(format_errors),\n",
    "        \"confusion\": confusion\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ROUGE-L (pure python)\n",
    "# =========================\n",
    "def _lcs_len(a_tokens, b_tokens):\n",
    "    n, m = len(a_tokens), len(b_tokens)\n",
    "    dp = [0] * (m + 1)\n",
    "    for i in range(1, n + 1):\n",
    "        prev = 0\n",
    "        for j in range(1, m + 1):\n",
    "            tmp = dp[j]\n",
    "            if a_tokens[i - 1] == b_tokens[j - 1]:\n",
    "                dp[j] = prev + 1\n",
    "            else:\n",
    "                dp[j] = max(dp[j], dp[j - 1])\n",
    "            prev = tmp\n",
    "    return dp[m]\n",
    "\n",
    "def rouge_l_f1(pred: str, ref: str) -> float:\n",
    "    pred = (pred or \"\").strip()\n",
    "    ref = (ref or \"\").strip()\n",
    "    if not pred or not ref:\n",
    "        return 0.0\n",
    "    pred_tokens = re.findall(r\"\\w+\", pred.lower())\n",
    "    ref_tokens  = re.findall(r\"\\w+\", ref.lower())\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "    lcs = _lcs_len(pred_tokens, ref_tokens)\n",
    "    prec = lcs / len(pred_tokens)\n",
    "    rec  = lcs / len(ref_tokens)\n",
    "    return 0.0 if (prec + rec) == 0 else (2 * prec * rec / (prec + rec))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# BERTScore\n",
    "# =========================\n",
    "def compute_bertscore(preds: List[str], refs: List[str]) -> Tuple[Dict[str, float], Dict[str, List[float]]]:\n",
    "    preds = [\"\" if p is None else str(p) for p in preds]\n",
    "    refs  = [\"\" if r is None else str(r) for r in refs]\n",
    "\n",
    "    import torch\n",
    "    import evaluate\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    metric = evaluate.load(\"bertscore\")\n",
    "    res = metric.compute(\n",
    "        predictions=preds,\n",
    "        references=refs,\n",
    "        lang=BERT_LANG,\n",
    "        model_type=BERT_MODEL_TYPE,\n",
    "        device=device,\n",
    "        batch_size=BERT_BATCH_SIZE,\n",
    "        rescale_with_baseline=bool(BERT_RESCALE_WITH_BASELINE),\n",
    "    )\n",
    "\n",
    "    p_list = [float(x) for x in res[\"precision\"]]\n",
    "    r_list = [float(x) for x in res[\"recall\"]]\n",
    "    f_list = [float(x) for x in res[\"f1\"]]\n",
    "\n",
    "    agg = {\n",
    "        \"precision\": float(np.mean(p_list)) if p_list else 0.0,\n",
    "        \"recall\": float(np.mean(r_list)) if r_list else 0.0,\n",
    "        \"f1\": float(np.mean(f_list)) if f_list else 0.0,\n",
    "    }\n",
    "    per = {\"precision\": p_list, \"recall\": r_list, \"f1\": f_list}\n",
    "    return agg, per\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Optional: backfill GT from PubMedQA parquet\n",
    "# =========================\n",
    "def backfill_gt_from_parquet(df: pd.DataFrame, id_col: str) -> pd.DataFrame:\n",
    "    pq_path = normalize_path(PUBMEDQA_PARQUET)\n",
    "    if not PUBMEDQA_PARQUET or not os.path.exists(pq_path):\n",
    "        return df\n",
    "\n",
    "    pq_df = pd.read_parquet(pq_path)\n",
    "    # æœŸæœ› parquet é‡Œæœ‰ final_decision / long_answer\n",
    "    if \"final_decision\" not in pq_df.columns:\n",
    "        return df\n",
    "\n",
    "    if PARQUET_ID_IS_ROW_INDEX:\n",
    "        # id ç›´æ¥å½“è¡Œå·\n",
    "        def get_decision(i):\n",
    "            try:\n",
    "                return str(pq_df.iloc[int(i)][\"final_decision\"]).strip().lower()\n",
    "            except Exception:\n",
    "                return \"\"\n",
    "        df[\"__gt_from_parquet\"] = df[id_col].apply(get_decision)\n",
    "    else:\n",
    "        # å¦‚æœä½ çš„ parquet é‡Œæœ‰æŸä¸ª id åˆ—ï¼Œå°±æ”¹è¿™é‡Œåš merge\n",
    "        return df\n",
    "\n",
    "    # å¦‚æœåŸæ¥å°±æœ‰ gt_decision/final_decisionï¼Œå°±ä¸è¦†ç›–\n",
    "    if \"gt_decision\" not in df.columns and \"final_decision\" not in df.columns:\n",
    "        df[\"gt_decision\"] = df[\"__gt_from_parquet\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Core run\n",
    "# =========================\n",
    "def format_section(title: str, obj: Dict) -> str:\n",
    "    return f\"\\n=== {title}  ===\\n{json.dumps(obj, indent=2, ensure_ascii=False)}\\n\"\n",
    "\n",
    "def stem_tag(path: str) -> str:\n",
    "    base = os.path.basename(path)\n",
    "    return os.path.splitext(base)[0]\n",
    "\n",
    "def run_one(path: str) -> Dict[str, Any]:\n",
    "    df = load_table(path)\n",
    "\n",
    "    # ----- å…³é”®åˆ—å€™é€‰ -----\n",
    "    COL_ID  = pick_col(df, [\"id\", \"i\", \"idx\"])\n",
    "    COL_REF = pick_col(df, [\"reference\", \"ref\", \"long_answer\", \"answer\"])\n",
    "    COL_GT  = pick_col(df, [\"gt_decision\", \"final_decision\", \"ground_truth\", \"gold\", \"label\"])\n",
    "\n",
    "    COL_BASE_TEXT = pick_col(df, [\"base_text\", \"base_pred\", \"pred_no_rag\", \"pred\", \"prediction\"])\n",
    "    COL_RAG_TEXT  = pick_col(df, [\"rag_text\", \"rag_pred\", \"pred_with_rag\"])\n",
    "\n",
    "    # ä½ è¿™ä»½ llama2 jsonl é‡Œæ˜¯ base_kw / rag_kwï¼ˆé¢„æµ‹å†³ç­–ï¼‰\n",
    "    COL_BASE_DEC  = pick_col(df, [\"base_kw\", \"base_decision\"])\n",
    "    COL_RAG_DEC   = pick_col(df, [\"rag_kw\", \"rag_decision\"])\n",
    "\n",
    "    if COL_REF is None:\n",
    "        raise KeyError(f\"æ‰¾ä¸åˆ° reference åˆ—ã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "    if COL_BASE_TEXT is None:\n",
    "        raise KeyError(f\"æ‰¾ä¸åˆ° base_pred/base_text åˆ—ã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "    if COL_RAG_TEXT is None:\n",
    "        raise KeyError(f\"æ‰¾ä¸åˆ° rag_pred/rag_text åˆ—ã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "\n",
    "    # ----- å¦‚æœæ²¡æœ‰ GT decisionï¼Œå°è¯•ä» parquet å›å¡« -----\n",
    "    if COL_GT is None:\n",
    "        if COL_ID is None:\n",
    "            raise KeyError(\n",
    "                \"ç»“æœæ–‡ä»¶é‡Œæ²¡æœ‰ gt_decision/final_decisionï¼ŒåŒæ—¶ä¹Ÿæ²¡æœ‰ idï¼Œæ— æ³•å›å¡« GTã€‚\\n\"\n",
    "                f\"å½“å‰åˆ—: {list(df.columns)}\"\n",
    "            )\n",
    "        df = backfill_gt_from_parquet(df, COL_ID)\n",
    "        COL_GT = pick_col(df, [\"gt_decision\", \"final_decision\"])\n",
    "\n",
    "    if COL_GT is None:\n",
    "        raise KeyError(\n",
    "            \"ä»ç„¶æ‰¾ä¸åˆ° gt_decision/final_decisionï¼ˆå›å¡«å¤±è´¥ï¼‰ã€‚\\n\"\n",
    "            \"è¯·ç¡®è®¤ä½ æä¾›äº†æ­£ç¡®çš„ PubMedQA parquetï¼Œä¸”åŒ…å« final_decisionã€‚\"\n",
    "        )\n",
    "\n",
    "    refs = df[COL_REF].fillna(\"\").astype(str).tolist()\n",
    "    gt   = df[COL_GT].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    base_text = df[COL_BASE_TEXT].fillna(\"\").astype(str).tolist()\n",
    "    rag_text  = df[COL_RAG_TEXT].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    # é¢„æµ‹å†³ç­–ï¼šä¼˜å…ˆç”¨ base_kw/rag_kwï¼›æ²¡æœ‰å°±ä»ç”Ÿæˆæ–‡æœ¬é‡ŒæŠ½\n",
    "    if COL_BASE_DEC is not None:\n",
    "        base_dec = df[COL_BASE_DEC].fillna(\"\").astype(str).apply(norm_decision).tolist()\n",
    "        if sum(1 for x in base_dec if x) == 0:\n",
    "            base_dec = [extract_decision_from_text(t) for t in base_text]\n",
    "    else:\n",
    "        base_dec = [extract_decision_from_text(t) for t in base_text]\n",
    "\n",
    "    if COL_RAG_DEC is not None:\n",
    "        rag_dec = df[COL_RAG_DEC].fillna(\"\").astype(str).apply(norm_decision).tolist()\n",
    "        if sum(1 for x in rag_dec if x) == 0:\n",
    "            rag_dec = [extract_decision_from_text(t) for t in rag_text]\n",
    "    else:\n",
    "        rag_dec = [extract_decision_from_text(t) for t in rag_text]\n",
    "\n",
    "    # ----- Metrics -----\n",
    "    base_acc = compute_acc_and_confusion_decision(gt, base_dec)\n",
    "    rag_acc  = compute_acc_and_confusion_decision(gt, rag_dec)\n",
    "\n",
    "    rouge_base_list = [rouge_l_f1(p, r) for p, r in zip(base_text, refs)]\n",
    "    rouge_rag_list  = [rouge_l_f1(p, r) for p, r in zip(rag_text,  refs)]\n",
    "    rouge = {\n",
    "        \"base_mean\": float(np.mean(rouge_base_list)) if rouge_base_list else 0.0,\n",
    "        \"rag_mean\":  float(np.mean(rouge_rag_list)) if rouge_rag_list else 0.0,\n",
    "        \"gain\":      float(np.mean(rouge_rag_list) - np.mean(rouge_base_list)) if rouge_base_list else 0.0,\n",
    "    }\n",
    "\n",
    "    bert = {\"base\": None, \"rag\": None, \"error\": None}\n",
    "    bert_per = {\"base\": None, \"rag\": None}\n",
    "    try:\n",
    "        base_bs, base_per = compute_bertscore(base_text, refs)\n",
    "        rag_bs,  rag_per  = compute_bertscore(rag_text,  refs)\n",
    "        bert[\"base\"] = base_bs\n",
    "        bert[\"rag\"]  = rag_bs\n",
    "        bert_per[\"base\"] = base_per\n",
    "        bert_per[\"rag\"]  = rag_per\n",
    "    except Exception as e:\n",
    "        bert[\"error\"] = str(e)\n",
    "\n",
    "    # expanded df\n",
    "    out_df = df.copy()\n",
    "    out_df[\"gt_decision_used\"] = [norm_decision(x) for x in gt]\n",
    "    out_df[\"base_decision_used\"] = base_dec\n",
    "    out_df[\"rag_decision_used\"] = rag_dec\n",
    "    out_df[\"rougeL_base\"] = rouge_base_list\n",
    "    out_df[\"rougeL_rag\"]  = rouge_rag_list\n",
    "\n",
    "    if bert_per[\"base\"] is not None:\n",
    "        out_df[\"bert_f1_base\"] = bert_per[\"base\"][\"f1\"]\n",
    "    if bert_per[\"rag\"] is not None:\n",
    "        out_df[\"bert_f1_rag\"] = bert_per[\"rag\"][\"f1\"]\n",
    "    if (\"bert_f1_base\" in out_df.columns) and (\"bert_f1_rag\" in out_df.columns):\n",
    "        out_df[\"bert_diff\"] = out_df[\"bert_f1_rag\"] - out_df[\"bert_f1_base\"]\n",
    "\n",
    "    info = {\n",
    "        \"path\": normalize_path(path),\n",
    "        \"rows\": int(len(df)),\n",
    "        \"columns\": list(df.columns),\n",
    "        \"col_id\": COL_ID,\n",
    "        \"col_ref\": COL_REF,\n",
    "        \"col_gt_decision\": COL_GT,\n",
    "        \"col_base_text\": COL_BASE_TEXT,\n",
    "        \"col_rag_text\": COL_RAG_TEXT,\n",
    "        \"col_base_dec\": COL_BASE_DEC,\n",
    "        \"col_rag_dec\": COL_RAG_DEC,\n",
    "        \"task_forced\": \"decision(yes/no/maybe)\"\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"info\": info,\n",
    "        \"metrics\": {\n",
    "            \"bertscore\": bert,\n",
    "            \"acc\": {\"base\": base_acc, \"rag\": rag_acc},\n",
    "            \"rougeL\": rouge\n",
    "        },\n",
    "        \"out_df\": out_df\n",
    "    }\n",
    "\n",
    "\n",
    "def save_outputs(result: Dict[str, Any], out_dir: str, tag: str) -> Tuple[str, str, str]:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_txt  = os.path.join(out_dir, f\"{tag}_report.txt\")\n",
    "    out_json = os.path.join(out_dir, f\"{tag}_metrics.json\")\n",
    "    out_csv  = os.path.join(out_dir, f\"{tag}_expanded.csv\")\n",
    "\n",
    "    info = result[\"info\"]\n",
    "    m = result[\"metrics\"]\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"{TITLE}\\n\" + \"=\"*60)\n",
    "    lines.append(f\"[Info] Loaded: {info['path']} | rows={info['rows']}\")\n",
    "    lines.append(f\"[Info] Columns: {info['columns']}\")\n",
    "    lines.append(f\"[Info] Task: {info['task_forced']}\")\n",
    "    lines.append(f\"[Info] Using cols: ref={info['col_ref']} | gt={info['col_gt_decision']} | base_text={info['col_base_text']} | rag_text={info['col_rag_text']} | base_dec={info['col_base_dec']} | rag_dec={info['col_rag_dec']}\")\n",
    "\n",
    "    if m[\"bertscore\"][\"base\"] is not None:\n",
    "        lines.append(\"\\n=== Base BERTScore  ===\")\n",
    "        lines.append(json.dumps(m[\"bertscore\"][\"base\"], indent=2, ensure_ascii=False))\n",
    "        lines.append(\"\\n=== RAG  BERTScore  ===\")\n",
    "        lines.append(json.dumps(m[\"bertscore\"][\"rag\"], indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        lines.append(\"\\n[Warn] BERTScore skipped: \" + str(m[\"bertscore\"][\"error\"]))\n",
    "\n",
    "    lines.append(format_section(\"Base ACC\", m[\"acc\"][\"base\"]))\n",
    "    lines.append(format_section(\"RAG  ACC\", m[\"acc\"][\"rag\"]))\n",
    "    lines.append(format_section(\"ROUGE-L (pure python)\", m[\"rougeL\"]))\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result[\"metrics\"], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    result[\"out_df\"].to_csv(out_csv, index=False)\n",
    "\n",
    "    return out_txt, out_json, out_csv\n",
    "\n",
    "\n",
    "def main():\n",
    "    for p in INPUT_PATHS:\n",
    "        res = run_one(p)\n",
    "        tag = stem_tag(p)\n",
    "        out_txt, out_json, out_csv = save_outputs(res, OUT_DIR, tag)\n",
    "        print(f\"[Done] Saved report : {out_txt}\")\n",
    "        print(f\"[Done] Saved metrics: {out_json}\")\n",
    "        print(f\"[Done] Saved csv    : {out_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48589f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"æ‰¾ä¸åˆ° reference åˆ—ã€‚å½“å‰åˆ—: ['i', 'question', 'gold_decision', 'pred_decision', 'gold_text', 'pred_text', 'rag_context']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 423\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Done] Saved csv    : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 423\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 414\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m INPUT_PATHS:\n\u001b[0;32m--> 414\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mrun_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m         tag \u001b[38;5;241m=\u001b[39m stem_tag(p)\n\u001b[1;32m    416\u001b[0m         out_txt, out_json, out_csv \u001b[38;5;241m=\u001b[39m save_outputs(res, OUT_DIR, tag)\n",
      "Cell \u001b[0;32mIn[24], line 266\u001b[0m, in \u001b[0;36mrun_one\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    263\u001b[0m COL_RAG_DEC   \u001b[38;5;241m=\u001b[39m pick_col(df, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_kw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_decision\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COL_REF \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæ‰¾ä¸åˆ° reference åˆ—ã€‚å½“å‰åˆ—: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COL_BASE_TEXT \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæ‰¾ä¸åˆ° base_pred/base_text åˆ—ã€‚å½“å‰åˆ—: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"æ‰¾ä¸åˆ° reference åˆ—ã€‚å½“å‰åˆ—: ['i', 'question', 'gold_decision', 'pred_decision', 'gold_text', 'pred_text', 'rag_context']\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "INPUT_PATHS = [\n",
    "    # ä½ çš„ç»“æœæ–‡ä»¶ï¼ˆjsonl / csv éƒ½è¡Œï¼‰\n",
    "    r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_out_kgcsv_pubmedqa_gpt35/run_20251226_182026_gpt35/results.csv\",\n",
    "]\n",
    "\n",
    "# å¦‚æœä½ çš„ç»“æœæ–‡ä»¶é‡Œæ²¡æœ‰ gt_decision/final_decisionï¼Œå°±åœ¨è¿™é‡Œå¡« PubMedQA parquetï¼ˆç”¨äºæŒ‰ id å›å¡« GTï¼‰\n",
    "PUBMEDQA_PARQUET = r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/data/pubmedqa_hf/pqa_labeled_splits/test.parquet\"\n",
    "PARQUET_ID_IS_ROW_INDEX = True  # PubMedQA è¿™ä¸ª parquet é€šå¸¸ id=è¡Œå·(0..n-1)ï¼ŒåŸºæœ¬éƒ½å¯¹\n",
    "\n",
    "LIMIT = None\n",
    "OUT_DIR = \"reports\"\n",
    "TITLE = \"A. llama2 (KG.csv RAG)\"\n",
    "\n",
    "# BERTScore é…ç½®\n",
    "BERT_LANG = \"en\"\n",
    "BERT_MODEL_TYPE = \"distilbert-base-uncased\"\n",
    "BERT_BATCH_SIZE = 32\n",
    "BERT_RESCALE_WITH_BASELINE = False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def normalize_path(p: str) -> str:\n",
    "    p = p.strip()\n",
    "    # ä½ å†™çš„ //media/... åœ¨ Linux æœ‰æ—¶ä¹Ÿèƒ½ç”¨ï¼Œä½†ä¸å»ºè®®ï¼›è¿™é‡Œç›´æ¥è§„æ•´\n",
    "    if p.startswith(\"//media/\"):\n",
    "        p = \"/\" + p.lstrip(\"/\")\n",
    "    return os.path.normpath(p)\n",
    "\n",
    "def canon(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower().strip())\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    mp = {canon(c): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        key = canon(cand)\n",
    "        if key in mp:\n",
    "            return mp[key]\n",
    "    return None\n",
    "\n",
    "def load_table(path: str) -> pd.DataFrame:\n",
    "    path = normalize_path(path)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    elif ext == \".jsonl\":\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    rows.append(json.loads(line))\n",
    "        df = pd.DataFrame(rows)\n",
    "    elif ext == \".json\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "        df = pd.DataFrame(obj if isinstance(obj, list) else obj.get(\"data\", []))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "    if LIMIT is not None:\n",
    "        df = df.head(LIMIT)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Decision parsing\n",
    "# =========================\n",
    "DEC_SET = (\"yes\", \"no\", \"maybe\")\n",
    "\n",
    "def norm_decision(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    s = str(x).strip().lower()\n",
    "    if s in DEC_SET:\n",
    "        return s\n",
    "    m = re.search(r\"\\b(yes|no|maybe)\\b\", s)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_decision_from_text(text: str) -> str:\n",
    "    s = (text or \"\").lower()\n",
    "    matches = re.findall(r\"\\b(yes|no|maybe)\\b\", s)\n",
    "    return matches[-1] if matches else \"unknown\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Decision ACC + confusion\n",
    "# =========================\n",
    "def compute_acc_and_confusion_decision(gt_raw: List[str], pred_raw: List[str]) -> Dict[str, Any]:\n",
    "    total = 0\n",
    "    support = 0\n",
    "    format_errors = 0\n",
    "    correct = 0\n",
    "\n",
    "    confusion = {\n",
    "        \"yes->yes\": 0, \"yes->no\": 0, \"yes->maybe\": 0,\n",
    "        \"no->yes\": 0,  \"no->no\": 0,  \"no->maybe\": 0,\n",
    "        \"maybe->yes\": 0, \"maybe->no\": 0, \"maybe->maybe\": 0\n",
    "    }\n",
    "\n",
    "    for g0, p0 in zip(gt_raw, pred_raw):\n",
    "        g = norm_decision(g0)\n",
    "        if not g:\n",
    "            continue\n",
    "        total += 1\n",
    "\n",
    "        p = norm_decision(p0)\n",
    "        if not p:\n",
    "            format_errors += 1\n",
    "            continue\n",
    "\n",
    "        support += 1\n",
    "        confusion[f\"{g}->{p}\"] += 1\n",
    "        if g == p:\n",
    "            correct += 1\n",
    "\n",
    "    acc = (correct / total) if total else 0.0  # format_errors ä¹Ÿç®—é”™\n",
    "    return {\n",
    "        \"acc\": float(acc),\n",
    "        \"support\": int(support),\n",
    "        \"format_errors\": int(format_errors),\n",
    "        \"confusion\": confusion\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ROUGE-L (pure python)\n",
    "# =========================\n",
    "def _lcs_len(a_tokens, b_tokens):\n",
    "    n, m = len(a_tokens), len(b_tokens)\n",
    "    dp = [0] * (m + 1)\n",
    "    for i in range(1, n + 1):\n",
    "        prev = 0\n",
    "        for j in range(1, m + 1):\n",
    "            tmp = dp[j]\n",
    "            if a_tokens[i - 1] == b_tokens[j - 1]:\n",
    "                dp[j] = prev + 1\n",
    "            else:\n",
    "                dp[j] = max(dp[j], dp[j - 1])\n",
    "            prev = tmp\n",
    "    return dp[m]\n",
    "\n",
    "def rouge_l_f1(pred: str, ref: str) -> float:\n",
    "    pred = (pred or \"\").strip()\n",
    "    ref = (ref or \"\").strip()\n",
    "    if not pred or not ref:\n",
    "        return 0.0\n",
    "    pred_tokens = re.findall(r\"\\w+\", pred.lower())\n",
    "    ref_tokens  = re.findall(r\"\\w+\", ref.lower())\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "    lcs = _lcs_len(pred_tokens, ref_tokens)\n",
    "    prec = lcs / len(pred_tokens)\n",
    "    rec  = lcs / len(ref_tokens)\n",
    "    return 0.0 if (prec + rec) == 0 else (2 * prec * rec / (prec + rec))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# BERTScore\n",
    "# =========================\n",
    "def compute_bertscore(preds: List[str], refs: List[str]) -> Tuple[Dict[str, float], Dict[str, List[float]]]:\n",
    "    preds = [\"\" if p is None else str(p) for p in preds]\n",
    "    refs  = [\"\" if r is None else str(r) for r in refs]\n",
    "\n",
    "    import torch\n",
    "    import evaluate\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    metric = evaluate.load(\"bertscore\")\n",
    "    res = metric.compute(\n",
    "        predictions=preds,\n",
    "        references=refs,\n",
    "        lang=BERT_LANG,\n",
    "        model_type=BERT_MODEL_TYPE,\n",
    "        device=device,\n",
    "        batch_size=BERT_BATCH_SIZE,\n",
    "        rescale_with_baseline=bool(BERT_RESCALE_WITH_BASELINE),\n",
    "    )\n",
    "\n",
    "    p_list = [float(x) for x in res[\"precision\"]]\n",
    "    r_list = [float(x) for x in res[\"recall\"]]\n",
    "    f_list = [float(x) for x in res[\"f1\"]]\n",
    "\n",
    "    agg = {\n",
    "        \"precision\": float(np.mean(p_list)) if p_list else 0.0,\n",
    "        \"recall\": float(np.mean(r_list)) if r_list else 0.0,\n",
    "        \"f1\": float(np.mean(f_list)) if f_list else 0.0,\n",
    "    }\n",
    "    per = {\"precision\": p_list, \"recall\": r_list, \"f1\": f_list}\n",
    "    return agg, per\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Optional: backfill GT from PubMedQA parquet\n",
    "# =========================\n",
    "def backfill_gt_from_parquet(df: pd.DataFrame, id_col: str) -> pd.DataFrame:\n",
    "    pq_path = normalize_path(PUBMEDQA_PARQUET)\n",
    "    if not PUBMEDQA_PARQUET or not os.path.exists(pq_path):\n",
    "        return df\n",
    "\n",
    "    pq_df = pd.read_parquet(pq_path)\n",
    "    # æœŸæœ› parquet é‡Œæœ‰ final_decision / long_answer\n",
    "    if \"final_decision\" not in pq_df.columns:\n",
    "        return df\n",
    "\n",
    "    if PARQUET_ID_IS_ROW_INDEX:\n",
    "        # id ç›´æ¥å½“è¡Œå·\n",
    "        def get_decision(i):\n",
    "            try:\n",
    "                return str(pq_df.iloc[int(i)][\"final_decision\"]).strip().lower()\n",
    "            except Exception:\n",
    "                return \"\"\n",
    "        df[\"__gt_from_parquet\"] = df[id_col].apply(get_decision)\n",
    "    else:\n",
    "        # å¦‚æœä½ çš„ parquet é‡Œæœ‰æŸä¸ª id åˆ—ï¼Œå°±æ”¹è¿™é‡Œåš merge\n",
    "        return df\n",
    "\n",
    "    # å¦‚æœåŸæ¥å°±æœ‰ gt_decision/final_decisionï¼Œå°±ä¸è¦†ç›–\n",
    "    if \"gt_decision\" not in df.columns and \"final_decision\" not in df.columns:\n",
    "        df[\"gt_decision\"] = df[\"__gt_from_parquet\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Core run\n",
    "# =========================\n",
    "def format_section(title: str, obj: Dict) -> str:\n",
    "    return f\"\\n=== {title}  ===\\n{json.dumps(obj, indent=2, ensure_ascii=False)}\\n\"\n",
    "\n",
    "def stem_tag(path: str) -> str:\n",
    "    base = os.path.basename(path)\n",
    "    return os.path.splitext(base)[0]\n",
    "\n",
    "def run_one(path: str) -> Dict[str, Any]:\n",
    "    df = load_table(path)\n",
    "\n",
    "    # ----- å…³é”®åˆ—å€™é€‰ -----\n",
    "    COL_ID  = pick_col(df, [\"id\", \"i\", \"idx\"])\n",
    "    COL_REF = pick_col(df, [\"reference\", \"ref\", \"long_answer\", \"answer\"])\n",
    "    COL_GT  = pick_col(df, [\"gt_decision\", \"final_decision\", \"ground_truth\", \"gold\", \"label\"])\n",
    "\n",
    "    COL_BASE_TEXT = pick_col(df, [\"base_text\", \"base_pred\", \"pred_no_rag\", \"pred\", \"prediction\"])\n",
    "    COL_RAG_TEXT  = pick_col(df, [\"rag_text\", \"rag_pred\", \"pred_with_rag\"])\n",
    "\n",
    "    # ä½ è¿™ä»½ llama2 jsonl é‡Œæ˜¯ base_kw / rag_kwï¼ˆé¢„æµ‹å†³ç­–ï¼‰\n",
    "    COL_BASE_DEC  = pick_col(df, [\"base_kw\", \"base_decision\"])\n",
    "    COL_RAG_DEC   = pick_col(df, [\"rag_kw\", \"rag_decision\"])\n",
    "\n",
    "    if COL_REF is None:\n",
    "        raise KeyError(f\"æ‰¾ä¸åˆ° reference åˆ—ã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "    if COL_BASE_TEXT is None:\n",
    "        raise KeyError(f\"æ‰¾ä¸åˆ° base_pred/base_text åˆ—ã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "    if COL_RAG_TEXT is None:\n",
    "        raise KeyError(f\"æ‰¾ä¸åˆ° rag_pred/rag_text åˆ—ã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "\n",
    "    # ----- å¦‚æœæ²¡æœ‰ GT decisionï¼Œå°è¯•ä» parquet å›å¡« -----\n",
    "    if COL_GT is None:\n",
    "        if COL_ID is None:\n",
    "            raise KeyError(\n",
    "                \"ç»“æœæ–‡ä»¶é‡Œæ²¡æœ‰ gt_decision/final_decisionï¼ŒåŒæ—¶ä¹Ÿæ²¡æœ‰ idï¼Œæ— æ³•å›å¡« GTã€‚\\n\"\n",
    "                f\"å½“å‰åˆ—: {list(df.columns)}\"\n",
    "            )\n",
    "        df = backfill_gt_from_parquet(df, COL_ID)\n",
    "        COL_GT = pick_col(df, [\"gt_decision\", \"final_decision\"])\n",
    "\n",
    "    if COL_GT is None:\n",
    "        raise KeyError(\n",
    "            \"ä»ç„¶æ‰¾ä¸åˆ° gt_decision/final_decisionï¼ˆå›å¡«å¤±è´¥ï¼‰ã€‚\\n\"\n",
    "            \"è¯·ç¡®è®¤ä½ æä¾›äº†æ­£ç¡®çš„ PubMedQA parquetï¼Œä¸”åŒ…å« final_decisionã€‚\"\n",
    "        )\n",
    "\n",
    "    refs = df[COL_REF].fillna(\"\").astype(str).tolist()\n",
    "    gt   = df[COL_GT].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    base_text = df[COL_BASE_TEXT].fillna(\"\").astype(str).tolist()\n",
    "    rag_text  = df[COL_RAG_TEXT].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    # é¢„æµ‹å†³ç­–ï¼šä¼˜å…ˆç”¨ base_kw/rag_kwï¼›æ²¡æœ‰å°±ä»ç”Ÿæˆæ–‡æœ¬é‡ŒæŠ½\n",
    "    if COL_BASE_DEC is not None:\n",
    "        base_dec = df[COL_BASE_DEC].fillna(\"\").astype(str).apply(norm_decision).tolist()\n",
    "        if sum(1 for x in base_dec if x) == 0:\n",
    "            base_dec = [extract_decision_from_text(t) for t in base_text]\n",
    "    else:\n",
    "        base_dec = [extract_decision_from_text(t) for t in base_text]\n",
    "\n",
    "    if COL_RAG_DEC is not None:\n",
    "        rag_dec = df[COL_RAG_DEC].fillna(\"\").astype(str).apply(norm_decision).tolist()\n",
    "        if sum(1 for x in rag_dec if x) == 0:\n",
    "            rag_dec = [extract_decision_from_text(t) for t in rag_text]\n",
    "    else:\n",
    "        rag_dec = [extract_decision_from_text(t) for t in rag_text]\n",
    "\n",
    "    # ----- Metrics -----\n",
    "    base_acc = compute_acc_and_confusion_decision(gt, base_dec)\n",
    "    rag_acc  = compute_acc_and_confusion_decision(gt, rag_dec)\n",
    "\n",
    "    rouge_base_list = [rouge_l_f1(p, r) for p, r in zip(base_text, refs)]\n",
    "    rouge_rag_list  = [rouge_l_f1(p, r) for p, r in zip(rag_text,  refs)]\n",
    "    rouge = {\n",
    "        \"base_mean\": float(np.mean(rouge_base_list)) if rouge_base_list else 0.0,\n",
    "        \"rag_mean\":  float(np.mean(rouge_rag_list)) if rouge_rag_list else 0.0,\n",
    "        \"gain\":      float(np.mean(rouge_rag_list) - np.mean(rouge_base_list)) if rouge_base_list else 0.0,\n",
    "    }\n",
    "\n",
    "    bert = {\"base\": None, \"rag\": None, \"error\": None}\n",
    "    bert_per = {\"base\": None, \"rag\": None}\n",
    "    try:\n",
    "        base_bs, base_per = compute_bertscore(base_text, refs)\n",
    "        rag_bs,  rag_per  = compute_bertscore(rag_text,  refs)\n",
    "        bert[\"base\"] = base_bs\n",
    "        bert[\"rag\"]  = rag_bs\n",
    "        bert_per[\"base\"] = base_per\n",
    "        bert_per[\"rag\"]  = rag_per\n",
    "    except Exception as e:\n",
    "        bert[\"error\"] = str(e)\n",
    "\n",
    "    # expanded df\n",
    "    out_df = df.copy()\n",
    "    out_df[\"gt_decision_used\"] = [norm_decision(x) for x in gt]\n",
    "    out_df[\"base_decision_used\"] = base_dec\n",
    "    out_df[\"rag_decision_used\"] = rag_dec\n",
    "    out_df[\"rougeL_base\"] = rouge_base_list\n",
    "    out_df[\"rougeL_rag\"]  = rouge_rag_list\n",
    "\n",
    "    if bert_per[\"base\"] is not None:\n",
    "        out_df[\"bert_f1_base\"] = bert_per[\"base\"][\"f1\"]\n",
    "    if bert_per[\"rag\"] is not None:\n",
    "        out_df[\"bert_f1_rag\"] = bert_per[\"rag\"][\"f1\"]\n",
    "    if (\"bert_f1_base\" in out_df.columns) and (\"bert_f1_rag\" in out_df.columns):\n",
    "        out_df[\"bert_diff\"] = out_df[\"bert_f1_rag\"] - out_df[\"bert_f1_base\"]\n",
    "\n",
    "    info = {\n",
    "        \"path\": normalize_path(path),\n",
    "        \"rows\": int(len(df)),\n",
    "        \"columns\": list(df.columns),\n",
    "        \"col_id\": COL_ID,\n",
    "        \"col_ref\": COL_REF,\n",
    "        \"col_gt_decision\": COL_GT,\n",
    "        \"col_base_text\": COL_BASE_TEXT,\n",
    "        \"col_rag_text\": COL_RAG_TEXT,\n",
    "        \"col_base_dec\": COL_BASE_DEC,\n",
    "        \"col_rag_dec\": COL_RAG_DEC,\n",
    "        \"task_forced\": \"decision(yes/no/maybe)\"\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"info\": info,\n",
    "        \"metrics\": {\n",
    "            \"bertscore\": bert,\n",
    "            \"acc\": {\"base\": base_acc, \"rag\": rag_acc},\n",
    "            \"rougeL\": rouge\n",
    "        },\n",
    "        \"out_df\": out_df\n",
    "    }\n",
    "\n",
    "\n",
    "def save_outputs(result: Dict[str, Any], out_dir: str, tag: str) -> Tuple[str, str, str]:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_txt  = os.path.join(out_dir, f\"{tag}_report.txt\")\n",
    "    out_json = os.path.join(out_dir, f\"{tag}_metrics.json\")\n",
    "    out_csv  = os.path.join(out_dir, f\"{tag}_expanded.csv\")\n",
    "\n",
    "    info = result[\"info\"]\n",
    "    m = result[\"metrics\"]\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"{TITLE}\\n\" + \"=\"*60)\n",
    "    lines.append(f\"[Info] Loaded: {info['path']} | rows={info['rows']}\")\n",
    "    lines.append(f\"[Info] Columns: {info['columns']}\")\n",
    "    lines.append(f\"[Info] Task: {info['task_forced']}\")\n",
    "    lines.append(f\"[Info] Using cols: ref={info['col_ref']} | gt={info['col_gt_decision']} | base_text={info['col_base_text']} | rag_text={info['col_rag_text']} | base_dec={info['col_base_dec']} | rag_dec={info['col_rag_dec']}\")\n",
    "\n",
    "    if m[\"bertscore\"][\"base\"] is not None:\n",
    "        lines.append(\"\\n=== Base BERTScore  ===\")\n",
    "        lines.append(json.dumps(m[\"bertscore\"][\"base\"], indent=2, ensure_ascii=False))\n",
    "        lines.append(\"\\n=== RAG  BERTScore  ===\")\n",
    "        lines.append(json.dumps(m[\"bertscore\"][\"rag\"], indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        lines.append(\"\\n[Warn] BERTScore skipped: \" + str(m[\"bertscore\"][\"error\"]))\n",
    "\n",
    "    lines.append(format_section(\"Base ACC\", m[\"acc\"][\"base\"]))\n",
    "    lines.append(format_section(\"RAG  ACC\", m[\"acc\"][\"rag\"]))\n",
    "    lines.append(format_section(\"ROUGE-L (pure python)\", m[\"rougeL\"]))\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result[\"metrics\"], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    result[\"out_df\"].to_csv(out_csv, index=False)\n",
    "\n",
    "    return out_txt, out_json, out_csv\n",
    "\n",
    "\n",
    "def main():\n",
    "    for p in INPUT_PATHS:\n",
    "        res = run_one(p)\n",
    "        tag = stem_tag(p)\n",
    "        out_txt, out_json, out_csv = save_outputs(res, OUT_DIR, tag)\n",
    "        print(f\"[Done] Saved report : {out_txt}\")\n",
    "        print(f\"[Done] Saved metrics: {out_json}\")\n",
    "        print(f\"[Done] Saved csv    : {out_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cdf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] Failed to process /media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/eval_out/pubmedqa_rag_20251222_143546.jsonl: \"æ‰¾ä¸åˆ° reference/ref/gold/gt åˆ—ã€‚å½“å‰åˆ—: ['i', 'question', 'gold_decision', 'pred_decision', 'gold_text', 'pred_text', 'rag_context']\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "INPUT_PATHS = [\n",
    "    r\"/media/miaoen/ad4277ac-5cfe-47b0-a2cc-f9e50e0da444/LLM/PrimeKG/eval_results_compare/compare_primekg_only_20251229_1538.csv\",\n",
    "]\n",
    "LIMIT = None\n",
    "OUT_DIR = \"reports\"\n",
    "TITLE = \"A. gpt\"\n",
    "\n",
    "# BERTScore é…ç½®\n",
    "BERT_LANG = \"en\"\n",
    "BERT_MODEL_TYPE = \"distilbert-base-uncased\"\n",
    "BERT_BATCH_SIZE = 32\n",
    "BERT_RESCALE_WITH_BASELINE = False\n",
    "\n",
    "# =========================\n",
    "# Helpers: load + column pick\n",
    "# =========================\n",
    "def canon(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower().strip())\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    mp = {canon(c): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        k = canon(cand)\n",
    "        if k in mp:\n",
    "            return mp[k]\n",
    "    return None\n",
    "\n",
    "def load_table(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    elif ext == \".jsonl\":\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    rows.append(json.loads(line))\n",
    "        df = pd.DataFrame(rows)\n",
    "    elif ext == \".json\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "        if isinstance(obj, list):\n",
    "            df = pd.DataFrame(obj)\n",
    "        else:\n",
    "            df = pd.DataFrame(obj.get(\"data\", []))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "    if LIMIT is not None:\n",
    "        df = df.head(LIMIT)\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# Task inference (decision / mcqa / text_only)\n",
    "# =========================\n",
    "LETTERS = (\"A\", \"B\", \"C\", \"D\")\n",
    "DEC_SET = (\"yes\", \"no\", \"maybe\")\n",
    "\n",
    "def norm_mcqa(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    s = str(x).strip().upper()\n",
    "    mp = {\"1\":\"A\",\"2\":\"B\",\"3\":\"C\",\"4\":\"D\"}\n",
    "    if s in mp: return mp[s]\n",
    "    if s in LETTERS: return s\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", s)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def norm_decision(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    s = str(x).strip().lower()\n",
    "    if s in DEC_SET: return s\n",
    "    m = re.search(r\"\\b(yes|no|maybe)\\b\", s)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_decision_from_text(text: str) -> str:\n",
    "    s = (text or \"\").lower()\n",
    "    matches = re.findall(r\"\\b(yes|no|maybe)\\b\", s)\n",
    "    return matches[-1] if matches else \"unknown\"\n",
    "\n",
    "def infer_task_from_gold(gold_list: List[str]) -> str:\n",
    "    mc = sum(1 for g in gold_list if norm_mcqa(g))\n",
    "    dc = sum(1 for g in gold_list if norm_decision(g))\n",
    "    if mc == 0 and dc == 0:\n",
    "        return \"text_only\"\n",
    "    return \"mcqa\" if mc >= dc else \"decision\"\n",
    "\n",
    "# =========================\n",
    "# Metrics: ACC + confusion\n",
    "# =========================\n",
    "def compute_acc_and_confusion_decision(gt_raw: List[str], pred_raw: List[str]) -> Dict[str, Any]:\n",
    "    total = 0\n",
    "    support = 0\n",
    "    format_errors = 0\n",
    "    correct = 0\n",
    "    confusion = {\n",
    "        \"yes->yes\": 0, \"yes->no\": 0, \"yes->maybe\": 0,\n",
    "        \"no->yes\": 0,  \"no->no\": 0,  \"no->maybe\": 0,\n",
    "        \"maybe->yes\": 0, \"maybe->no\": 0, \"maybe->maybe\": 0\n",
    "    }\n",
    "\n",
    "    for g0, p0 in zip(gt_raw, pred_raw):\n",
    "        g = norm_decision(g0)\n",
    "        if not g:\n",
    "            continue\n",
    "        total += 1\n",
    "\n",
    "        p = norm_decision(p0)\n",
    "        if not p:\n",
    "            format_errors += 1\n",
    "            continue\n",
    "\n",
    "        support += 1\n",
    "        key = f\"{g}->{p}\"\n",
    "        if key in confusion:\n",
    "            confusion[key] += 1\n",
    "        if g == p:\n",
    "            correct += 1\n",
    "\n",
    "    acc = (correct / total) if total else 0.0\n",
    "    return {\"acc\": float(acc), \"support\": int(support), \"format_errors\": int(format_errors), \"confusion\": confusion}\n",
    "\n",
    "def compute_acc_and_confusion_mcqa(gt_raw: List[str], pred_raw: List[str]) -> Dict[str, Any]:\n",
    "    labels = list(LETTERS)\n",
    "    idx = {c:i for i,c in enumerate(labels)}\n",
    "    conf = [[0]*4 for _ in range(4)]\n",
    "\n",
    "    total = 0\n",
    "    support = 0\n",
    "    format_errors = 0\n",
    "    correct = 0\n",
    "\n",
    "    for g0, p0 in zip(gt_raw, pred_raw):\n",
    "        g = norm_mcqa(g0)\n",
    "        if not g:\n",
    "            continue\n",
    "        total += 1\n",
    "\n",
    "        p = norm_mcqa(p0)\n",
    "        if p not in idx:\n",
    "            format_errors += 1\n",
    "            continue\n",
    "\n",
    "        support += 1\n",
    "        conf[idx[g]][idx[p]] += 1\n",
    "        if g == p:\n",
    "            correct += 1\n",
    "\n",
    "    acc = (correct / total) if total else 0.0\n",
    "    confusion = {f\"{g}->{p}\": conf[idx[g]][idx[p]] for g in labels for p in labels}\n",
    "    return {\"acc\": float(acc), \"support\": int(support), \"format_errors\": int(format_errors), \"confusion\": confusion}\n",
    "\n",
    "# =========================\n",
    "# BERTScore\n",
    "# =========================\n",
    "def compute_bertscore(preds: List[str], refs: List[str]) -> Tuple[Dict[str, float], Dict[str, List[float]]]:\n",
    "    preds = [\"\" if p is None else str(p) for p in preds]\n",
    "    refs  = [\"\" if r is None else str(r) for r in refs]\n",
    "\n",
    "    import torch\n",
    "    import evaluate\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    metric = evaluate.load(\"bertscore\")\n",
    "    res = metric.compute(\n",
    "        predictions=preds,\n",
    "        references=refs,\n",
    "        lang=BERT_LANG,\n",
    "        model_type=BERT_MODEL_TYPE,\n",
    "        device=device,\n",
    "        batch_size=BERT_BATCH_SIZE,\n",
    "        rescale_with_baseline=bool(BERT_RESCALE_WITH_BASELINE),\n",
    "    )\n",
    "\n",
    "    p_list = [float(x) for x in res[\"precision\"]]\n",
    "    r_list = [float(x) for x in res[\"recall\"]]\n",
    "    f_list = [float(x) for x in res[\"f1\"]]\n",
    "\n",
    "    agg = {\n",
    "        \"precision\": float(np.mean(p_list)) if p_list else 0.0,\n",
    "        \"recall\": float(np.mean(r_list)) if r_list else 0.0,\n",
    "        \"f1\": float(np.mean(f_list)) if f_list else 0.0,\n",
    "        \"model_type\": BERT_MODEL_TYPE,\n",
    "        \"rescale_with_baseline\": bool(BERT_RESCALE_WITH_BASELINE),\n",
    "        \"device\": device,\n",
    "        \"batch_size\": int(BERT_BATCH_SIZE),\n",
    "        \"hashcode\": res.get(\"hashcode\"),\n",
    "    }\n",
    "    per = {\"precision\": p_list, \"recall\": r_list, \"f1\": f_list}\n",
    "    return agg, per\n",
    "\n",
    "# =========================\n",
    "# Reporting / Save\n",
    "# =========================\n",
    "def format_section(title: str, obj: Dict) -> str:\n",
    "    return f\"\\n=== {title}  ===\\n{json.dumps(obj, indent=2, ensure_ascii=False)}\\n\"\n",
    "\n",
    "def stem_tag(path: str) -> str:\n",
    "    base = os.path.basename(path)\n",
    "    return os.path.splitext(base)[0]\n",
    "\n",
    "def run_one(path: str) -> Dict[str, Any]:\n",
    "    df = load_table(path)\n",
    "    info = {\"path\": path, \"rows\": int(len(df)), \"columns\": list(df.columns)}\n",
    "\n",
    "    # --- ä¿®å¤éƒ¨åˆ†ï¼šæ·»åŠ  \"gt\" åˆ°å€™é€‰åˆ—è¡¨ ---\n",
    "    COL_REF = pick_col(df, [\"reference\", \"ref\", \"long_answer\", \"answer\", \"gold_answer\", \"gold\", \"gt\"])\n",
    "    COL_GT  = pick_col(df, [\"gt_decision\", \"ground_truth\", \"gold\", \"label\", \"final_decision\", \"gt\"])\n",
    "\n",
    "    # é¢„æµ‹åˆ—\n",
    "    COL_BASE_TEXT = pick_col(df, [\"base_text\", \"base_pred\", \"base_prediction\", \"pred_no_rag\", \"pred\"])\n",
    "    COL_RAG_TEXT  = pick_col(df, [\"rag_text\", \"rag_pred\", \"rag_prediction\", \"pred_with_rag\", \"pred_vector_rag\", \"pred\"])\n",
    "\n",
    "    if COL_REF is None and COL_GT is not None:\n",
    "        COL_REF = COL_GT  # æ²¡æœ‰ referenceï¼Œç”¨ gt åš reference\n",
    "    if COL_REF is None:\n",
    "        raise KeyError(f\"æ‰¾ä¸åˆ° reference/ref/gold/gt åˆ—ã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "\n",
    "    if COL_GT is None:\n",
    "        COL_GT = pick_col(df, [\"gold\", \"gt\"]) # Double check\n",
    "    if COL_GT is None:\n",
    "        raise KeyError(f\"æ‰¾ä¸åˆ° GT åˆ—ï¼ˆgt_decision/ground_truth/gold/gtï¼‰ã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "\n",
    "    if COL_BASE_TEXT is None:\n",
    "        raise KeyError(f\"æ‰¾ä¸åˆ°é¢„æµ‹åˆ— pred/base_predã€‚å½“å‰åˆ—: {list(df.columns)}\")\n",
    "    \n",
    "    # å¦‚æœæ‰¾ä¸åˆ° rag é¢„æµ‹åˆ—ï¼Œé»˜è®¤ç”¨ base åˆ— (å› ä¸ºä½ çš„æ–‡ä»¶åªæœ‰ä¸€åˆ— pred)\n",
    "    if COL_RAG_TEXT is None:\n",
    "        COL_RAG_TEXT = COL_BASE_TEXT  \n",
    "\n",
    "    refs = df[COL_REF].fillna(\"\").astype(str).tolist()\n",
    "    gold = df[COL_GT].fillna(\"\").astype(str).tolist()\n",
    "    base_text = df[COL_BASE_TEXT].fillna(\"\").astype(str).tolist()\n",
    "    rag_text  = df[COL_RAG_TEXT].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    task = infer_task_from_gold(gold)\n",
    "\n",
    "    info.update({\n",
    "        \"task\": task,\n",
    "        \"col_ref\": COL_REF,\n",
    "        \"col_gt\": COL_GT,\n",
    "        \"col_base_text\": COL_BASE_TEXT,\n",
    "        \"col_rag_text\": COL_RAG_TEXT,\n",
    "        \"same_pred_col\": (COL_BASE_TEXT == COL_RAG_TEXT),\n",
    "    })\n",
    "\n",
    "    # --- BERTScoreï¼ˆå¯¹ base_text vs refsï¼Œrag_text vs refsï¼‰---\n",
    "    bert = {\"base\": None, \"rag\": None, \"error\": None}\n",
    "    bert_per = {\"base\": None, \"rag\": None}\n",
    "    try:\n",
    "        base_bs, base_per = compute_bertscore(base_text, refs)\n",
    "        rag_bs, rag_per   = compute_bertscore(rag_text,  refs)\n",
    "        bert[\"base\"], bert[\"rag\"] = base_bs, rag_bs\n",
    "        bert_per[\"base\"], bert_per[\"rag\"] = base_per, rag_per\n",
    "    except Exception as e:\n",
    "        bert[\"error\"] = str(e)\n",
    "\n",
    "    # --- ACC ---\n",
    "    acc = {\"base\": None, \"rag\": None}\n",
    "    if task == \"decision\":\n",
    "        base_dec = [extract_decision_from_text(t) for t in base_text]\n",
    "        rag_dec  = [extract_decision_from_text(t) for t in rag_text]\n",
    "        acc[\"base\"] = compute_acc_and_confusion_decision(gold, base_dec)\n",
    "        acc[\"rag\"]  = compute_acc_and_confusion_decision(gold, rag_dec)\n",
    "    elif task == \"mcqa\":\n",
    "        acc[\"base\"] = compute_acc_and_confusion_mcqa(gold, base_text)\n",
    "        acc[\"rag\"]  = compute_acc_and_confusion_mcqa(gold, rag_text)\n",
    "    else:\n",
    "        acc[\"base\"] = {\"skipped\": \"gold æ—¢éå†³ç­–ä¹ŸéMCQAæ ¼å¼\"}\n",
    "        acc[\"rag\"]  = {\"skipped\": \"gold æ—¢éå†³ç­–ä¹ŸéMCQAæ ¼å¼\"}\n",
    "\n",
    "    # --- è¾“å‡ºæ‰©å±•è¡¨ ---\n",
    "    out_df = df.copy()\n",
    "    if bert_per[\"base\"] is not None:\n",
    "        out_df[\"bert_f1_base\"] = bert_per[\"base\"][\"f1\"]\n",
    "    if bert_per[\"rag\"] is not None:\n",
    "        out_df[\"bert_f1_rag\"] = bert_per[\"rag\"][\"f1\"]\n",
    "    if (\"bert_f1_base\" in out_df.columns) and (\"bert_f1_rag\" in out_df.columns):\n",
    "        out_df[\"bert_diff\"] = out_df[\"bert_f1_rag\"] - out_df[\"bert_f1_base\"]\n",
    "\n",
    "    return {\"info\": info, \"metrics\": {\"bertscore\": bert, \"acc\": acc}, \"out_df\": out_df}\n",
    "\n",
    "def save_outputs(result: Dict[str, Any], out_dir: str, tag: str) -> Tuple[str, str, str]:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_txt  = os.path.join(out_dir, f\"{tag}_report.txt\")\n",
    "    out_json = os.path.join(out_dir, f\"{tag}_metrics.json\")\n",
    "    out_csv  = os.path.join(out_dir, f\"{tag}_expanded.csv\")\n",
    "\n",
    "    info = result[\"info\"]\n",
    "    m = result[\"metrics\"]\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"{TITLE}\\n\" + \"=\"*60)\n",
    "    lines.append(f\"[Info] Loaded: {info['path']} | rows={info['rows']}\")\n",
    "    lines.append(f\"[Info] Columns: {info['columns']}\")\n",
    "    lines.append(f\"[Info] Task: {info['task']}\")\n",
    "    lines.append(f\"[Info] Using cols: ref={info['col_ref']} | gt={info['col_gt']} | base={info['col_base_text']} | rag={info['col_rag_text']}\")\n",
    "    if info.get(\"same_pred_col\", False):\n",
    "        lines.append(\"[Warn] è¿™æ˜¯ä¸€ä¸ªå•é¢„æµ‹æ–‡ä»¶ï¼ŒBase å’Œ RAG æŒ‡æ ‡å°†æ˜¾ç¤ºç›¸åŒçš„å€¼ã€‚\")\n",
    "\n",
    "    if m[\"bertscore\"][\"base\"] is not None:\n",
    "        lines.append(\"\\n=== Base BERTScore  ===\")\n",
    "        lines.append(json.dumps(m[\"bertscore\"][\"base\"], indent=2, ensure_ascii=False))\n",
    "        lines.append(\"\\n=== RAG  BERTScore  ===\")\n",
    "        lines.append(json.dumps(m[\"bertscore\"][\"rag\"], indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        lines.append(\"\\n[Warn] BERTScore skipped: \" + str(m[\"bertscore\"][\"error\"]))\n",
    "\n",
    "    lines.append(format_section(\"Base ACC\", m[\"acc\"][\"base\"]))\n",
    "    lines.append(format_section(\"RAG  ACC\", m[\"acc\"][\"rag\"]))\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result[\"metrics\"], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    result[\"out_df\"].to_csv(out_csv, index=False)\n",
    "    return out_txt, out_json, out_csv\n",
    "\n",
    "def main():\n",
    "    for p in INPUT_PATHS:\n",
    "        try:\n",
    "            res = run_one(p)\n",
    "            tag = stem_tag(p)\n",
    "            out_txt, out_json, out_csv = save_outputs(res, OUT_DIR, tag)\n",
    "            print(f\"[Done] Processed {p}\")\n",
    "            print(f\"       Report : {out_txt}\")\n",
    "            print(f\"       CSV    : {out_csv}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to process {p}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89be09f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
